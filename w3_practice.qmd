---
title: 第三讲上机实践
---

本上机讲义覆盖以下内容：

- **Part 1**: 环境准备与数据加载
- **Part 2**: 理解过拟合问题
- **Part 3**: Ridge、Lasso、Elastic Net 实战
- **Part 4**: 交叉验证与参数选择
- **Part 5**: 正则化路径可视化

**学习目标**

✅ 掌握 Ridge、Lasso、Elastic Net 的使用
✅ 理解正则化参数 α 的作用
✅ 学会使用交叉验证选择最优参数
✅ 理解 Lasso 的特征选择功能

⚠️ **重要提醒**

1. 正则化前必须标准化特征
2. 不要在测试集上选择参数
3. 多种方法对比，选择最适合的

## 环境准备与数据加载

首先导入必要的库，并加载 Boston 房价数据集。

```{python}
#| message: false
#| warning: false
# 导入必要的库
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Boston housing 数据集已从 sklearn 中移除，我们从原始数据源获取
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# 设置中文字体
import platform
system = platform.system()
if system == 'Windows':
    plt.rcParams['font.sans-serif'] = ['SimHei']
elif system == 'Darwin':
    plt.rcParams['font.sans-serif'] = ['Songti SC']
else:
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei']
plt.rcParams['axes.unicode_minus'] = False

# 设置随机种子
np.random.seed(42)

print("✓ 库导入成功！")
```

### 加载数据集

```{python}
#| message: false
#| warning: false
# 加载 Boston Housing 数据集
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

X = data
y = target

# 创建DataFrame
feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
df = pd.DataFrame(X, columns=feature_names)
df['Price'] = y

print(f"数据形状: {X.shape}")
print(f"样本数: {X.shape[0]}, 特征数: {X.shape[1]}")
print(f"\n特征名称: {feature_names}")
print(f"\n目标变量: 房价中位数（单位：千美元）")
print(f"\n前5行数据:")
df.head()
```

### 数据探索

观察数据的基本统计信息和特征尺度。

```{python}
#| message: false
#| warning: false
# 基本统计信息
print("数据统计信息:")
print(df.describe())

print("\n" + "="*60)
print("⚠️ 关键观察：不同特征的尺度差异很大！")
print("\n这就是为什么正则化前需要标准化！")
print("="*60)
```

### 数据切分与标准化

**重要步骤**：
1. 先切分训练集和测试集
2. 在训练集上 fit StandardScaler
3. 用训练集的参数 transform 训练集和测试集

```{python}
#| message: false
#| warning: false
# 切分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"训练集大小: {X_train.shape}")
print(f"测试集大小: {X_test.shape}")

# 标准化（重要！）
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # 在训练集上 fit
X_test_scaled = scaler.transform(X_test)  # 用训练集的参数 transform

# 验证标准化效果
print("\n标准化验证:")
print(f"训练集均值（应接近0）: {X_train_scaled.mean(axis=0).round(10)}")
print(f"训练集标准差（应接近1）: {X_train_scaled.std(axis=0).round(3)}")
print("\n✓ 标准化完成！")
```

## 理解过拟合问题

用多项式回归演示过拟合现象。

### 生成简单数据集

```{python}
#| message: false
#| warning: false
# 生成简单的数据集：y = 2x + 1 + 噪声
np.random.seed(42)
X_simple = np.linspace(0, 10, 20).reshape(-1, 1)
y_simple = 2 * X_simple.flatten() + 1 + np.random.randn(20) * 3

# 真实关系（无噪声）
X_true = np.linspace(0, 10, 100).reshape(-1, 1)
y_true = 2 * X_true.flatten() + 1

# 绘制数据
plt.figure(figsize=(10, 6))
plt.scatter(X_simple, y_simple, s=80, alpha=0.6, label='训练数据（有噪声）')
plt.plot(X_true, y_true, 'k--', linewidth=2, alpha=0.5, label='真实关系')
plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('简单线性关系 + 噪声', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.show()
```

### 对比不同复杂度的模型

尝试用不同次数的多项式拟合数据。

```{python}
#| message: false
#| warning: false
# 训练不同次数的多项式回归
degrees = [1, 4, 15]
titles = ['欠拟合（1次）', '刚刚好（4次）', '过拟合（15次）']

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, (degree, title) in enumerate(zip(degrees, titles)):
    # 生成多项式特征
    poly = PolynomialFeatures(degree=degree)
    X_poly = poly.fit_transform(X_simple)
    X_true_poly = poly.transform(X_true)

    # 训练模型
    model = LinearRegression()
    model.fit(X_poly, y_simple)
    y_pred = model.predict(X_true_poly)

    # 计算误差
    train_mse = mean_squared_error(y_simple, model.predict(X_poly))
    test_mse = mean_squared_error(y_true, y_pred)

    # 绘图
    axes[idx].scatter(X_simple, y_simple, s=80, alpha=0.6, label='训练数据', zorder=3)
    axes[idx].plot(X_true, y_true, 'k--', linewidth=2, alpha=0.3, label='真实关系')
    axes[idx].plot(X_true, y_pred, linewidth=2.5, label='拟合曲线')
    axes[idx].set_xlabel('X', fontsize=11)
    axes[idx].set_ylabel('y', fontsize=11)
    axes[idx].set_title(f'{title}\n训练MSE={train_mse:.1f}, 测试MSE={test_mse:.1f}', fontsize=12)
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)
    axes[idx].set_ylim(-5, 25)

plt.tight_layout()
plt.show()

print("\n观察：")
print("- 1次多项式：训练误差大，测试误差大 → 欠拟合")
print("- 4次多项式：训练误差小，测试误差小 → 刚刚好")
print("- 15次多项式：训练误差很小，测试误差大 → 过拟合")
```

## Ridge、Lasso、Elastic Net 实战

在 Boston 房价数据上对比四种模型。

### 基线模型：普通线性回归

```{python}
#| message: false
#| warning: false
# 训练普通线性回归
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)

# 预测
y_train_pred_lr = lr.predict(X_train_scaled)
y_test_pred_lr = lr.predict(X_test_scaled)

# 评估
train_rmse_lr = np.sqrt(mean_squared_error(y_train, y_train_pred_lr))
test_rmse_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))
train_r2_lr = r2_score(y_train, y_train_pred_lr)
test_r2_lr = r2_score(y_test, y_test_pred_lr)

print("="*50)
print("普通线性回归（基线模型）")
print("="*50)
print(f"训练集 RMSE: {train_rmse_lr:.4f}")
print(f"测试集 RMSE: {test_rmse_lr:.4f}")
print(f"训练集 R²:   {train_r2_lr:.4f}")
print(f"测试集 R²:   {test_r2_lr:.4f}")
print(f"\n系数L2范数:  {np.linalg.norm(lr.coef_):.4f}")
print(f"非零系数数:  {np.sum(lr.coef_ != 0)}/{len(lr.coef_)}")
```

### Ridge 回归（L2 正则化）

惩罚系数的平方和，让系数变小但不为0。

```{python}
#| message: false
#| warning: false
# 训练 Ridge 回归
ridge = Ridge(alpha=1.0)  # alpha 越大，正则化越强
ridge.fit(X_train_scaled, y_train)

# 预测与评估
y_train_pred_ridge = ridge.predict(X_train_scaled)
y_test_pred_ridge = ridge.predict(X_test_scaled)

train_rmse_ridge = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))
test_rmse_ridge = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))
train_r2_ridge = r2_score(y_train, y_train_pred_ridge)
test_r2_ridge = r2_score(y_test, y_test_pred_ridge)

print("="*50)
print("Ridge 回归 (α=1.0)")
print("="*50)
print(f"训练集 RMSE: {train_rmse_ridge:.4f}")
print(f"测试集 RMSE: {test_rmse_ridge:.4f}")
print(f"训练集 R²:   {train_r2_ridge:.4f}")
print(f"测试集 R²:   {test_r2_ridge:.4f}")
```

### Lasso 回归（L1 正则化）

惩罚系数的绝对值和，让某些系数变为0。

```{python}
#| message: false
#| warning: false
# 训练 Lasso 回归
lasso = Lasso(alpha=0.01, max_iter=10000)  # alpha 越大，正则化越强
lasso.fit(X_train_scaled, y_train)

# 预测与评估
y_train_pred_lasso = lasso.predict(X_train_scaled)
y_test_pred_lasso = lasso.predict(X_test_scaled)

train_rmse_lasso = np.sqrt(mean_squared_error(y_train, y_train_pred_lasso))
test_rmse_lasso = np.sqrt(mean_squared_error(y_test, y_test_pred_lasso))
train_r2_lasso = r2_score(y_train, y_train_pred_lasso)
test_r2_lasso = r2_score(y_test, y_test_pred_lasso)

print("="*50)
print("Lasso 回归 (α=0.01)")
print("="*50)
print(f"训练集 RMSE: {train_rmse_lasso:.4f}")
print(f"测试集 RMSE: {test_rmse_lasso:.4f}")
print(f"训练集 R²:   {train_r2_lasso:.4f}")
print(f"测试集 R²:   {test_r2_lasso:.4f}")
print(f"\n保留特征数: {np.sum(lasso.coef_ != 0)}/{len(lasso.coef_)}")
```

### Elastic Net 回归（L1+L2 正则化）

结合 Ridge 和 Lasso 的优点。

```{python}
#| message: false
#| warning: false
# 训练 Elastic Net 回归
elastic = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)
# l1_ratio=0.5 表示 L1 和 L2 正则化各占 50%
elastic.fit(X_train_scaled, y_train)

# 预测与评估
y_train_pred_elastic = elastic.predict(X_train_scaled)
y_test_pred_elastic = elastic.predict(X_test_scaled)

train_rmse_elastic = np.sqrt(mean_squared_error(y_train, y_train_pred_elastic))
test_rmse_elastic = np.sqrt(mean_squared_error(y_test, y_test_pred_elastic))
train_r2_elastic = r2_score(y_train, y_train_pred_elastic)
test_r2_elastic = r2_score(y_test, y_test_pred_elastic)

print("="*50)
print("Elastic Net 回归 (α=0.01, l1_ratio=0.5)")
print("="*50)
print(f"训练集 RMSE: {train_rmse_elastic:.4f}")
print(f"测试集 RMSE: {test_rmse_elastic:.4f}")
print(f"训练集 R²:   {train_r2_elastic:.4f}")
print(f"测试集 R²:   {test_r2_elastic:.4f}")
print(f"\n保留特征数: {np.sum(elastic.coef_ != 0)}/{len(elastic.coef_)}")
```

### 模型对比总结

```{python}
#| message: false
#| warning: false
# 创建对比表格
results = pd.DataFrame({
    '模型': ['普通回归', 'Ridge (α=1.0)', 'Lasso (α=0.01)', 'Elastic Net (α=0.01)'],
    '训练R²': [train_r2_lr, train_r2_ridge, train_r2_lasso, train_r2_elastic],
    '测试R²': [test_r2_lr, test_r2_ridge, test_r2_lasso, test_r2_elastic],
    '系数L2范数': [
        np.linalg.norm(lr.coef_),
        np.linalg.norm(ridge.coef_),
        np.linalg.norm(lasso.coef_),
        np.linalg.norm(elastic.coef_)
    ],
    '非零系数': [
        len(lr.coef_),
        len(ridge.coef_),
        np.sum(lasso.coef_ != 0),
        np.sum(elastic.coef_ != 0)
    ]
})

print("="*70)
print("四种模型对比")
print("="*70)
print(results.to_string(index=False, float_format='%.4f'))

# 找出最佳模型
best_idx = results['测试R²'].idxmax()
print(f"\n✓ 测试集表现最好: {results.loc[best_idx, '模型']}")
print(f"  测试R² = {results.loc[best_idx, '测试R²']:.4f}")
```

### 可视化系数对比

```{python}
#| message: false
#| warning: false
# 绘制系数对比图
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

models_list = [
    ('普通线性回归', lr.coef_),
    ('Ridge (α=1.0)', ridge.coef_),
    ('Lasso (α=0.01)', lasso.coef_),
    ('Elastic Net', elastic.coef_)
]

for idx, (name, coef) in enumerate(models_list):
    colors = ['green' if c > 0 else 'red' for c in coef]
    axes[idx].barh(feature_names, coef, color=colors, alpha=0.7)
    axes[idx].set_xlabel('系数值', fontsize=11)
    axes[idx].set_title(f'{name}\n非零系数: {np.sum(coef != 0)}个', fontsize=12, weight='bold')
    axes[idx].axvline(x=0, color='black', linestyle='-', linewidth=1)
    axes[idx].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

print("\n观察：")
print("- 普通回归：所有系数都非零")
print("- Ridge：系数变小，但都保留")
print("- Lasso：某些系数变为0（特征选择）")
print("- Elastic Net：介于 Ridge 和 Lasso 之间")
```

## 交叉验证与参数选择

使用交叉验证自动选择最优的正则化参数 α。

### RidgeCV：自动选择最优 α

```{python}
#| message: false
#| warning: false
# 准备候选的 alpha 值
alphas_cv = np.logspace(-3, 3, 50)  # 从 0.001 到 1000，50个值

# RidgeCV：5折交叉验证
ridge_cv = RidgeCV(alphas=alphas_cv, cv=5)
ridge_cv.fit(X_train_scaled, y_train)

print("="*50)
print("RidgeCV 结果（5折交叉验证）")
print("="*50)
print(f"候选 α 范围: {alphas_cv.min():.3f} ~ {alphas_cv.max():.1f}")
print(f"\n选出的最优 α: {ridge_cv.alpha_:.4f}")
print(f"\n训练集 R²: {ridge_cv.score(X_train_scaled, y_train):.4f}")
print(f"测试集 R²: {ridge_cv.score(X_test_scaled, y_test):.4f}")
print(f"\n对比固定 α=1.0:")
print(f"  测试R² 提升: {(ridge_cv.score(X_test_scaled, y_test) - test_r2_ridge):.5f}")
```

### LassoCV：自动选择最优 α

```{python}
#| message: false
#| warning: false
# LassoCV：5折交叉验证
lasso_cv = LassoCV(alphas=alphas_cv, cv=5, max_iter=10000)
lasso_cv.fit(X_train_scaled, y_train)

print("="*50)
print("LassoCV 结果（5折交叉验证）")
print("="*50)
print(f"选出的最优 α: {lasso_cv.alpha_:.4f}")
print(f"\n训练集 R²: {lasso_cv.score(X_train_scaled, y_train):.4f}")
print(f"测试集 R²: {lasso_cv.score(X_test_scaled, y_test):.4f}")
print(f"\n保留特征数: {np.sum(lasso_cv.coef_ != 0)}/{len(lasso_cv.coef_)}")
print(f"被剔除特征: {[feature_names[i] for i in range(len(lasso_cv.coef_)) if lasso_cv.coef_[i] == 0]}")
```

### ElasticNetCV：自动选择最优 α

```{python}
#| message: false
#| warning: false
# ElasticNetCV：5折交叉验证
elastic_cv = ElasticNetCV(alphas=alphas_cv, l1_ratio=0.5, cv=5, max_iter=10000)
elastic_cv.fit(X_train_scaled, y_train)

print("="*50)
print("ElasticNetCV 结果（5折交叉验证）")
print("="*50)
print(f"选出的最优 α: {elastic_cv.alpha_:.4f}")
print(f"\n训练集 R²: {elastic_cv.score(X_train_scaled, y_train):.4f}")
print(f"测试集 R²: {elastic_cv.score(X_test_scaled, y_test):.4f}")
print(f"\n保留特征数: {np.sum(elastic_cv.coef_ != 0)}/{len(elastic_cv.coef_)}")
```

### 最终模型对比

```{python}
#| message: false
#| warning: false
# 汇总所有结果
final_results = pd.DataFrame({
    '模型': [
        '普通回归',
        'Ridge (α=1.0)',
        'Lasso (α=0.01)',
        'Elastic Net (α=0.01)',
        'Ridge (CV)',
        'Lasso (CV)',
        'Elastic Net (CV)'
    ],
    'α': [
        '-',
        '1.0',
        '0.01',
        '0.01',
        f'{ridge_cv.alpha_:.4f}',
        f'{lasso_cv.alpha_:.4f}',
        f'{elastic_cv.alpha_:.4f}'
    ],
    '测试R²': [
        test_r2_lr,
        test_r2_ridge,
        test_r2_lasso,
        test_r2_elastic,
        ridge_cv.score(X_test_scaled, y_test),
        lasso_cv.score(X_test_scaled, y_test),
        elastic_cv.score(X_test_scaled, y_test)
    ],
    '非零特征': [
        len(lr.coef_),
        len(ridge.coef_),
        np.sum(lasso.coef_ != 0),
        np.sum(elastic.coef_ != 0),
        len(ridge_cv.coef_),
        np.sum(lasso_cv.coef_ != 0),
        np.sum(elastic_cv.coef_ != 0)
    ]
})

print("\n" + "="*80)
print("最终模型对比（包含交叉验证优化后的模型）")
print("="*80)
print(final_results.to_string(index=False))

# 找出最佳模型
best_idx = final_results['测试R²'].idxmax()
print(f"\n" + "="*80)
print(f"✓ 推荐模型: {final_results.loc[best_idx, '模型']}")
print(f"  - 测试R²: {final_results.loc[best_idx, '测试R²']:.4f}")
print(f"  - 最优α: {final_results.loc[best_idx, 'α']}")
print(f"  - 保留特征: {final_results.loc[best_idx, '非零特征']}个")
print("="*80)
```

---

## 正则化路径可视化（选做）

观察随着 α 增大，各特征系数如何变化。

### 计算 Lasso 正则化路径

```{python}
#| message: false
#| warning: false
# 计算不同 alpha 下的系数
alphas_path = np.logspace(-3, 1, 100)
coefs_lasso_path = []

for alpha in alphas_path:
    lasso_temp = Lasso(alpha=alpha, max_iter=10000)
    lasso_temp.fit(X_train_scaled, y_train)
    coefs_lasso_path.append(lasso_temp.coef_)

coefs_lasso_path = np.array(coefs_lasso_path)

print("✓ 正则化路径计算完成！")
```

### 绘制正则化路径图

```{python}
#| message: false
#| warning: false

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 图1：系数路径
for i in range(coefs_lasso_path.shape[1]):
    axes[0].plot(alphas_path, coefs_lasso_path[:, i],
                linewidth=2, alpha=0.8, label=feature_names[i])

axes[0].set_xlabel('α (正则化强度)', fontsize=12)
axes[0].set_ylabel('系数值', fontsize=12)
axes[0].set_title('Lasso 正则化路径\nα 增大时，系数逐渐变为 0', fontsize=13, weight='bold')
axes[0].set_xscale('log')
axes[0].axhline(y=0, color='black', linestyle='--', linewidth=1)
axes[0].axvline(x=lasso_cv.alpha_, color='red', linestyle='--', linewidth=2,
               label=f'CV选择的α={lasso_cv.alpha_:.3f}')
axes[0].legend(loc='best', fontsize=9, ncol=2)
axes[0].grid(True, alpha=0.3)

# 图2：非零系数数量
n_nonzero_path = [np.sum(coefs != 0) for coefs in coefs_lasso_path]
axes[1].plot(alphas_path, n_nonzero_path, 'b-', linewidth=2.5)
axes[1].set_xlabel('α (正则化强度)', fontsize=12)
axes[1].set_ylabel('非零系数个数', fontsize=12)
axes[1].set_title('Lasso 特征选择效果\nα 越大，保留的特征越少', fontsize=13, weight='bold')
axes[1].set_xscale('log')
axes[1].grid(True, alpha=0.3)
axes[1].fill_between(alphas_path, 0, n_nonzero_path, alpha=0.3)
axes[1].axvline(x=lasso_cv.alpha_, color='red', linestyle='--',
               linewidth=2, label=f'CV选择的α={lasso_cv.alpha_:.3f}')
axes[1].legend(fontsize=10)

plt.tight_layout()
plt.show()

print("\n观察：")
print(f"- α = {alphas_path[0]:.3f} 时：{n_nonzero_path[0]}个特征保留")
print(f"- α = {lasso_cv.alpha_:.3f} (CV选择) 时：{np.sum(lasso_cv.coef_ != 0)}个特征保留")
print(f"- α = {alphas_path[-1]:.1f} 时：{n_nonzero_path[-1]}个特征保留")
```

## 总结与思考

### 本次课程要点

✅ **过拟合**：模型太复杂，记住了训练数据的噪声
✅ **正则化**：限制模型复杂度，提高泛化能力
✅ **Ridge (L2)**：让系数变小但不为0
✅ **Lasso (L1)**：让某些系数变为0（特征选择）
✅ **Elastic Net**：结合 Ridge 和 Lasso
✅ **交叉验证**：自动选择最优参数

### 重要提醒

⚠️ 正则化前必须标准化特征
⚠️ 不要在测试集上选参数
⚠️ 用 RidgeCV/LassoCV 自动选择 α
