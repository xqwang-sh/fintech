{
  "hash": "617428e462fb6c17440b3502d06ce2bd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 第六讲：神经网络基础\n---\n\n\n\n## 开场：深度学习的神秘面纱\n\n---\n\n### 深度学习无处不在\n\n**你每天都在使用深度学习**：\n\n- 📱 手机人脸解锁\n- 🗣️ 语音助手（Siri、小爱）\n- 📷 拍照美颜\n- 🚗 自动驾驶\n- 💬 ChatGPT\n\n**问题**：这些\"黑科技\"的底层原理是什么？\n\n**答案**：**神经网络**\n\n---\n\n### 本周目标：揭开神秘面纱\n\n#### 知识目标\n1. 理解神经元的基本概念\n2. 理解多层感知机（MLP）的网络结构\n3. 了解常见激活函数（ReLU、Sigmoid、Tanh）\n4. 理解前向传播和反向传播的直觉\n5. 知道何时用树模型 vs 神经网络\n\n#### 技能目标\n1. 使用 Keras 搭建简单的 MLP\n2. 训练 MLP 并可视化学习曲线\n3. 在测试集上评估 MLP 的 AUC\n4. 调整网络结构（层数、神经元数量）\n\n---\n\n#### 核心理念\n\n**深度学习不神秘**\n\n- 它是基于简单数学运算的累积\n- 关键是\"学习\"：通过数据自动调整参数\n- 不要被复杂的公式吓倒，先建立直觉！\n\n---\n\n## 第一部分：从生物到人工\n\n---\n\n### 1.1 生物神经元\n\n#### 人脑的神经网络\n\n- 人脑约有 **860 亿个神经元**\n- 每个神经元连接 **数千到上万个**其他神经元\n- 通过**电信号**传递信息\n- 学习 = 调整神经元之间的**连接强度**\n\n---\n\n#### 生物神经元的结构\n\n\n\n\n```{mermaid}\ngraph LR\n    A[树突<br/>Dendrites<br/>接收信号] --> B[细胞体<br/>Soma<br/>整合处理]\n    B --> C[轴突<br/>Axon<br/>输出信号]\n    C --> D[突触<br/>Synapse<br/>连接下一个神经元]\n    \n    style A fill:#fff9c4\n    style B fill:#c8e6c9\n    style C fill:#bbdefb\n    style D fill:#ffccbc\n```\n\n\n\n\n**关键过程**：\n\n1. 树突接收多个输入信号\n2. 细胞体对信号进行**加权求和**\n3. 如果总和超过**阈值**，神经元\"发火\"（激活）\n4. 通过轴突传递信号给下一个神经元\n\n---\n\n### 1.2 人工神经元\n\n#### 数学模型\n\n**简化的神经元模型（1943年，McCulloch & Pitts）**：\n\n$$\ny = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\n$$\n\n**组成部分**：\n\n- $x_i$：输入信号（特征）\n- $w_i$：权重（连接强度）\n- $b$：偏置（阈值）\n- $f$：激活函数（决定是否\"发火\"）\n- $y$：输出\n\n---\n\n#### 可视化人工神经元\n\n\n\n\n```{mermaid}\ngraph LR\n    X1[x₁] -->|w₁| S((Σ))\n    X2[x₂] -->|w₂| S\n    X3[x₃] -->|w₃| S\n    B[+b] --> S\n    S --> F[激活函数<br/>f]\n    F --> Y[y]\n    \n    style S fill:#fff9c4\n    style F fill:#c8e6c9\n    style Y fill:#4caf50,color:#fff\n```\n\n\n\n\n**步骤 1：加权求和**\n\n$$\nz = w_1 x_1 + w_2 x_2 + w_3 x_3 + b\n$$\n\n**步骤 2：激活**\n\n$$\ny = f(z)\n$$\n\n---\n\n#### 具体例子：Titanic 生存预测\n\n**特征**：\n\n- $x_1$ = 性别（1=男，0=女）\n- $x_2$ = 年龄\n- $x_3$ = 票价\n\n**假设权重**：\n\n- $w_1 = -2$（性别越男越不利）\n- $w_2 = -0.01$（年龄越大越不利）\n- $w_3 = 0.001$（票价越高越有利）\n- $b = 1$（基础偏置）\n\n---\n\n**计算过程**：\n\n```\n男性乘客：x₁=1, x₂=25, x₃=50\nz = (-2)×1 + (-0.01)×25 + 0.001×50 + 1\n  = -2 - 0.25 + 0.05 + 1\n  = -1.2\n\n经过 Sigmoid 激活：\ny = 1/(1+e^1.2) ≈ 0.23\n\n解释：存活概率 23%（较低）\n```\n\n**权重的含义**：\n\n- $w_1 = -2$：性别对生存影响最大\n- 负权重表示负相关\n- 权重的绝对值表示重要性\n\n---\n\n## 第二部分：激活函数\n\n---\n\n### 为什么需要激活函数？\n\n**没有激活函数的问题**：\n\n$$\n\\begin{align}\n\\text{层1: } &h_1 = W_1 x + b_1 \\\\\n\\text{层2: } &h_2 = W_2 h_1 + b_2 = W_2(W_1 x + b_1) + b_2 \\\\\n&= (W_2 W_1) x + (W_2 b_1 + b_2) \\\\\n&= W' x + b' \\quad \\text{（仍然是线性）}\n\\end{align}\n$$\n\n**结论**：多层线性叠加 = 单层线性\n\n**激活函数的作用**：**引入非线性**，让网络能学习复杂模式\n\n---\n\n### 常见激活函数\n\n$$\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\n\n::: {#f6a10ce3 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nz = np.linspace(-6, 6, 100)\n\nsigmoid = 1 / (1 + np.exp(-z))\nrelu = np.maximum(0, z)\ntanh = np.tanh(z)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Sigmoid\naxes[0].plot(z, sigmoid, linewidth=3, color='#2196f3')\naxes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[0].axhline(y=1, color='k', linestyle='--', alpha=0.3)\naxes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[0].set_xlabel('z', fontsize=12)\naxes[0].set_ylabel('σ(z)', fontsize=12)\naxes[0].set_title('Sigmoid\\nσ(z) = 1/(1+e⁻ᶻ)', fontsize=14)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_ylim(-0.1, 1.1)\naxes[0].text(3, 0.5, '输出: (0, 1)', fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat'))\n\n# ReLU\naxes[1].plot(z, relu, linewidth=3, color='#4caf50')\naxes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[1].set_xlabel('z', fontsize=12)\naxes[1].set_ylabel('ReLU(z)', fontsize=12)\naxes[1].set_title('ReLU\\nReLU(z) = max(0, z)', fontsize=14)\naxes[1].grid(True, alpha=0.3)\naxes[1].text(3, 3, '输出: [0, ∞)', fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat'))\n\n# Tanh\naxes[2].plot(z, tanh, linewidth=3, color='#ff9800')\naxes[2].axhline(y=0, color='k', linestyle='--', alpha=0.3)\naxes[2].axhline(y=1, color='k', linestyle='--', alpha=0.3)\naxes[2].axhline(y=-1, color='k', linestyle='--', alpha=0.3)\naxes[2].axvline(x=0, color='k', linestyle='--', alpha=0.3)\naxes[2].set_xlabel('z', fontsize=12)\naxes[2].set_ylabel('tanh(z)', fontsize=12)\naxes[2].set_title('Tanh\\ntanh(z) = (eᶻ-e⁻ᶻ)/(eᶻ+e⁻ᶻ)', fontsize=14)\naxes[2].grid(True, alpha=0.3)\naxes[2].set_ylim(-1.1, 1.1)\naxes[2].text(3, 0, '输出: (-1, 1)', fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat'))\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/dh/sd70vd1d0jg1tkw7h3zwjqxr0000gn/T/ipykernel_80177/1535391376.py:48: UserWarning:\n\nGlyph 8315 (\\N{SUPERSCRIPT MINUS}) missing from current font.\n\n/var/folders/dh/sd70vd1d0jg1tkw7h3zwjqxr0000gn/T/ipykernel_80177/1535391376.py:48: UserWarning:\n\nGlyph 7611 (\\N{MODIFIER LETTER SMALL Z}) missing from current font.\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nGlyph 8315 (\\N{SUPERSCRIPT MINUS}) missing from current font.\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nGlyph 7611 (\\N{MODIFIER LETTER SMALL Z}) missing from current font.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](w6_neural_networks_files/figure-html/cell-3-output-2.png){width=1429 height=472}\n:::\n:::\n\n\n---\n\n#### Sigmoid 详解\n\n**公式**：\n\n$$\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\n\n**特点**：\n\n- 输出范围：(0, 1)\n- S 形曲线，平滑\n- 可解释为**概率**\n\n**优点**：\n\n- 输出有界，稳定\n- 适合二分类输出层\n\n**缺点**：\n\n- **梯度消失**：当 $|z|$ 很大时，梯度接近 0\n- 计算成本高（指数运算）\n\n**使用场景**：\n\n- ✓ 二分类问题的**输出层**\n- ✗ 隐藏层（已被 ReLU 取代）\n\n---\n\n#### ReLU 详解\n\n**公式**：\n\n$$\n\\text{ReLU}(z) = \\max(0, z) = \\begin{cases}\nz & \\text{if } z > 0 \\\\\n0 & \\text{if } z \\leq 0\n\\end{cases}\n$$\n\n**优点**：\n\n- ⚡ **计算超快**（只是比较和选择）\n- 🎯 缓解梯度消失（$z>0$ 时梯度恒为 1）\n- 🧠 生物学合理性（神经元要么激活要么不激活）\n- 📊 稀疏激活（约 50% 神经元输出 0）\n\n**缺点**：\n\n- 💀 **\"神经元死亡\"**：某些神经元可能永远输出 0\n\n**使用场景**：\n\n- ✓ **隐藏层首选**\n- ✓ 绝大多数深度学习任务\n\n---\n\n#### Tanh 详解\n\n**公式**：\n\n$$\n\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n$$\n\n**特点**：\n\n- 输出范围：(-1, 1)\n- **零中心**（Sigmoid 不是）\n- 比 Sigmoid 更陡\n\n**优点**：\n\n- 零中心 → 收敛更快\n- 输出范围对称\n\n**缺点**：\n\n- 仍有梯度消失问题\n- 计算成本高\n\n**使用场景**：\n\n- 循环神经网络（RNN）\n- 需要零中心输出的场景\n\n---\n\n#### 激活函数对比表\n\n| 激活函数 | 范围 | 优点 | 缺点 | 主要用途 |\n|---------|------|------|------|---------|\n| **Sigmoid** | (0, 1) | 概率解释<br/>平滑 | 梯度消失<br/>计算慢 | 二分类输出层 |\n| **ReLU** | [0, ∞) | 快速<br/>缓解梯度消失 | 神经元死亡 | **隐藏层首选** |\n| **Tanh** | (-1, 1) | 零中心<br/>对称 | 梯度消失<br/>计算慢 | RNN |\n\n**经验法则**：\n\n- 隐藏层：**优先选 ReLU**\n- 输出层：\n  - 二分类 → Sigmoid\n  - 多分类 → Softmax（后续会学）\n  - 回归 → 不用激活（线性输出）\n\n---\n\n## 第三部分：多层感知机（MLP）\n\n---\n\n### 网络结构\n\n#### 从单个神经元到神经网络\n\n\n\n\n```{mermaid}\ngraph LR\n    subgraph 输入层\n    X1[特征1]\n    X2[特征2]\n    X3[特征3]\n    end\n    subgraph 隐藏层1\n    H11[神经元1]\n    H12[神经元2]\n    H13[神经元3]\n    H14[神经元4]\n    end\n    subgraph 隐藏层2\n    H21[神经元1]\n    H22[神经元2]\n    end\n    subgraph 输出层\n    Y[预测]\n    end\n\n    X1 --> H11\n    X1 --> H12\n    X1 --> H13\n    X1 --> H14\n    X2 --> H11\n    X2 --> H12\n    X2 --> H13\n    X2 --> H14\n    X3 --> H11\n    X3 --> H12\n    X3 --> H13\n    X3 --> H14\n\n    H11 --> H21\n    H11 --> H22\n    H12 --> H21\n    H12 --> H22\n    H13 --> H21\n    H13 --> H22\n    H14 --> H21\n    H14 --> H22\n\n    H21 --> Y\n    H22 --> Y\n\n    style X1 fill:#fff9c4\n    style X2 fill:#fff9c4\n    style X3 fill:#fff9c4\n    style H11 fill:#c8e6c9\n    style H12 fill:#c8e6c9\n    style H13 fill:#c8e6c9\n    style H14 fill:#c8e6c9\n    style H21 fill:#bbdefb\n    style H22 fill:#bbdefb\n    style Y fill:#4caf50,color:#fff\n    %% subgraph 背景色美化\n    style 输入层 fill:#f8f9fa,stroke:#e0e0e0\n    style 隐藏层1 fill:#f1f8e9,stroke:#aed581\n    style 隐藏层2 fill:#e3f2fd,stroke:#90caf9\n    style 输出层 fill:#fce4ec,stroke:#f06292\n```\n\n\n\n\n---\n\n#### 符号表示\n\n**输入层 → 隐藏层1 → 隐藏层2 → 输出层**\n\n- 输入：3 个特征\n- 隐藏层1：4 个神经元（ReLU）\n- 隐藏层2：2 个神经元（ReLU）\n- 输出层：1 个神经元（Sigmoid）\n\n**简写**：`Input(3) → [4] → [2] → Output(1)`\n\n---\n\n### 参数数量计算\n\n#### Titanic 例子\n\n```\nInput(8) → Dense(64, ReLU) → Dense(32, ReLU) → Output(1, Sigmoid)\n```\n\n**计算**：\n\n| 层 | 参数数量 | 计算 |\n|----|---------|------|\n| 输入 → 隐藏1 | 8×64 + 64 = **576** | 权重 + 偏置 |\n| 隐藏1 → 隐藏2 | 64×32 + 32 = **2,080** | 权重 + 偏置 |\n| 隐藏2 → 输出 | 32×1 + 1 = **33** | 权重 + 偏置 |\n| **总计** | **2,689** | |\n\n**对比**：\n\n- 决策树：只有分裂规则（少量参数）\n- MLP：**数千个参数需要学习**\n\n---\n\n### 为什么需要多层？\n\n#### 单层的局限性\n\n**单层神经网络**（= 逻辑回归）：\n\n- 只能学习**线性决策边界**\n- 无法解决 XOR 问题\n\n::: {#1cc25abe .cell execution_count=3}\n``` {.python .cell-code}\n# XOR 问题示例\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# XOR 数据\nX_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_xor = np.array([0, 1, 1, 0])\n\n# 单层无法分类\naxes[0].scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], s=200, c='red', marker='o', label='类别 0', edgecolors='k', linewidth=2)\naxes[0].scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], s=200, c='blue', marker='s', label='类别 1', edgecolors='k', linewidth=2)\naxes[0].plot([0, 1], [0.5, 0.5], 'k--', linewidth=2, alpha=0.5, label='线性边界？')\naxes[0].set_xlabel('x₁', fontsize=12)\naxes[0].set_ylabel('x₂', fontsize=12)\naxes[0].set_title('XOR 问题：单层网络无法解决', fontsize=14)\naxes[0].legend(fontsize=10)\naxes[0].set_xlim(-0.2, 1.2)\naxes[0].set_ylim(-0.2, 1.2)\naxes[0].grid(True, alpha=0.3)\n\n# 多层可以分类\nfrom sklearn.neural_network import MLPClassifier\nmlp_xor = MLPClassifier(hidden_layer_sizes=(4,), activation='relu', random_state=42, max_iter=5000)\nmlp_xor.fit(X_xor, y_xor)\n\nxx, yy = np.meshgrid(np.linspace(-0.2, 1.2, 100), np.linspace(-0.2, 1.2, 100))\nZ = mlp_xor.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\naxes[1].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\naxes[1].scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], s=200, c='red', marker='o', label='类别 0', edgecolors='k', linewidth=2)\naxes[1].scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], s=200, c='blue', marker='s', label='类别 1', edgecolors='k', linewidth=2)\naxes[1].set_xlabel('x₁', fontsize=12)\naxes[1].set_ylabel('x₂', fontsize=12)\naxes[1].set_title('XOR 问题：多层网络可以解决', fontsize=14)\naxes[1].legend(fontsize=10)\naxes[1].set_xlim(-0.2, 1.2)\naxes[1].set_ylim(-0.2, 1.2)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/dh/sd70vd1d0jg1tkw7h3zwjqxr0000gn/T/ipykernel_80177/3615795970.py:39: UserWarning:\n\nGlyph 8321 (\\N{SUBSCRIPT ONE}) missing from current font.\n\n/var/folders/dh/sd70vd1d0jg1tkw7h3zwjqxr0000gn/T/ipykernel_80177/3615795970.py:39: UserWarning:\n\nGlyph 8322 (\\N{SUBSCRIPT TWO}) missing from current font.\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nGlyph 8322 (\\N{SUBSCRIPT TWO}) missing from current font.\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nGlyph 8321 (\\N{SUBSCRIPT ONE}) missing from current font.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](w6_neural_networks_files/figure-html/cell-4-output-2.png){width=1142 height=470}\n:::\n:::\n\n\n---\n\n#### 多层的威力：层次特征学习\n\n\n\n\n```{mermaid}\ngraph TD\n    A[图像识别例子] --> B[第1层<br/>检测边缘]\n    B --> C[第2层<br/>检测形状]\n    C --> D[第3层<br/>检测物体部件]\n    D --> E[第4层<br/>识别完整物体]\n    \n    style B fill:#fff9c4\n    style C fill:#c8e6c9\n    style D fill:#bbdefb\n    style E fill:#4caf50,color:#fff\n```\n\n\n\n\n**关键洞察**：\n\n- 浅层：学习简单特征（边缘、纹理）\n- 深层：学习复杂特征（组合模式）\n- 这是**自动特征工程**！\n\n---\n\n#### 层数选择建议\n\n| 数据量 | 推荐层数 | 原因 |\n|--------|---------|------|\n| < 1万 | 1-2 层隐藏层 | 数据少，深层易过拟合 |\n| 1万-10万 | 2-3 层隐藏层 | 中等复杂度 |\n| > 10万 | 3-5 层隐藏层 | 大数据支持深层网络 |\n| > 100万 | 5+ 层 | 需要特殊技巧（下周讲） |\n\n**经验法则**：\n\n- 从简单开始（2层）\n- 如果欠拟合，增加层数或神经元数\n- 如果过拟合，减少复杂度或使用正则化（下周讲）\n\n---\n\n## 第四部分：前向传播与反向传播\n\n---\n\n### 前向传播（Forward Propagation）\n\n#### 从输入到输出的信息流\n\n\n\n\n```{mermaid}\ngraph LR\n    A[输入<br/>x] --> B[隐藏层1<br/>h₁ = ReLU W₁x+b₁]\n    B --> C[隐藏层2<br/>h₂ = ReLU W₂h₁+b₂]\n    C --> D[输出<br/>ŷ = σ W₃h₂+b₃]\n    \n    style A fill:#fff9c4\n    style B fill:#c8e6c9\n    style C fill:#bbdefb\n    style D fill:#4caf50,color:#fff\n```\n\n\n\n\n**逐层计算**：\n\n1. 输入：$x$ = [性别, 年龄, 票价, ...]\n2. 隐藏层1：$h_1 = \\text{ReLU}(W_1 x + b_1)$\n3. 隐藏层2：$h_2 = \\text{ReLU}(W_2 h_1 + b_2)$\n4. 输出：$\\hat{y} = \\sigma(W_3 h_2 + b_3)$\n\n**结果**：$\\hat{y}$ = 0.85（85% 存活概率）\n\n---\n\n### 损失函数（Loss Function）\n\n#### 衡量预测误差\n\n**二分类：Binary Cross-Entropy**\n\n$$\n\\text{Loss} = -\\frac{1}{n}\\sum_{i=1}^{n}\\left[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\right]\n$$\n\n**直观理解**：\n\n- 真实是 1，预测 0.9 → Loss 小（好）\n- 真实是 1，预测 0.1 → Loss 大（差）\n\n**类比**：Loss = 考试分数与满分的差距\n\n---\n\n#### 损失函数可视化\n\n::: {#0d84873c .cell execution_count=4}\n``` {.python .cell-code}\n# Binary Cross-Entropy 可视化\ny_true_demo = 1  # 真实标签\ny_pred_range = np.linspace(0.01, 0.99, 100)\nloss = -np.log(y_pred_range)\n\nplt.figure(figsize=(10, 6))\nplt.plot(y_pred_range, loss, linewidth=3, color='#f44336')\nplt.xlabel('预测概率 ŷ', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('Binary Cross-Entropy Loss\\n（真实标签 y=1 时）', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.axvline(x=1, color='green', linestyle='--', linewidth=2, label='完美预测')\nplt.axvline(x=0.5, color='orange', linestyle='--', linewidth=2, label='随机猜测')\nplt.legend(fontsize=11)\n\n# 标注\nplt.text(0.9, 0.5, 'ŷ → 1\\nLoss → 0\\n(预测对了)', fontsize=11, bbox=dict(boxstyle='round', facecolor='#c8e6c9'))\nplt.text(0.1, 4, 'ŷ → 0\\nLoss → ∞\\n(预测错了)', fontsize=11, bbox=dict(boxstyle='round', facecolor='#ffccbc'))\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nGlyph 375 (\\N{LATIN SMALL LETTER Y WITH CIRCUMFLEX}) missing from current font.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](w6_neural_networks_files/figure-html/cell-5-output-2.png){width=798 height=547}\n:::\n:::\n\n\n---\n\n### 反向传播（Backpropagation）\n\n#### 核心思想（不讲数学细节）\n\n**问题**：如何调整 W 和 b 来降低 Loss？\n\n**方法**：计算 Loss 对每个参数的\"贡献\"（梯度）\n\n\n\n\n```{mermaid}\ngraph RL\n    A[Loss] --> B[∂Loss/∂W₃<br/>∂Loss/∂b₃]\n    B --> C[∂Loss/∂W₂<br/>∂Loss/∂b₂]\n    C --> D[∂Loss/∂W₁<br/>∂Loss/∂b₁]\n    \n    style A fill:#f44336,color:#fff\n    style B fill:#ffccbc\n    style C fill:#fff9c4\n    style D fill:#c8e6c9\n```\n\n\n\n\n**更新规则**：\n\n$$\nW_{\\text{new}} = W_{\\text{old}} - \\eta \\times \\frac{\\partial \\text{Loss}}{\\partial W}\n$$\n\n- $\\eta$：学习率（步长）\n- $\\frac{\\partial \\text{Loss}}{\\partial W}$：梯度（方向）\n\n---\n\n#### 类比：登山下坡\n\n::: {#5115c837 .cell execution_count=5}\n``` {.python .cell-code}\n# 梯度下降可视化\nx = np.linspace(-2, 2, 100)\ny = x**2 + 1  # 简单的凸函数\n\nplt.figure(figsize=(12, 6))\nplt.plot(x, y, linewidth=3, color='#2196f3', label='Loss 曲线')\n\n# 梯度下降路径\nx_path = [1.8, 1.2, 0.7, 0.3, 0.1, 0.0]\ny_path = [xi**2 + 1 for xi in x_path]\n\nplt.plot(x_path, y_path, 'ro-', markersize=10, linewidth=2, label='梯度下降路径')\nplt.scatter([0], [1], s=300, c='green', marker='*', zorder=5, label='最优点')\n\nplt.xlabel('参数 W', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('梯度下降：沿着坡度下山', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.annotate('起点', xy=(1.8, y_path[0]), xytext=(2, 4.5), \n            arrowprops=dict(arrowstyle='->', color='red'), fontsize=11)\nplt.annotate('终点（最优）', xy=(0, 1), xytext=(0.5, 2), \n            arrowprops=dict(arrowstyle='->', color='green'), fontsize=11)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](w6_neural_networks_files/figure-html/cell-6-output-1.png){width=956 height=527}\n:::\n:::\n\n\n**类比**：\n\n- 做菜太咸（Loss 大） → 下次少放盐（调整参数）\n- 通过不断尝试，找到最佳配方（最优参数）\n\n---\n\n### 优化器（Optimizer）\n\n#### SGD vs Adam\n\n| 优化器 | 全称 | 特点 | 何时用 |\n|--------|------|------|--------|\n| **SGD** | Stochastic Gradient Descent | 最基础<br/>学习率固定 | 简单问题<br/>理论研究 |\n| **Adam** | Adaptive Moment Estimation | 自适应学习率<br/>收敛快 | **大多数情况首选** |\n\n**建议**：\n\n- 初学者：**直接用 Adam**\n- 效果不好再试 SGD\n\n---\n\n## 第五部分：MLP vs 树模型\n\n---\n\n### 何时用树模型，何时用 MLP？\n\n#### 对比表\n\n| 维度 | 树模型（RF/GBDT） | MLP |\n|------|------------|----------|\n| **适用数据** | 表格数据<br/>（行列结构） | 图像、文本、音频<br/>（非结构化） |\n| **特征工程** | 需要人工处理<br/>（编码、缺失值） | 自动学习特征 |\n| **训练速度** | 快（分钟级） | 慢（小时级） |\n| **调参难度** | 简单（3-5个参数） | 复杂（层数、神经元数、学习率...） |\n| **可解释性** | 高（特征重要性、可视化） | 低（黑盒） |\n| **样本量要求** | 小数据也可以（100+） | 需要大量数据（10000+） |\n| **过拟合** | 较容易控制 | 容易过拟合 |\n\n---\n\n#### 金融领域实际情况\n\n| 场景             | 数据类型     | 首选模型         | 原因     |\n|------------------|-------------|------------------|----------|\n| 贷款违约预测     | 表格        | GBDT             | 可解释   |\n| 欺诈检测         | 表格        | GBDT/RF          | 速度快   |\n| 客户流失         | 表格        | GBDT             | 可解释   |\n| 票据识别（OCR）  | 图像        | CNN              | 图像专用 |\n| 舆情分析         | 文本        | BERT             | 文本专用 |\n| 股票预测         | 时间序列    | LSTM/Transformer | 序列专用 |\n\n**核心结论**：\n\n- **表格数据（90%的金融任务）** → **树模型**\n- **图像/文本/音频** → 深度学习\n\n---\n\n## 总结\n\n---\n\n### 本讲知识回顾\n\n#### 神经元模型\n\n- 生物神经元 → 人工神经元\n- 加权求和 + 激活函数\n\n#### 激活函数\n\n- **Sigmoid**：输出层（二分类）\n- **ReLU**：隐藏层首选\n- **Tanh**：RNN 专用\n\n#### 多层感知机\n\n- 输入层 → 隐藏层 → 输出层\n- 参数：权重 W + 偏置 b\n- 多层 → 学习复杂非线性模式\n\n---\n\n#### 训练过程\n\n- **前向传播**：输入 → 输出\n- **损失函数**：衡量误差\n- **反向传播**：计算梯度\n- **优化器**：更新参数（推荐 Adam）\n\n#### MLP vs 树模型\n\n| 场景 | 推荐模型 |\n|------|---------|\n| 表格数据 | 树模型 |\n| 图像 | CNN |\n| 文本 | Transformer |\n| 音频 | RNN/CNN |\n\n---\n\n### 核心要点\n\n1. **深度学习不神秘**  \n   基于简单的数学运算累积\n\n2. **激活函数很重要**  \n   隐藏层用 ReLU，输出层看任务\n\n3. **归一化是必须的**  \n   神经网络对数据尺度敏感\n\n4. **表格数据树模型更好**  \n   不要迷信深度学习\n\n5. **Keras 很简单**  \n   Sequential API 易上手\n\n---\n\n## Q&A\n\n**Q1：人工神经元是如何模拟生物神经元的？请写出人工神经元的基本数学模型。**\n\n**A：**\n人工神经元模拟生物神经元“接收信号-整合处理-激活发火”的过程。\n\n它的数学模型是：\n\n1.  加权求和： $z = \\sum_{i=1}^{n} w_i x_i + b$ \n2.  激活函数： $y = f(z)$ \n（其中 $x_i$ 是输入特征，$w_i$ 是权重，$b$ 是偏置，$f$ 是激活函数）\n\n---\n\n**Q2：为什么神经网络中必须使用“非线性”激活函数（如 ReLU）？如果只用线性激活（或不用激活），多层网络会退化成什么？**\n\n**A：**\n\n* 原因：激活函数的作用是引入非线性，使得网络能够学习和拟合复杂的非线性模式（例如 XOR 问题）。\n* 退化：如果只用线性激活，多层线性叠加的结果仍然是线性的（$W'x + b'$）。这会导致整个深度神经网络退化成一个单层的线性模型（如逻辑回归），失去深度学习的意义。\n\n---\n\n**Q3：为神经网络的“隐藏层”和“二分类输出层”选择激活函数时，首选的推荐分别是什么？**\n\n**A：**\n\n* 隐藏层首选：ReLU。因为它计算快，并能有效缓解梯度消失问题。\n* 二分类输出层首选：Sigmoid。因为它的输出范围是 (0, 1) ，可以将输出结果解释为概率。\n\n---\n\n**Q4：请简要描述“前向传播”（Forward Propagation）和“反向传播”（Backward Propagation）在神经网络训练中的角色。**\n\n**A：**\n\n* 前向传播：是指数据从输入层开始，逐层计算，直到在输出层得到预测值（$\\hat{y}$）的过程。\n* 反向传播：是指在得到预测值并计算出 Loss（误差）后，从输出层反向逐层计算 Loss 对每个参数（W 和 b）的梯度（贡献度）的过程。\n\n---\n\n**Q5：为什么 ReLU ($\\max(0, z)$) 作为隐藏层激活函数，通常优于 Sigmoid？Sigmoid 用在隐藏层时有什么主要缺陷？**\n\n**A：**\nReLU 之所以更优，主要是因为它解决了 Sigmoid 的一个主要缺陷：梯度消失（Gradient Vanishing）。\n\n* Sigmoid 的缺陷：Sigmoid 函数在输入值（z）很大或很小时，其曲线非常平坦，导致梯度（导数）接近于 0。在反向传播时，这些接近 0 的梯度逐层相乘，导致深层网络的梯度变得极小，使得参数几乎无法更新。\n* ReLU 的优势：当输入 $z > 0$ 时，ReLU 的梯度恒为 1。这使得梯度在反向传播时能够更顺畅地流动，极大地缓解了梯度消失问题，让网络训练得更快、更深。\n\n---\n\n**Q6：假设一个 MLP 网络结构为 `Input(10) -> Dense(32, ReLU) -> Output(1, Sigmoid)`。请问从 Input 层到 Dense(32) 层（第一个隐藏层）总共有多少个可训练参数？**\n\n**A：**\n可训练参数 = 权重（Weights）+ 偏置（Biases）。\n\n1.  权重 (W)：每个输入神经元都连接到每个隐藏神经元。\n    * 计算：`输入维度 × 隐藏层维度` = $10 \\times 32 = 320$ 个。\n2.  偏置 (b)：隐藏层中的每个神经元都有 1 个偏置项。\n    * 计算：`隐藏层维度` = $32$ 个。\n3.  总计： $320 + 32 = \\mathbf{352}$ 个可训练参数。\n\n---\n\n**Q7：在反向传播中，我们计算出 Loss 对权重 W 的梯度（$\\frac{\\partial \\text{Loss}}{\\partial W}$）后，是如何更新这个权重 W 的？（请写出更新规则）。在这个规则中，“学习率（$\\eta$）”扮演了什么角色？**\n\n**A：**\n\n* 更新规则： $W_{\\text{new}} = W_{\\text{old}} - \\eta \\times \\frac{\\partial \\text{Loss}}{\\partial W}$ 。\n* 学习率（$\\eta$）的角色：学习率（Learning Rate）扮演着“步长”的角色。它控制了模型在“下山”（降低 Loss）时每一步迈多大。\n    *  $\\eta$ 太大：可能会导致“步子迈太大”，越过最低点，导致 Loss 震荡或发散。\n    *  $\\eta$ 太小：会导致“小碎步”下山，训练速度过慢，可能需要很久才能收敛。\n\n---\n\n**Q8：对于绝大多数金融场景中的“表格数据”（像一个 Excel 表格、数据库表类型的数据），推荐首选哪一类模型（树模型还是MLP）？为什么？**\n\n**A：**\n\n* 首选模型：树模型（特别是 GBDT）。\n* 原因：对于表格数据，树模型通常训练速度更快、调参更简单、可解释性更强，并且在中小规模的数据集上（金融领域很常见）表现往往优于需要大量数据和复杂调参的 MLP。\n\n---\n\n**Q9：一个金融科技团队想用 MLP（神经网络）来预测客户流失，他们的数据集只有 5000 行（样本）和 20 个特征（表格数据）。你认为他们应该优先使用 MLP 吗？为什么？**\n\n**A：** 不应该优先使用 MLP。\n\n理由如下：\n\n* 数据量太小：MLP 通常需要大量数据（例如 1 万条以上）才能学习其数千个参数并防止过拟合。5000 行数据对于 MLP 来说太少了。\n* 树模型更优：对于这种中小型表格数据，树模型（如 GBDT 或 随机森林）通常表现更好，并且训练更快、可解释性更强（这对业务很重要）。\n\n",
    "supporting": [
      "w6_neural_networks_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}