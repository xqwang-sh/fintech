{
  "hash": "4d38f891ae173a9b2f3afb37b12af6a1",
  "result": {
    "engine": "jupyter",
    "markdown": "# 第五讲：集成学习 - 随机森林与 GBDT\n\n::: {#bc0248e8 .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n# 根据操作系统设置不同的字体\nimport platform\n\n# 获取操作系统类型\nsystem = platform.system()\n\n# 设置 matplotlib 字体\nif system == 'Windows':\n    plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows 使用黑体\nelif system == 'Darwin':\n    plt.rcParams['font.sans-serif'] = ['Songti SC']  # Mac 使用宋体\nelse:\n    plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei']  # Linux 使用文泉驿正黑\n\n# 解决负号显示问题\nplt.rcParams['axes.unicode_minus'] = False\n```\n:::\n\n\n# 开场：单棵树的困境\n\n---\n\n## 上周回顾\n\n**决策树**：\n\n- 易于理解和可视化\n- 能处理非线性问题\n- 但有一个致命问题...\n\n---\n\n## 过拟合演示\n\n::: {#342346cf .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# 模拟数据\nnp.random.seed(42)\nX = np.random.rand(100, 2) * 10\ny = ((X[:, 0] > 5) & (X[:, 1] > 5)).astype(int)\n# 添加噪声\nnoise_idx = np.random.choice(100, 10, replace=False)\ny[noise_idx] = 1 - y[noise_idx]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 训练不同深度的树\ndepths = [2, 5, 10, None]\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\nfor idx, depth in enumerate(depths):\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    tree.fit(X_train, y_train)\n    \n    train_acc = accuracy_score(y_train, tree.predict(X_train))\n    test_acc = accuracy_score(y_test, tree.predict(X_test))\n    \n    # 决策边界\n    xx, yy = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 10, 100))\n    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n    \n    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n    axes[idx].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', edgecolors='k', s=50)\n    axes[idx].set_title(f'深度={depth}\\n训练={train_acc:.2f} 测试={test_acc:.2f}')\n    axes[idx].set_xlabel('特征 1')\n    axes[idx].set_ylabel('特征 2')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](w5_ensemble_learning_files/figure-html/cell-3-output-1.png){width=1527 height=376}\n:::\n:::\n\n\n**观察**：\n\n- 深度=2：欠拟合（训练和测试都差）\n- 深度=5：恰好（训练和测试接近）\n- 深度=10 或无限制：**过拟合**（训练完美，测试很差）\n\n---\n\n## 本周的解决方案\n\n**核心思想**：\"三个臭皮匠，顶个诸葛亮\"\n\n不再依赖单棵树，而是：\n\n1. 训练多棵不同的树\n2. 让它们共同决策（投票/平均）\n3. 减少过拟合，提高稳定性\n\n这就是 **集成学习（Ensemble Learning）**\n\n---\n\n## 本周学习目标\n\n### 知识目标\n1. 理解集成学习的核心思想\n2. 掌握 Bagging 和 Boosting 的本质区别\n3. 了解随机森林的工作原理\n4. 了解 GBDT 的工作原理\n5. 理解关键超参数的作用\n\n### 技能目标\n1. 使用 sklearn 训练随机森林\n2. 使用 LightGBM 训练 GBDT\n3. 进行超参数调整\n4. 对比多个模型性能\n5. 绘制特征重要性图\n\n---\n\n# 第一部分：集成学习核心思想\n\n---\n\n## 类比：专家团队 vs 单个专家\n\n### 单个专家（单棵树）\n\n- 可能有盲点\n- 受个人经验限制\n- 容易出错\n\n### 专家团队（多棵树）\n\n- 多个视角\n- 互补长短\n- 投票决策，更可靠\n\n---\n\n## 集成学习的数学直觉\n\n假设每个模型准确率 60%（略强于随机猜测 50%）\n\n- **1 个模型**：准确率 60%\n- **3 个模型投票**：至少 2 个正确的概率 ≈ 65%\n- **5 个模型投票**：至少 3 个正确的概率 ≈ 68%\n- **更多模型**：准确率持续提升\n\n**前提**：各模型的错误要**独立**（不能都在同一个地方出错）\n\n---\n\n## 如何保证模型\"不一样\"？\n\n\n\n\n```{mermaid}\ngraph TD\n    A[如何让多个模型不同?] --> B[Bagging<br/>数据随机]\n    A --> C[Boosting<br/>关注错误]\n    \n    B --> B1[每个模型用不同的训练数据]\n    B --> B2[代表: 随机森林]\n    \n    C --> C1[后面的模型专注于<br/>前面模型的错误]\n    C --> C2[代表: GBDT/XGBoost]\n    \n    style B fill:#c8e6c9\n    style C fill:#bbdefb\n```\n\n\n\n\n---\n\n# 第二部分：Bagging 与随机森林\n\n---\n\n## 2.1 Bagging 原理\n\n**Bagging = Bootstrap Aggregating**\n\n---\n\n### Step 1: Bootstrap（自助采样）\n\n**问题**：只有一份训练数据，如何创造多份不同的数据？\n\n**方法**：有放回地随机抽样\n\n::: {#efa89653 .cell execution_count=3}\n``` {.python .cell-code}\n# 演示 Bootstrap\nnp.random.seed(42)\noriginal_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\naxes[0].bar(range(len(original_data)), original_data, color='skyblue')\naxes[0].set_title('原始数据 (10个样本)', fontsize=12)\naxes[0].set_xlabel('索引')\naxes[0].set_ylabel('值')\n\nfor i in range(1, 4):\n    bootstrap_sample = np.random.choice(original_data, size=10, replace=True)\n    axes[i].bar(range(len(bootstrap_sample)), bootstrap_sample, alpha=0.7)\n    axes[i].set_title(f'Bootstrap 样本 {i}', fontsize=12)\n    axes[i].set_xlabel('索引')\n    axes[i].set_ylabel('值')\n    axes[i].set_ylim(0, 11)\n    \n    # 标注哪些是重复的\n    unique_count = len(np.unique(bootstrap_sample))\n    axes[i].text(0.5, 10, f'唯一值: {unique_count}/10', fontsize=10, ha='center')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](w5_ensemble_learning_files/figure-html/cell-4-output-1.png){width=1527 height=377}\n:::\n:::\n\n\n**特点**：\n\n- 每个 Bootstrap 样本大小与原数据相同\n- 某些样本会被重复选中\n- 约 63% 的唯一样本（其余是重复）\n\n---\n\n### Step 2: Aggregating（聚合）\n\n\n\n\n```{mermaid}\ngraph TD\n    A[原始训练数据<br/>1000个样本] --> B[Bootstrap 1]\n    A --> C[Bootstrap 2]\n    A --> D[Bootstrap 3]\n    A --> E[...]\n    A --> F[Bootstrap M]\n    \n    B --> G[树1]\n    C --> H[树2]\n    D --> I[树3]\n    E --> J[...]\n    F --> K[树M]\n    \n    G --> L[投票/平均]\n    H --> L\n    I --> L\n    J --> L\n    K --> L\n    \n    L --> M[最终预测]\n    \n    style A fill:#fff9c4\n    style L fill:#c8e6c9\n    style M fill:#4caf50,color:#fff\n```\n\n\n\n\n---\n\n### Bagging 完整流程\n\n1. **重复 M 次**：\n   - 从训练集中 Bootstrap 采样\n   - 在该样本上训练一棵树\n   \n2. **预测时**：\n   - 分类：M 棵树投票，多数票获胜\n   - 回归：M 棵树预测值的平均\n\n3. **为什么有效**：\n   - 每棵树看到不同的数据\n   - 每棵树的误差方向不同\n   - 投票后误差相互抵消\n\n---\n\n## 2.2 随机森林（Random Forest）\n\n**随机森林 = Bagging + 特征随机**\n\n---\n\n### 额外的随机性：特征随机选择\n\n**问题**：如果某个特征特别强（如 Titanic 中的性别），所有树都会优先用它分裂 → 树之间太相似\n\n**解决**：每次分裂时，只考虑 **随机选择的一部分特征**\n\n---\n\n### 特征随机演示\n\n\n\n\n```{mermaid}\ngraph TD\n    A[9个特征] --> B[每次分裂时]\n    B --> C[随机选3个特征<br/>√9≈3]\n    \n    C --> D[树1选: 特征2, 5, 8]\n    C --> E[树2选: 特征1, 3, 6]\n    C --> F[树3选: 特征4, 7, 9]\n    \n    D --> G[树之间更不同]\n    E --> G\n    F --> G\n    \n    G --> H[集成效果更好]\n    \n    style C fill:#fff9c4\n    style H fill:#4caf50,color:#fff\n```\n\n\n\n\n**常用规则**：\n\n- 分类：每次考虑 $\\sqrt{p}$ 个特征（p = 总特征数）\n- 回归：每次考虑 $p/3$ 个特征\n\n---\n\n## 随机森林关键参数\n\n| 参数 | 含义 | 默认值 | 调参建议 |\n|------|------|--------|----------|\n| `n_estimators` | 树的数量 | 100 | 越多越好（但速度慢）<br/>50-200 |\n| `max_depth` | 每棵树的最大深度 | None | 通常 5-20<br/>防止过拟合 |\n| `max_features` | 每次分裂考虑的特征数 | √p | 可尝试 log₂(p) |\n| `min_samples_split` | 分裂所需最小样本数 | 2 | 增大防止过拟合<br/>5-20 |\n| `min_samples_leaf` | 叶节点最小样本数 | 1 | 增大防止过拟合<br/>2-10 |\n\n---\n\n## 随机森林 vs 单棵决策树\n\n| 维度 | 决策树 | 随机森林 |\n|------|--------|----------|\n| **偏差** | 低 | 低 |\n| **方差** | 高（易过拟合） | **低（稳定）** ✓ |\n| **训练速度** | 快 | 慢（M 棵树） |\n| **预测速度** | 快 | 慢（M 次预测） |\n| **可解释性** | 高 | 低（黑盒） |\n| **准确率** | 中 | **高** ✓ |\n\n**核心优势**：**降低方差，防止过拟合**\n\n---\n\n# 第三部分：Boosting 与 GBDT\n\n---\n\n## 3.1 Boosting 核心思想\n\n**Bagging**：独立专家各自诊断，最后投票\n\n**Boosting**：第一个医生诊断，第二个医生专门看第一个医生漏掉的症状\n\n---\n\n### Bagging vs Boosting\n\n\n\n\n```{mermaid}\ngraph TD\n    A[Bagging] --> A1[并行训练]\n    A --> A2[数据: Bootstrap]\n    A --> A3[目标: 降低方差]\n    A --> A4[代表: 随机森林]\n    \n    B[Boosting] --> B1[串行训练]\n    B --> B2[数据: 加权/残差]\n    B --> B3[目标: 降低偏差]\n    B --> B4[代表: GBDT/XGBoost]\n    \n    style A fill:#c8e6c9\n    style B fill:#bbdefb\n```\n\n\n\n\n---\n\n### Boosting 流程\n\n\n\n\n```{mermaid}\ngraph LR\n    A[训练集] --> B[树1]\n    B --> C[找出错误样本]\n    C --> D[树2<br/>专注错误]\n    D --> E[再找错误]\n    E --> F[树3<br/>继续修正]\n    F --> G[...]\n    G --> H[组合所有树]\n    \n    style B fill:#fff9c4\n    style D fill:#ffccbc\n    style F fill:#ffccbc\n    style H fill:#4caf50,color:#fff\n```\n\n\n\n\n**关键**：后面的树专注于前面树的错误\n\n---\n\n## 3.2 GBDT 原理\n\n**GBDT = Gradient Boosting Decision Tree**\n\n**核心思想**：每棵新树去拟合之前所有树的\"预测误差\"（残差）\n\n---\n\n### GBDT 示例（回归）\n\n**数据**：真实房价 [100, 150, 200]\n\n**Step 1**：初始预测 = 平均值 = 150\n\n```\n真实值: [100, 150, 200]\n预测值: [150, 150, 150]\n残差:   [-50,   0,  50]\n```\n\n**Step 2**：训练树1 拟合残差 [-50, 0, 50]\n\n假设树1学到：`预测残差 = [-45, 0, 45]`\n\n更新预测 = 150 + 0.1 × [-45, 0, 45] = [145.5, 150, 154.5]\n\n（0.1 是学习率）\n\n---\n\n**Step 3**：计算新残差\n\n```\n真实值: [100, 150, 200]\n预测值: [145.5, 150, 154.5]\n新残差: [-45.5,   0,  45.5]\n```\n\n**Step 4**：训练树2 拟合新残差，继续更新...\n\n---\n\n### GBDT 可视化\n\n::: {#5db64ad6 .cell execution_count=4}\n``` {.python .cell-code}\n# GBDT 逐步修正示意\nnp.random.seed(42)\nX_demo = np.linspace(0, 10, 50).reshape(-1, 1)\ny_demo = 2*X_demo.flatten() + 3 + np.random.randn(50)*2\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\nstages = [1, 5, 20, 100]\nfor idx, n_estimators in enumerate(stages):\n    ax = axes[idx//2, idx%2]\n    \n    gbdt = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=0.1, \n                                      max_depth=3, random_state=42)\n    gbdt.fit(X_demo, y_demo)\n    y_pred = gbdt.predict(X_demo)\n    \n    ax.scatter(X_demo, y_demo, alpha=0.5, label='真实数据')\n    ax.plot(X_demo, y_pred, 'r-', linewidth=2, label=f'{n_estimators}棵树')\n    ax.set_xlabel('X')\n    ax.set_ylabel('y')\n    ax.set_title(f'GBDT - {n_estimators} 棵树')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](w5_ensemble_learning_files/figure-html/cell-5-output-1.png){width=1336 height=954}\n:::\n:::\n\n\n**观察**：随着树数量增加，拟合越来越精确\n\n---\n\n## GBDT 关键参数\n\n| 参数 | 含义 | 默认值 | 调参建议 |\n|------|------|--------|----------|\n| `n_estimators` | 树的数量 | 100 | 100-1000<br/>配合 learning_rate |\n| `learning_rate` | 学习率 | 0.1 | **0.01-0.3**<br/>越小需要越多树 |\n| `max_depth` | 树的深度 | 3 | 3-8<br/>通常用**浅树** |\n| `subsample` | 每次迭代用多少数据 | 1.0 | 0.8-1.0<br/>防止过拟合 |\n\n---\n\n### 学习率的作用\n\n::: {#1a278654 .cell execution_count=5}\n``` {.python .cell-code}\n# 学习率对比\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nlearning_rates = [0.01, 0.1, 0.5]\n\nfor idx, lr in enumerate(learning_rates):\n    ax = axes[idx]\n    \n    train_scores = []\n    test_scores = []\n    \n    for n in range(1, 101):\n        gbdt = GradientBoostingRegressor(n_estimators=n, learning_rate=lr, \n                                          max_depth=3, random_state=42)\n        gbdt.fit(X_demo, y_demo)\n        train_scores.append(gbdt.score(X_demo, y_demo))\n    \n    ax.plot(range(1, 101), train_scores, label=f'lr={lr}')\n    ax.set_xlabel('树的数量')\n    ax.set_ylabel('R² 分数')\n    ax.set_title(f'学习率 = {lr}')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](w5_ensemble_learning_files/figure-html/cell-6-output-1.png){width=1431 height=473}\n:::\n:::\n\n\n**规律**：\n\n- lr 小（0.01）：收敛慢，需要更多树，但最终可能更好\n- lr 大（0.5）：收敛快，但容易过拟合\n\n---\n\n# 总结\n\n---\n\n## 本讲知识回顾\n\n### 集成学习\n\n- **核心思想**：\"三个臭皮匠顶个诸葛亮\"\n- **关键**：让多个模型\"不一样\"\n\n### Bagging（随机森林）\n\n- **方法**：Bootstrap 采样 + 特征随机\n- **优点**：降低方差，防止过拟合\n- **适用**：数据噪声大、需要稳定模型\n\n### Boosting（GBDT）\n\n- **方法**：串行训练，逐步修正错误\n- **优点**：降低偏差，精度高\n- **适用**：数据干净、追求极致精度\n\n---\n\n## Bagging vs Boosting 总结\n\n| 维度 | Bagging（随机森林） | Boosting（GBDT） |\n|------|---------------------|------------------|\n| **训练方式** | 并行 | 串行 |\n| **数据** | Bootstrap | 加权/残差 |\n| **降低** | 方差（过拟合） | 偏差（欠拟合） |\n| **速度** | 可并行，较快 | 串行，较慢 |\n| **鲁棒性** | 对异常值鲁棒 | 对异常值敏感 |\n| **调参** | 较简单 | 较复杂 |\n| **何时用** | 数据噪声大 | 数据干净、追求精度 |\n\n**选择建议**：\n\n- 不确定 → **都试试**\n- 需要稳定 → 随机森林\n- 追求精度 → GBDT\n\n---\n\n## 实战要点\n\n1. **先用默认参数跑通**  \n   了解基线性能\n\n2. **对比多个模型**  \n   决策树 vs 随机森林 vs GBDT\n\n3. **查看特征重要性**  \n   了解哪些特征重要\n\n4. **调参时控制变量**  \n   一次只改一个参数\n\n5. **警惕过拟合**  \n   训练集和测试集差距过大是警报\n\n---\n\n# Q&A\n\n**Q1：集成学习（Ensemble Learning）试图解决单棵决策树的什么核心问题？**\n\n**A：**\n核心问题是过拟合（Overfitting）。单棵决策树如果深度没有限制，很容易学到训练数据中的噪声，导致训练集表现完美，但测试集表现很差。集成学习通过组合多棵树的决策来降低这种过拟合，提高模型的稳定性和泛化能力。\n\n---\n\n**Q2：Bagging（例如随机森林）和 Boosting（例如 GBDT）在训练模型时，最核心的区别是什么？**\n\n**A：**\n最核心的区别在于模型的训练方式：\n\n* Bagging (并行)：像一个“专家团队”，每棵树（专家）独立并行地在不同的数据子集上训练，最后“投票”决定结果。\n* Boosting (串行)：像一个“师徒体系”，树（学徒）是串行训练的，后一棵树的主要任务是专注于修正前一棵树犯下的错误（残差）。\n\n---\n\n**Q3：随机森林（Random Forest）中的“随机”体现在哪两个方面？**\n\n**A：**\n体现在两个方面：\n\n1. 数据随机（Bootstrap）：每棵树的训练数据都是从原始数据集中有放回地随机抽样（Bootstrap）得到的。\n2. 特征随机：在构建每棵树的每个分裂节点时，并不会考虑所有特征，而是随机选择一部分特征（例如 $\\sqrt{p}$ 个）作为候选，再从中选择最好的分裂点。\n\n---\n\n**Q4：GBDT (梯度提升决策树) 的核心思想是什么？（即，后一棵树是如何“修正”前一棵树的？）**\n\n**A：**\nGBDT 的核心思想是拟合残差（Residuals）。\n\n1.  模型从一个初始预测（例如平均值）开始。\n2.  第一棵树训练的目标是拟合真实值与初始预测之间的误差（残差）。\n3.  第二棵树训练的目标是拟合真实值与“初始预测+第一棵树预测”之间的新残差。\n4.  以此类推，每棵新树都在逐步修正前面所有树累积下来的预测误差。\n\n---\n\n**Q5：Bagging（随机森林）和 Boosting（GBDT）分别主要致力于降低哪种误差（偏差 vs 方差）？**\n\n**A：**\n\n* Bagging (随机森林) 主要通过平均/投票来抵消噪声，降低方差（Variance），解决的是过拟合问题。\n* Boosting (GBDT) 主要通过不断修正错误来提高模型精度，降低偏差（Bias），解决的是欠拟合问题。\n\n---\n\n**Q6：为什么随机森林在“数据随机”之外，还需要引入“特征随机”？**\n\n**A：**\n这是为了“去相关性”，保证树的多样性。\n\n* 问题：假设数据中有一个特征（例如“性别”）特别重要，如果 Bagging 时每棵树都能看到所有特征，那么几乎每棵树都会在根节点附近使用这个强特征进行分裂。\n* 后果：这会导致所有树的结构都非常相似，它们会犯同样的错误。\n* 解决：“特征随机”强迫一些树在分裂时“看不到”那个强特征，必须寻找其他次优特征来分裂。这使得每棵树长得更不一样，它们之间的相关性降低，集成“投票”时的纠错能力更强，最终的集成效果更好。\n\n---\n\n**Q7：在 GBDT 中，`learning_rate`（学习率）和 `n_estimators`（树的数量）之间是什么关系？我们为什么通常倾向于使用“较小”的学习率？**\n\n**A：**\n\n* 关系：它们是权衡（Trade-off）关系。`learning_rate` 控制了每棵树修正错误的“步长”。\n    * 高 `learning_rate` (如 0.5)：收敛快，但可能“冲过头”导致过拟合，需要的 `n_estimators` 较少。\n    * 低 `learning_rate` (如 0.01)：收敛慢，模型拟合更精细，需要更多的 `n_estimators` (树) 才能达到同样的拟合程度，但通常泛化能力更强。\n* 为何用较小的 LR：我们倾向于用“较小”的学习率（如 0.01-0.1）配合“较多”的树（`n_estimators`），这被称为“Shrinkage”。这使得模型在修正错误的道路上“小步慢走”，防止在训练过程中过早地过拟合，从而找到一个更稳健、泛化能力更强的最终模型。\n\n---\n\n**Q8：为什么 Boosting (GBDT) 对异常值（Outliers）比 Bagging (随机森林) 更敏感？**\n\n**A：**\n* Boosting (GBDT)：因为 GBDT 的核心机制是串行地关注错误（残差）。异常值（Outlier）通常会产生巨大的残差。在后续的迭代中，GBDT 会越来越“专注”地试图去拟合这个由异常值导致的巨大残差，这会扭曲整个模型，导致模型为了迁就一个异常点而牺牲了整体的泛化能力。\n* Bagging (随机森林)：随机森林是并行地，且最后通过投票或平均来聚合结果。异常值可能只会影响到少数（包含该异常值的 Bootstrap 样本）树的决策，但在最终的“民主投票”中，这些少数树的错误决策很容易被其他大量正确的树“淹没”或“平均掉”，因此模型整体表现更稳健（鲁棒）。\n\n---\n\n**Q9：在 GBDT 和随机森林中，我们通常对单棵树的 `max_depth`（最大深度）有截然不同的设置（一个深一个浅）。请问哪个模型通常使用“浅树”，哪个使用“深树”，为什么？**\n\n**A：**\n\n* GBDT (Boosting)：通常使用“浅树”（例如 `max_depth` = 3 到 8）。\n    * 原因：GBDT 的强大之处在于“集成”而非“单树”。它依靠大量简单的（高偏差、低方差）浅树串行叠加，逐步降低整体模型的偏差。如果单棵树太深（低偏差、高方差），模型会过快地过拟合，失去 Boosting 的优势。\n* 随机森林 (Bagging)：通常使用“深树”（例如 `max_depth` = None 或 10-20），让树充分生长。\n    * 原因：随机森林的目标是降低方差。它希望每棵树都是一个“低偏差、高方差”的“聪明但偏科”的专家。通过让每棵树充分生长（深树）来保证“低偏差”（拟合能力强）。然后通过 Bagging 和特征随机来保证树之间的差异性，最后通过“投票”来消除“高方差”，实现整体的低方差。\n\n",
    "supporting": [
      "w5_ensemble_learning_files"
    ],
    "filters": [],
    "includes": {}
  }
}