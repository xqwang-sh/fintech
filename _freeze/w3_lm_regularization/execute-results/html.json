{
  "hash": "64a10f5d803a4726779fd98ba5a0986d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 第三讲：线性回归的正则化\n---\n\n\n\n## 开场：一个问题\n\n---\n\n### 当模型\"太聪明\"时...\n\n**场景**：你训练了一个房价预测模型\n\n| 数据集 | RMSE | 表现 |\n|--------|------|------|\n| 训练集 | 5万 | ✓ 很好 |\n| 测试集 | 50万 | ✗ 糟糕 |\n\n**问题出在哪里？**\n\n::: {.fragment}\n🚨 **过拟合（Overfitting）**：模型记住了训练数据的细节（包括噪声），但不能泛化到新数据！\n:::\n\n---\n\n### 本周学习目标\n\n#### 知识目标\n1. 理解过拟合与欠拟合的概念\n2. 理解正则化的基本思想（限制模型复杂度）\n3. 掌握 Ridge、Lasso、Elastic Net 的区别和应用场景\n4. 了解交叉验证的作用\n\n#### 技能目标\n1. 使用 sklearn 训练正则化回归模型\n2. 使用交叉验证选择最优正则化参数\n3. 绘制正则化路径图\n4. 解释 Lasso 的特征选择结果\n\n#### 核心理念\n\n**\"简单的模型往往更好\"（Occam's Razor）**  \n复杂度与泛化能力的权衡是机器学习的核心！\n\n---\n\n## 第一部分：过拟合与欠拟合\n\n---\n\n### 用多项式回归理解过拟合\n\n#### 真实场景模拟\n\n假设真实关系是： $y = 2x + 1 + \\epsilon$，其中 $\\epsilon$ 是噪声。\n\n我们用不同次数的多项式拟合：\n\n::: {#96bebd4e .cell fig-height='5' fig-width='14' execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](w3_lm_regularization_files/figure-html/cell-3-output-1.png){width=1430 height=471}\n:::\n:::\n\n\n---\n\n### 欠拟合 vs 过拟合 vs 刚刚好\n\n| 模型状态   | 模型复杂度 | 训练集误差 | 测试集误差 | 问题         | 表现                  |\n|:--------:|:---------:|:----------:|:----------:|:-----------:|:---------------------:|\n| 欠拟合    | 太简单    | 大         | 大         | 无法学到规律 | 🙁 差                 |\n| 刚刚好    | 适中      | 小         | 小         | -           | 😊 好                 |\n| 过拟合    | 太复杂    | 非常小     | 大         | 记住了噪声   | 😱 看起来好但实际差    |\n\n**关键观察**：\n\n- 欠拟合：训练和测试都差\n- 过拟合：训练很好，测试很差（**差距大**是关键信号）\n- 刚刚好：训练和测试都还不错，且差距小\n\n---\n\n### Bias-Variance Tradeoff（偏差-方差权衡）\n\n::: {#abc6dbc8 .cell fig-height='6' fig-width='10' execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](w3_lm_regularization_files/figure-html/cell-4-output-1.png){width=951 height=569}\n:::\n:::\n\n\n**直观理解**：\n \n- **偏差（Bias）**：模型的\"先天不足\"，太简单无法拟合真实关系\n- **方差（Variance）**：模型对训练数据的\"过度敏感\"，训练数据稍有变化预测就变很多\n- **目标**：找到偏差和方差都较低的平衡点\n\n---\n\n## 第二部分：正则化的直觉\n\n---\n\n### 什么是正则化？\n\n#### 问题\n\n**普通线性回归**的目标：\n$$\n\\min \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\min \\text{MSE}\n$$\n\n只关心**拟合训练数据**，不管模型复杂度！\n\n#### 正则化的想法\n\n**在损失函数中加入惩罚项**：\n$$\n\\min \\underbrace{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}_{\\text{拟合误差}} + \\underbrace{\\alpha \\times \\text{惩罚项}}_{\\text{复杂度惩罚}}\n$$\n\n- **拟合误差**：让模型拟合数据\n- **复杂度惩罚**：防止模型过于复杂\n- **α（alpha）**：权衡两者的超参数\n\n---\n\n### 正则化的类比\n\n**类比 1：给模型\"戴镣铐\"**\n\n- 不正则化 = 模型完全自由，可能\"乱跑\"（过拟合）\n- 正则化 = 给模型加限制，让它\"老实点\"\n\n**类比 2：奥卡姆剃刀（Occam's Razor）**\n\n> \"如无必要，勿增实体\"\n\n- 在同等拟合效果下，选择更简单的模型\n- 简单模型 = 系数小、特征少\n\n---\n\n### 正则化的几何直觉\n\n::: {#f49ab261 .cell fig-height='5' fig-width='12' execution_count=4}\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/dh/sd70vd1d0jg1tkw7h3zwjqxr0000gn/T/ipykernel_80120/3492466871.py:40: UserWarning:\n\nGlyph 8321 (\\N{SUBSCRIPT ONE}) missing from current font.\n\n/var/folders/dh/sd70vd1d0jg1tkw7h3zwjqxr0000gn/T/ipykernel_80120/3492466871.py:40: UserWarning:\n\nGlyph 8322 (\\N{SUBSCRIPT TWO}) missing from current font.\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nGlyph 8322 (\\N{SUBSCRIPT TWO}) missing from current font.\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning:\n\nGlyph 8321 (\\N{SUBSCRIPT ONE}) missing from current font.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](w3_lm_regularization_files/figure-html/cell-5-output-2.png){width=1142 height=473}\n:::\n:::\n\n\n**关键区别**：\n\n- **L2（圆形）**：约束边界是平滑的，最优解很少正好在坐标轴上 → 系数小但不为0\n- **L1（菱形）**：约束边界有\"尖角\"在坐标轴上，最优解容易碰到尖角 → 某些系数直接为0\n\n---\n\n## 第三部分：三种正则化方法\n\n---\n\n### Ridge 回归（L2 正则化）\n\n#### 损失函数\n\n$$\n\\min \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p}w_j^2\n$$\n\n**惩罚项**：$ \\alpha \\sum_{j=1}^{p}w_j^2 $ = 权重的平方和\n\n#### 特点\n\n✅ **让所有权重变小**  \n✅ **不会让权重变成 0**  \n✅ **适合所有特征都重要的情况**  \n✅ **对多重共线性有帮助**\n\n---\n\n### Lasso 回归（L1 正则化）\n\n#### 损失函数\n\n$$\n\\min \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p}|w_j|\n$$\n\n**惩罚项**： $ \\alpha \\sum_{j=1}^{p}|w_j| $ = 权重的绝对值和\n\n#### 特点\n\n✅ **让某些权重直接变为 0**（自动特征选择）  \n✅ **产生稀疏模型**（只保留重要特征）  \n✅ **适合认为只有少数特征重要的情况**  \n✅ **结果易解释**（特征少）\n\n---\n\n### Elastic Net（L1 + L2）\n\n#### 损失函数\n\n$$\n\\min \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\alpha \\left( \\rho \\sum_{j=1}^{p}|w_j| + \\frac{1-\\rho}{2} \\sum_{j=1}^{p}w_j^2 \\right)\n$$\n\n**两个超参数**：\n\n- **α**：正则化强度\n- **ρ**（l1_ratio）：L1 和 L2 的混合比例（0 = 纯 L2，1 = 纯 L1）\n\n#### 特点\n\n✅ **结合 Ridge 和 Lasso 的优点**  \n✅ **在相关特征较多时表现更好**  \n✅ **不确定用哪个时的折中选择**\n\n---\n\n### 三种方法对比总结\n| 方法             | 正则化项                | 权重特点        | 特征选择 | 适用场景         | sklearn类         |\n|------------------|------------------------|----------------|----------|------------------|-------------------|\n| 普通线性回归     | 无                     | 可能很大       | 否       | 特征少、数据多   | LinearRegression  |\n| Ridge (L2)       | ∑w²                    | 变小但不为0    | 否       | 特征都重要       | Ridge             |\n| Lasso (L1)       | ∑\\|w\\|                 | 部分变为0      | 是       | 特征稀疏         | Lasso             |\n| Elastic Net      | ρ∑\\|w\\| + (1-ρ)∑w²    | 折中           | 是       | 不确定时         | ElasticNet        |\n\n---\n\n## 第四部分：正则化参数 α 的选择\n\n---\n\n### α 的作用\n\n::: {#7d35fc1d .cell fig-height='5' fig-width='12' execution_count=5}\n\n::: {.cell-output .cell-output-stderr}\n```\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](w3_lm_regularization_files/figure-html/cell-6-output-2.png){width=1334 height=469}\n:::\n:::\n\n\n**关键点**：\n\n- **α = 0**：无正则化（普通线性回归）\n- **α 很小**：弱正则化，接近普通回归\n- **α 适中**：平衡拟合与复杂度 ✓\n- **α 很大**：强正则化，可能欠拟合\n\n---\n\n### 交叉验证（Cross-Validation）\n\n#### 为什么需要交叉验证？\n\n**错误做法** ❌：在测试集上尝试不同的 α，选最好的\n\n- 这样会\"泄露\"测试集信息\n- 导致对模型效果的过于乐观估计\n\n**正确做法** ✓：用交叉验证在训练集上选 α\n\n#### K-Fold 交叉验证\n\n::: {#8033a51f .cell fig-height='6' fig-width='10' execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](w3_lm_regularization_files/figure-html/cell-7-output-1.png){width=950 height=564}\n:::\n:::\n\n\n**流程**：\n\n1. 将训练集分成 K 份（例如 K=5）\n2. 每次用其中 1 份作验证，其余 K-1 份训练\n3. 重复 K 次，得到 K 个评分\n4. 取平均作为该 α 的评估分数\n5. 选择评分最好的 α\n\n---\n\n## 第五部分：正则化路径与可视化\n\n---\n\n### 正则化路径图（Regularization Path）\n\n**问题**：随着 α 增大，各特征的系数如何变化？\n\n::: {#f7ffc4a0 .cell fig-height='5' fig-width='12' execution_count=7}\n\n::: {.cell-output .cell-output-stderr}\n```\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](w3_lm_regularization_files/figure-html/cell-8-output-2.png){width=1334 height=470}\n:::\n:::\n\n\n**观察**：\n\n- α 很小时：所有特征都保留\n- α 逐渐增大：不重要的特征系数先变为 0\n- α 很大时：几乎所有特征都被剔除\n\n---\n\n## 第六部分：实践建议\n\n---\n\n### 正则化回归完整流程\n\n\n\n\n\n```{mermaid}\ngraph TD\n    A[加载数据] --> B[数据标准化<br>StandardScaler]\n    B --> C[训练/测试切分]\n    C --> D[选择正则化方法]\n    D --> E1[Ridge: 特征都重要]\n    D --> E2[Lasso: 需要特征选择]\n    D --> E3[Elastic Net: 不确定]\n    \n    E1 --> F1[RidgeCV 自动选α]\n    E2 --> F2[LassoCV 自动选α]\n    E3 --> F3[ElasticNetCV 自动选α]\n    \n    F1 --> G[训练最终模型]\n    F2 --> G\n    F3 --> G\n    \n    G --> H[测试集评估]\n    H --> I[可视化分析<br>系数/路径/残差]\n    \n    style B fill:#fff9c4\n    style F1 fill:#c8e6c9\n    style F2 fill:#c8e6c9\n    style F3 fill:#c8e6c9\n    style I fill:#ffccbc\n```\n\n\n\n\n\n---\n\n### 常见错误与注意事项\n\n#### ❌ 错误 1：忘记标准化\n\n**错误做法**：直接在原始数据上训练正则化模型\n\n- 特征尺度差异很大时，正则化会不公平地惩罚大尺度特征\n- 例如：面积特征（0-200）和卧室数特征（1-10），面积特征的权重会被过度惩罚\n\n**正确做法**：\n\n1. 先用 `StandardScaler` 在训练集上 fit（计算均值和标准差）\n2. 用训练集的参数 transform 训练集和测试集\n3. 确保所有特征都在同一尺度（均值0，标准差1）\n\n**为什么重要**：正则化惩罚权重的大小，如果特征尺度不同，会导致不公平的惩罚。\n\n---\n\n#### ❌ 错误 2：在测试集上选 α\n\n**错误做法**：尝试不同的 α 值，在测试集上选择表现最好的\n\n- 这会\"泄露\"测试集信息，导致对模型效果的过于乐观估计\n- 相当于用测试数据训练模型，违背了机器学习的基本原则\n\n**正确做法**：\n\n1. 用交叉验证在训练集上选择最优 α\n2. 训练最终模型后，再在测试集上评估\n3. 确保测试集只用于最终评估\n\n**为什么重要**：避免数据泄露，得到真实的模型评估结果。\n\n---\n\n#### ❌ 错误 3：选择不合适的正则化方法\n\n| 场景                   | 推荐方法      | 原因                   |\n|------------------------|-------------|------------------------|\n| 100个特征，认为都重要  | Ridge       | Ridge保留所有特征      |\n| 100个特征，只有10个重要| Lasso       | Lasso自动特征选择      |\n| 特征间高度相关         | Elastic Net | Elastic Net处理共线性更好|\n| 不确定哪些特征重要     | Elastic Net | Elastic Net是折中方案  |\n| 需要可解释性（特征少） | Lasso       | Lasso产生稀疏模型      |\n\n---\n\n### 实用技巧\n\n#### 技巧 1：先尝试 Lasso\n\n**为什么**：Lasso 可以自动做特征选择，告诉你哪些特征真正重要\n\n- 如果 Lasso 把很多特征剔除了，说明数据有稀疏性\n- 可以根据 Lasso 的结果决定是否需要特征工程\n- 帮助理解数据的特征重要性分布\n\n#### 技巧 2：绘制验证曲线\n\n**目的**：观察 α 参数如何影响模型性能\n\n- X轴：α 值（对数刻度，从 0.001 到 1000）\n- Y轴：模型评分（R² 或其他指标）\n- 两条线：训练集和验证集的性能\n\n**如何解读**：\n\n- α 太小：训练和验证都表现良好（可能过拟合）\n- α 适中：验证集表现最好 ✓\n- α 太大：训练和验证都表现差（欠拟合）\n\n#### 技巧 3：对比多种方法\n\n**推荐做法**：\n\n1. 同时训练 Ridge、Lasso 和 Elastic Net（都用 CV 自动选参）\n2. 比较它们的测试集性能\n3. 选择最适合业务场景的方法\n4. 记录不同方法的优缺点，便于后续解释\n\n**为什么重要**：不同正则化方法适合不同场景，盲目选择可能错过最佳方案。\n\n---\n\n## 总结\n\n---\n\n### 本讲核心概念\n\n#### 过拟合与正则化\n- **过拟合**：模型太复杂，记住了训练数据的噪声\n- **正则化**：在损失函数中加入复杂度惩罚，限制模型复杂度\n- **Bias-Variance Tradeoff**：找到偏差和方差的平衡点\n\n#### 三种正则化方法\n- **Ridge (L2)**：让系数变小但不为 0，适合所有特征都重要\n- **Lasso (L1)**：让某些系数变为 0，自动特征选择\n- **Elastic Net**：结合 Ridge 和 Lasso，折中方案\n\n#### 参数选择\n- **交叉验证**：避免在测试集上调参\n- **RidgeCV/LassoCV**：自动选择最优 α\n- **正则化路径**：观察 α 变化时系数的变化\n\n---\n\n### 重要提醒\n\n1. **正则化前必须标准化特征** ⚠️  \n   否则正则化会不公平地惩罚某些特征\n\n2. **不要在测试集上选参数** ⚠️  \n   用交叉验证在训练集上选\n\n3. **多试几种方法** ⚠️  \n   Ridge、Lasso、Elastic Net 都试试，对比效果\n\n4. **可视化很重要** ⚠️  \n   正则化路径图、系数图、残差图\n\n---\n\n## Q&A\n\n**Q1：什么是“过拟合” (Overfitting)？根据讲义的开场案例，它的关键信号是什么？**\n\n**A：**\n\n* 定义：过拟合是指模型过度学习了训练数据中的细节和噪声，导致它无法很好地泛化到未见过的新数据上。\n* 关键信号：模型在训练集上表现很好（例如RMSE 5万），但在测试集上表现很差（例如RMSE 50万）。训练集表现和测试集表现之间存在巨大差距是过拟合的关键信号。\n\n---\n\n**Q2：什么是“正则化”？它如何修改普通线性回归的损失函数？**\n\n**A：**\n\n* 定义：正则化是一种用来防止过拟合、控制模型复杂度的技术。\n* 修改方式：它在普通线性回归的损失函数（拟合误差，如MSE）基础上，额外加入一个“复杂度惩罚项”。\n* 新目标：$\\min(\\text{拟合误差} + \\alpha \\times \\text{复杂度惩罚})$。这迫使模型在“拟合数据”和“保持简单”之间找到一个平衡。\n\n---\n\n**Q3：Ridge 回归和 Lasso 回归各自使用的是哪种正则化惩罚项（L1 还是 L2）？它们的惩罚项在数学上有什么不同？**\n\n**A：**\n\n* Ridge (L2)：使用 L2 正则化。惩罚项是所有特征系数（权重）的平方和 ($\\sum w_j^2$)。\n* Lasso (L1)：使用 L1 正则化。惩罚项是所有特征系数（权重）的绝对值和 ($\\sum |w_j|$)。\n\n---\n\n**Q4：Lasso (L1) 正则化最独特的特点是什么？这使它在实践中有什么重要应用？**\n\n**A：**\n\n* 最独特的特点：Lasso 最独特的特点是它能将某些不重要特征的系数（权重）直接压缩到 0。\n* 重要应用：这使 Lasso 具有自动特征选择的功能。它可以帮助我们从大量特征中筛选出真正重要的特征，从而得到一个更简单（稀疏）、可解释性更强的模型。\n\n---\n\n**Q5：为什么我们必须使用“交叉验证” (Cross-Validation) 来选择正则化参数 $\\alpha$，而不是直接在测试集上选择？**\n\n**A：**\n\n* 因为测试集的核心原则是“只在最后使用一次”，它用来评估最终模型的泛化能力。\n* 如果在测试集上反复尝试不同的 $\\alpha$ 值来“调参”，就相当于让模型在训练过程中“偷看”到了测试集的信息，这称为“数据泄露”。这会导致我们对模型性能的评估过于乐观，得到的不是模型真实的泛化能力。交叉验证允许我们在训练集内部安全地模拟这一评估过程，以找到最优的 $\\alpha$。\n\n---\n\n**Q6：正则化是如何体现在“偏差-方差权衡” (Bias-Variance Tradeoff) 上的？增加正则化强度（即增大 $\\alpha$）时，偏差和方差分别会如何变化？**\n\n**A：**\n\n* 关系：正则化是管理偏差-方差权衡的关键工具。一个过拟合的模型（$\\alpha=0$）通常具有低偏差（能完美拟合训练数据）和高方差（对训练数据中的噪声高度敏感）。\n* 增大 $\\alpha$ 的影响：\n    1.  增加偏差：当我们增大 $\\alpha$（加强惩罚），我们是在强迫模型变得更简单。这种限制会使模型增加偏差（Bias），因为它可能无法再完美捕捉训练数据中的所有复杂规律。\n    2.  降低方差：作为交换，一个更简单的模型对训练数据中特定噪声的敏感度显著降低了，即降低了方差（Variance）。\n* 目标：正则化的目标是找到一个最优的 $\\alpha$，在“欠拟合”（偏差高）和“过拟合”（方差高）之间找到一个平衡点，使得总误差（偏差 + 方差）最小。\n\n---\n\n**Q7：为什么讲义中反复强调“在应用正则化之前必须标准化特征”？如果忘记标准化（例如，一个特征“面积”范围 50-200，另一个“卧室数”范围 1-5），会导致什么严重后果？**\n\n**A：**\n\n* 核心原因：正则化（L1 和 L2）是通过惩罚系数（权重 $w$）的大小来限制模型复杂度的。\n* 严重后果：\n    1.  尺度影响系数：如果特征尺度不同，“面积”（50-200）的系数 $w_1$ 自然会很小（例如 0.01），而“卧室数”（1-5）的系数 $w_2$ 会相对很大（例如 10）。\n    2.  不公平的惩罚：在计算惩罚项时（例如 L2：$\\alpha (w_1^2 + w_2^2)$），模型会极大地惩罚那个尺度小、系数大的特征（$w_2=10$），而几乎忽略那个尺度大、系数小的特征（$w_1=0.01$）。\n    3.  结论：忘记标准化会导致正则化的惩罚变得不公平且毫无意义。它会错误地惩罚那些仅仅因为单位尺度小而系数大的特征，而不是真正“不重要”的特征。标准化（如 Z-score）使所有特征处于同一尺度（例如均值0，标准差1），惩罚才会公平有效。\n\n---\n\n**Q8：在以下三种场景中，你应优先选择哪种正则化方法（Ridge, Lasso, Elastic Net），并说明理由？**\n\n1.  **你有 500 个特征，但你怀疑其中只有约 20 个真正有用。**\n2.  **你有 50 个特征，它们都与目标相关，但彼此之间（例如“收入”和“房产价值”）高度相关。**\n3.  **你有 50 个特征，你认为它们都对预测有贡献，没有哪个是完全无用的。**\n\n**A：**\n\n1.  优先 Lasso：这是一个典型的“稀疏”场景。Lasso 的自动特征选择功能非常适合从大量特征中筛选出少数有用的特征，同时将其他无用特征的系数变为 0，得到一个简洁、可解释性强的模型。\n2.  优先 Elastic Net：Lasso 在处理高度相关的特征时表现不稳定（它可能会随机选择一个，而把其他相关的特征系数压到0）。Elastic Net 结合了 L2（Ridge）的特性，在处理相关特征时更稳健，倾向于将它们“成组”地保留或剔除。\n3.  优先 Ridge：因为我们认为所有特征都重要，我们不希望 Lasso 把任何一个特征的系数变为 0。Ridge (L2) 会保留所有特征，同时通过“缩小”所有系数来防止模型过拟合，这非常适合“所有特征都有用”的场景。\n\n",
    "supporting": [
      "w3_lm_regularization_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}