{
  "hash": "74c930397c629a3d3a1a4f089190153c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 第六讲上机实践\n---\n\n\n\n本上机讲义覆盖以下内容：\n\n- 环境准备：安装/验证 TensorFlow/Keras\n- 数据准备：读取 Titanic 预处理后数据\n- 搭建 MLP：使用 Sequential API 构建多层感知机\n- 训练模型：设置超参数并训练\n- 可视化学习曲线：观察训练过程\n- 模型评估：计算准确率、AUC 等指标\n- 模型对比：MLP vs 随机森林\n\n**目标：** 掌握 Keras 构建、训练和评估神经网络的基本流程\n\n## 环境准备\n\n首先，我们需要导入必要的库并验证 TensorFlow/Keras 的安装。\n\n::: {#1de7a874 .cell message='false' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import roc_auc_score, accuracy_score, classification_report, roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import fetch_openml\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# 设置中文字体\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n%matplotlib inline\n\nprint(f\"TensorFlow 版本: {tf.__version__}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow 版本: 2.15.0\n```\n:::\n:::\n\n\n## 数据准备\n\n我们使用 sklearn 的 `fetch_openml` 直接加载 Titanic 数据集，然后进行数据预处理：\n\n- 处理缺失值\n- 编码分类变量\n- 划分训练/测试集\n- **重要：** 对数值特征进行标准化（神经网络对特征尺度敏感）\n\n::: {#58c09a0d .cell message='false' execution_count=3}\n``` {.python .cell-code}\n# 读取 W4 预处理后的数据\nX = pd.read_csv('titanic_data/features_processed.csv')\ny = pd.read_csv('titanic_data/labels_processed.csv')\n\n# 划分训练/测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# 归一化（重要！神经网络对特征尺度敏感）\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(f\"训练集形状: {X_train.shape}\")\nprint(f\"测试集形状: {X_test.shape}\")\nprint(f\"特征数量: {X_train.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n训练集形状: (711, 8)\n测试集形状: (178, 8)\n特征数量: 8\n```\n:::\n:::\n\n\n## 搭建 MLP 模型\n\n使用 Keras 的 Sequential API 构建多层感知机：\n\n::: {#f929c9fc .cell message='false' execution_count=4}\n``` {.python .cell-code}\n# 搭建 MLP 模型\nmodel = keras.Sequential([\n    # 输入层 + 第一个隐藏层\n    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    # Dropout 层（防止过拟合）\n    layers.Dropout(0.2),\n    # 第二个隐藏层\n    layers.Dense(32, activation='relu'),\n    # Dropout 层\n    layers.Dropout(0.2),\n    # 输出层（二分类，使用 sigmoid 激活函数）\n    layers.Dense(1, activation='sigmoid')\n])\n\n# 查看模型结构\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 64)                576       \n                                                                 \n dropout (Dropout)           (None, 64)                0         \n                                                                 \n dense_1 (Dense)             (None, 32)                2080      \n                                                                 \n dropout_1 (Dropout)         (None, 32)                0         \n                                                                 \n dense_2 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 2689 (10.50 KB)\nTrainable params: 2689 (10.50 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n```\n:::\n:::\n\n\n## 编译模型\n\n设置损失函数、优化器和评估指标：\n\n::: {#b4975b06 .cell message='false' execution_count=5}\n``` {.python .cell-code}\n# 编译模型\nmodel.compile(\n    optimizer='adam',                    # 优化器\n    loss='binary_crossentropy',          # 二分类损失函数\n    metrics=['accuracy',                 # 准确率\n             tf.keras.metrics.AUC()]     # AUC 指标\n)\n```\n:::\n\n\n## 训练模型\n\n设置训练参数并开始训练：\n\n::: {#6cef3c72 .cell message='false' execution_count=6}\n``` {.python .cell-code}\n# 训练参数\nepochs = 50          # 训练轮数\nbatch_size = 32      # 批次大小\n\n# 训练模型\nhistory = model.fit(\n    X_train, y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    validation_split=0.2,  # 从训练集中划分验证集\n    verbose=1\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/50\n\r 1/18 [>.............................] - ETA: 3s - loss: 0.7160 - accuracy: 0.5312 - auc: 0.5392\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 5ms/step - loss: 0.6339 - accuracy: 0.6655 - auc: 0.6773 - val_loss: 0.5882 - val_accuracy: 0.7273 - val_auc: 0.8239\nEpoch 2/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5511 - accuracy: 0.7188 - auc: 0.8704\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.5644 - accuracy: 0.7394 - auc: 0.7901 - val_loss: 0.5303 - val_accuracy: 0.7413 - val_auc: 0.8409\nEpoch 3/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5495 - accuracy: 0.6875 - auc: 0.8360\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.5177 - accuracy: 0.7694 - auc: 0.8243 - val_loss: 0.4937 - val_accuracy: 0.7552 - val_auc: 0.8476\nEpoch 4/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4957 - accuracy: 0.7812 - auc: 0.8829\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4929 - accuracy: 0.7905 - auc: 0.8306 - val_loss: 0.4715 - val_accuracy: 0.7692 - val_auc: 0.8563\nEpoch 5/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5716 - accuracy: 0.6875 - auc: 0.7956\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4738 - accuracy: 0.7817 - auc: 0.8373 - val_loss: 0.4590 - val_accuracy: 0.7902 - val_auc: 0.8559\nEpoch 6/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4461 - accuracy: 0.8125 - auc: 0.8667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4639 - accuracy: 0.8011 - auc: 0.8440 - val_loss: 0.4473 - val_accuracy: 0.7972 - val_auc: 0.8612\nEpoch 7/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3724 - accuracy: 0.8438 - auc: 0.8986\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4569 - accuracy: 0.7958 - auc: 0.8454 - val_loss: 0.4422 - val_accuracy: 0.7972 - val_auc: 0.8656\nEpoch 8/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5582 - accuracy: 0.7500 - auc: 0.7652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4442 - accuracy: 0.7993 - auc: 0.8537 - val_loss: 0.4377 - val_accuracy: 0.8042 - val_auc: 0.8704\nEpoch 9/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5646 - accuracy: 0.6875 - auc: 0.7627\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.7835 - auc: 0.8485 - val_loss: 0.4369 - val_accuracy: 0.8112 - val_auc: 0.8697\nEpoch 10/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4289 - accuracy: 0.7500 - auc: 0.8551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.7852 - auc: 0.8472 - val_loss: 0.4347 - val_accuracy: 0.8042 - val_auc: 0.8714\nEpoch 11/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3120 - accuracy: 0.9062 - auc: 0.9610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4422 - accuracy: 0.7975 - auc: 0.8553 - val_loss: 0.4336 - val_accuracy: 0.8112 - val_auc: 0.8727\nEpoch 12/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3834 - accuracy: 0.8125 - auc: 0.9345\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4355 - accuracy: 0.8011 - auc: 0.8521 - val_loss: 0.4367 - val_accuracy: 0.8182 - val_auc: 0.8702\nEpoch 13/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4812 - accuracy: 0.7188 - auc: 0.9137\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4305 - accuracy: 0.8028 - auc: 0.8550 - val_loss: 0.4387 - val_accuracy: 0.8182 - val_auc: 0.8723\nEpoch 14/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4046 - accuracy: 0.7812 - auc: 0.9028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4344 - accuracy: 0.7940 - auc: 0.8548 - val_loss: 0.4398 - val_accuracy: 0.8112 - val_auc: 0.8718\nEpoch 15/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3113 - accuracy: 0.8438 - auc: 0.9524\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4076 - accuracy: 0.8222 - auc: 0.8776 - val_loss: 0.4442 - val_accuracy: 0.7972 - val_auc: 0.8687\nEpoch 16/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3088 - accuracy: 0.8438 - auc: 0.9393\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4129 - accuracy: 0.8292 - auc: 0.8688 - val_loss: 0.4434 - val_accuracy: 0.8112 - val_auc: 0.8720\nEpoch 17/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4229 - accuracy: 0.8438 - auc: 0.8563\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 983us/step - loss: 0.4130 - accuracy: 0.8081 - auc: 0.8697 - val_loss: 0.4481 - val_accuracy: 0.8042 - val_auc: 0.8702\nEpoch 18/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4200 - accuracy: 0.8438 - auc: 0.8745\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 972us/step - loss: 0.4125 - accuracy: 0.8222 - auc: 0.8650 - val_loss: 0.4494 - val_accuracy: 0.7972 - val_auc: 0.8706\nEpoch 19/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4222 - accuracy: 0.8750 - auc: 0.8333\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3931 - accuracy: 0.8380 - auc: 0.8844 - val_loss: 0.4467 - val_accuracy: 0.8042 - val_auc: 0.8716\nEpoch 20/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.6102 - accuracy: 0.7188 - auc: 0.6761\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8239 - auc: 0.8722 - val_loss: 0.4476 - val_accuracy: 0.8042 - val_auc: 0.8738\nEpoch 21/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5506 - accuracy: 0.7500 - auc: 0.7455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4173 - accuracy: 0.8187 - auc: 0.8628 - val_loss: 0.4535 - val_accuracy: 0.8112 - val_auc: 0.8710\nEpoch 22/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4835 - accuracy: 0.7812 - auc: 0.8417\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4290 - accuracy: 0.8169 - auc: 0.8549 - val_loss: 0.4534 - val_accuracy: 0.8112 - val_auc: 0.8722\nEpoch 23/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2969 - accuracy: 0.9375 - auc: 0.9250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4076 - accuracy: 0.8433 - auc: 0.8621 - val_loss: 0.4524 - val_accuracy: 0.8042 - val_auc: 0.8743\nEpoch 24/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4637 - accuracy: 0.8750 - auc: 0.7705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4155 - accuracy: 0.8257 - auc: 0.8625 - val_loss: 0.4551 - val_accuracy: 0.8042 - val_auc: 0.8724\nEpoch 25/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3360 - accuracy: 0.8438 - auc: 0.9365\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3974 - accuracy: 0.8345 - auc: 0.8736 - val_loss: 0.4512 - val_accuracy: 0.8182 - val_auc: 0.8752\nEpoch 26/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3815 - accuracy: 0.8125 - auc: 0.8979\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3949 - accuracy: 0.8363 - auc: 0.8773 - val_loss: 0.4545 - val_accuracy: 0.8182 - val_auc: 0.8726\nEpoch 27/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3252 - accuracy: 0.9062 - auc: 0.8457\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4103 - accuracy: 0.8398 - auc: 0.8659 - val_loss: 0.4591 - val_accuracy: 0.8182 - val_auc: 0.8708\nEpoch 28/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3368 - accuracy: 0.8750 - auc: 0.9067\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3963 - accuracy: 0.8327 - auc: 0.8756 - val_loss: 0.4606 - val_accuracy: 0.8042 - val_auc: 0.8706\nEpoch 29/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3087 - accuracy: 0.8438 - auc: 0.9481\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3886 - accuracy: 0.8363 - auc: 0.8872 - val_loss: 0.4617 - val_accuracy: 0.8112 - val_auc: 0.8716\nEpoch 30/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4082 - accuracy: 0.8750 - auc: 0.8173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3998 - accuracy: 0.8222 - auc: 0.8761 - val_loss: 0.4662 - val_accuracy: 0.7972 - val_auc: 0.8715\nEpoch 31/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2856 - accuracy: 0.9062 - auc: 0.9229\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8257 - auc: 0.8851 - val_loss: 0.4680 - val_accuracy: 0.8112 - val_auc: 0.8707\nEpoch 32/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2527 - accuracy: 0.9062 - auc: 0.9636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3894 - accuracy: 0.8451 - auc: 0.8747 - val_loss: 0.4677 - val_accuracy: 0.8112 - val_auc: 0.8732\nEpoch 33/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2613 - accuracy: 0.9062 - auc: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8415 - auc: 0.8835 - val_loss: 0.4716 - val_accuracy: 0.8042 - val_auc: 0.8719\nEpoch 34/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3947 - accuracy: 0.8750 - auc: 0.8043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3879 - accuracy: 0.8468 - auc: 0.8745 - val_loss: 0.4709 - val_accuracy: 0.8112 - val_auc: 0.8724\nEpoch 35/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2997 - accuracy: 0.8750 - auc: 0.9417\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8292 - auc: 0.8796 - val_loss: 0.4743 - val_accuracy: 0.8042 - val_auc: 0.8705\nEpoch 36/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2013 - accuracy: 0.9688 - auc: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3930 - accuracy: 0.8310 - auc: 0.8779 - val_loss: 0.4739 - val_accuracy: 0.7972 - val_auc: 0.8724\nEpoch 37/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3482 - accuracy: 0.8438 - auc: 0.8843\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3867 - accuracy: 0.8398 - auc: 0.8817 - val_loss: 0.4711 - val_accuracy: 0.7972 - val_auc: 0.8712\nEpoch 38/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2953 - accuracy: 0.8438 - auc: 0.9431\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3859 - accuracy: 0.8380 - auc: 0.8795 - val_loss: 0.4717 - val_accuracy: 0.8042 - val_auc: 0.8715\nEpoch 39/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2655 - accuracy: 0.9062 - auc: 0.9479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3796 - accuracy: 0.8468 - auc: 0.8832 - val_loss: 0.4747 - val_accuracy: 0.8112 - val_auc: 0.8723\nEpoch 40/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3254 - accuracy: 0.9062 - auc: 0.9294\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3879 - accuracy: 0.8398 - auc: 0.8772 - val_loss: 0.4752 - val_accuracy: 0.8112 - val_auc: 0.8727\nEpoch 41/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.6349 - accuracy: 0.6875 - auc: 0.7863\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3738 - accuracy: 0.8363 - auc: 0.8917 - val_loss: 0.4764 - val_accuracy: 0.8112 - val_auc: 0.8729\nEpoch 42/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.6640 - accuracy: 0.6875 - auc: 0.6548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8504 - auc: 0.8786 - val_loss: 0.4787 - val_accuracy: 0.8112 - val_auc: 0.8728\nEpoch 43/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4864 - accuracy: 0.7812 - auc: 0.8462\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3836 - accuracy: 0.8380 - auc: 0.8855 - val_loss: 0.4765 - val_accuracy: 0.8042 - val_auc: 0.8722\nEpoch 44/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4112 - accuracy: 0.7812 - auc: 0.8961\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8380 - auc: 0.8802 - val_loss: 0.4758 - val_accuracy: 0.8042 - val_auc: 0.8706\nEpoch 45/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2877 - accuracy: 0.8750 - auc: 0.9762\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3865 - accuracy: 0.8398 - auc: 0.8802 - val_loss: 0.4786 - val_accuracy: 0.7972 - val_auc: 0.8715\nEpoch 46/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4207 - accuracy: 0.8125 - auc: 0.8690\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3798 - accuracy: 0.8363 - auc: 0.8819 - val_loss: 0.4805 - val_accuracy: 0.7972 - val_auc: 0.8734\nEpoch 47/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3420 - accuracy: 0.8750 - auc: 0.8961\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3935 - accuracy: 0.8486 - auc: 0.8791 - val_loss: 0.4787 - val_accuracy: 0.8042 - val_auc: 0.8724\nEpoch 48/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3126 - accuracy: 0.8125 - auc: 0.9434\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3909 - accuracy: 0.8415 - auc: 0.8791 - val_loss: 0.4750 - val_accuracy: 0.8042 - val_auc: 0.8742\nEpoch 49/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3544 - accuracy: 0.8438 - auc: 0.9125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8468 - auc: 0.8776 - val_loss: 0.4761 - val_accuracy: 0.8042 - val_auc: 0.8762\nEpoch 50/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3290 - accuracy: 0.9062 - auc: 0.9271\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.8556 - auc: 0.8946 - val_loss: 0.4821 - val_accuracy: 0.8042 - val_auc: 0.8747\n```\n:::\n:::\n\n\n## 可视化学习曲线\n\n观察训练过程中的损失和准确率变化：\n\n::: {#00313c90 .cell message='false' execution_count=7}\n``` {.python .cell-code}\n# 绘制学习曲线\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 损失曲线\naxes[0].plot(history.history['loss'], label='训练损失', linewidth=2)\naxes[0].plot(history.history['val_loss'], label='验证损失', linewidth=2)\naxes[0].set_xlabel('训练轮数', fontsize=12)\naxes[0].set_ylabel('损失', fontsize=12)\naxes[0].set_title('损失曲线', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# 准确率曲线\naxes[1].plot(history.history['accuracy'], label='训练准确率', linewidth=2)\naxes[1].plot(history.history['val_accuracy'], label='验证准确率', linewidth=2)\naxes[1].set_xlabel('训练轮数', fontsize=12)\naxes[1].set_ylabel('准确率', fontsize=12)\naxes[1].set_title('准确率曲线', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# AUC 曲线\naxes[2].plot(history.history['auc'], label='训练 AUC', linewidth=2)\naxes[2].plot(history.history['val_auc'], label='验证 AUC', linewidth=2)\naxes[2].set_xlabel('训练轮数', fontsize=12)\naxes[2].set_ylabel('AUC', fontsize=12)\naxes[2].set_title('AUC 曲线', fontsize=14)\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"学习曲线观察:\")\nprint(\"- 训练损失应该逐渐下降\")\nprint(\"- 验证损失开始上升可能表示过拟合\")\nprint(\"- 训练和验证指标差距过大也可能表示过拟合\")\n```\n\n::: {.cell-output .cell-output-display}\n![](w6_practice_files/figure-html/cell-8-output-1.png){width=1719 height=470}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n学习曲线观察:\n- 训练损失应该逐渐下降\n- 验证损失开始上升可能表示过拟合\n- 训练和验证指标差距过大也可能表示过拟合\n```\n:::\n:::\n\n\n## 模型评估\n\n在训练集和测试集上评估模型性能：\n\n- 准确率 (Accuracy)\n- AUC (Area Under ROC Curve)\n- 分类报告：精确率、召回率、F1 分数\n\n::: {#f1c0c163 .cell message='false' execution_count=8}\n``` {.python .cell-code}\n# 预测\ny_train_pred_prob = model.predict(X_train).flatten()\ny_test_pred_prob = model.predict(X_test).flatten()\n\ny_train_pred = (y_train_pred_prob > 0.5).astype(int)\ny_test_pred = (y_test_pred_prob > 0.5).astype(int)\n\n# 评估\nprint(\"=== 训练集 ===\")\nprint(f\"准确率: {accuracy_score(y_train, y_train_pred):.3f}\")\nprint(f\"AUC: {roc_auc_score(y_train, y_train_pred_prob):.3f}\")\n\nprint(\"\\n=== 测试集 ===\")\nprint(f\"准确率: {accuracy_score(y_test, y_test_pred):.3f}\")\nprint(f\"AUC: {roc_auc_score(y_test, y_test_pred_prob):.3f}\")\n\nprint(\"\\n分类报告:\")\nprint(classification_report(y_test, y_test_pred, target_names=['死亡', '存活']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r 1/23 [>.............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 0s 342us/step\n\r1/6 [====>.........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6/6 [==============================] - 0s 395us/step\n=== 训练集 ===\n准确率: 0.844\nAUC: 0.891\n\n=== 测试集 ===\n准确率: 0.803\nAUC: 0.852\n\n分类报告:\n              precision    recall  f1-score   support\n\n          死亡       0.80      0.90      0.85       110\n          存活       0.80      0.65      0.72        68\n\n    accuracy                           0.80       178\n   macro avg       0.80      0.77      0.78       178\nweighted avg       0.80      0.80      0.80       178\n\n```\n:::\n:::\n\n\n## MLP vs 随机森林对比\n\n将 MLP 与传统的机器学习算法（随机森林）进行对比。\n\n**注意：** 随机森林不需要特征标准化，所以使用原始特征。\n\n::: {#fd3b537c .cell message='false' execution_count=9}\n``` {.python .cell-code}\n# 准备随机森林的数据（不需要标准化）\nX_train_orig, X_test_orig, _, _ = train_test_split(\n    X.values,\n    y.values,\n    test_size=0.2, random_state=42, stratify=y.values\n)\n\n# 训练随机森林\nrf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\nrf.fit(X_train_orig, y_train)\ny_test_pred_rf = rf.predict_proba(X_test_orig)[:, 1]\n\n# 计算 AUC\nmlp_auc = roc_auc_score(y_test, y_test_pred_prob)\nrf_auc = roc_auc_score(y_test, y_test_pred_rf)\n\n# 对比表格\ncomparison = pd.DataFrame({\n    '模型': ['MLP', '随机森林'],\n    '测试集 AUC': [mlp_auc, rf_auc]\n})\n\nprint(\"模型对比:\")\nprint(comparison.to_string(index=False))\nprint(f\"\\nMLP 优势: {mlp_auc - rf_auc:+.3f} (AUC 差异)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n模型对比:\n  模型  测试集 AUC\n MLP 0.851738\n随机森林 0.846791\n\nMLP 优势: +0.005 (AUC 差异)\n```\n:::\n:::\n\n\n::: {#d8b1e9be .cell message='false' execution_count=10}\n``` {.python .cell-code}\n# ROC 曲线对比\nfpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_test_pred_prob)\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_pred_rf)\n\nplt.figure(figsize=(10, 8))\nplt.plot(fpr_mlp, tpr_mlp, linewidth=3, label=f'MLP (AUC={mlp_auc:.3f})')\nplt.plot(fpr_rf, tpr_rf, linewidth=3, label=f'随机森林 (AUC={rf_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='随机猜测')\nplt.xlabel('FPR (False Positive Rate)', fontsize=12)\nplt.ylabel('TPR (True Positive Rate)', fontsize=12)\nplt.title('MLP vs 随机森林 - ROC 曲线对比', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](w6_practice_files/figure-html/cell-11-output-1.png){width=815 height=676}\n:::\n:::\n\n\n## 小结\n\n**结论：** 在 Titanic 数据集上，随机森林通常表现略好于或持平于基础 MLP。这是正常的，因为：\n\n1. **数据集较小**：神经网络需要大量数据才能充分发挥优势\n2. **特征工程**：随机森林对特征预处理要求较低\n3. **模型复杂度**：我们使用的 MLP 结构相对简单\n\n**MLP 的优势将在更大规模的数据集和更复杂的任务中体现。**\n\n**练习任务：**\n\n- 尝试调整 MLP 的隐藏层神经元数量\n- 尝试不同的激活函数\n- 观察过拟合现象（增加 epochs）\n- 尝试其他数据集（如 MNIST 手写数字识别）\n\n**神经网络的关键概念：**\n\n- **层（Layers）**：神经网络的基本构建块\n- **激活函数（Activation）**：为网络引入非线性\n- **损失函数（Loss）**：衡量预测与真实值的差距\n- **优化器（Optimizer）**：更新网络参数的算法\n- **批次（Batch）**：每次更新使用的样本数量\n- **轮次（Epoch）**：完整遍历训练集的次数\n\n**超参数调优建议：**\n\n- **网络结构**：尝试不同的隐藏层数量和神经元数量\n- **学习率**：太大会震荡，太小会收敛慢\n- **批次大小**：通常 32-128 之间\n- **正则化**：Dropout、L1/L2 正则化防止过拟合\n- **早停（Early Stopping）**：监控验证损失，防止过拟合\n\n",
    "supporting": [
      "w6_practice_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}