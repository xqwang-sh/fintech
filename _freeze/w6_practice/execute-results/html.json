{
  "hash": "93b01dbcda616f04ce6ee94903ca03b6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 第六讲上机实践\n---\n\n\n\n\n本上机讲义覆盖以下内容：\n\n- 环境准备：安装/验证 TensorFlow/Keras\n- 数据准备：读取 Titanic 预处理后数据\n- 搭建 MLP：使用 Sequential API 构建多层感知机\n- 训练模型：设置超参数并训练\n- 可视化学习曲线：观察训练过程\n- 模型评估：计算准确率、AUC 等指标\n- 模型对比：MLP vs 随机森林\n\n**目标：** 掌握 Keras 构建、训练和评估神经网络的基本流程\n\n## 环境准备\n\n首先，我们需要导入必要的库并验证 TensorFlow/Keras 的安装。\n\n::: {#d00a8f30 .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import roc_auc_score, accuracy_score, classification_report, roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import fetch_openml\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# 设置中文字体\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n%matplotlib inline\n\nprint(f\"TensorFlow 版本: {tf.__version__}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow 版本: 2.15.0\n```\n:::\n:::\n\n\n## 数据准备\n\n我们使用 sklearn 的 `fetch_openml` 直接加载 Titanic 数据集，然后进行数据预处理：\n\n- 处理缺失值\n- 编码分类变量\n- 划分训练/测试集\n- **重要：** 对数值特征进行标准化（神经网络对特征尺度敏感）\n\n::: {#25f61a82 .cell message='false' execution_count=2}\n``` {.python .cell-code}\n# 读取 W4 预处理后的数据\nX = pd.read_csv('titanic_data/features_processed.csv')\ny = pd.read_csv('titanic_data/labels_processed.csv')\n\n# 划分训练/测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# 归一化（重要！神经网络对特征尺度敏感）\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(f\"训练集形状: {X_train.shape}\")\nprint(f\"测试集形状: {X_test.shape}\")\nprint(f\"特征数量: {X_train.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n训练集形状: (711, 8)\n测试集形状: (178, 8)\n特征数量: 8\n```\n:::\n:::\n\n\n## 搭建 MLP 模型\n\n使用 Keras 的 Sequential API 构建多层感知机：\n\n::: {#4a350c35 .cell message='false' execution_count=3}\n``` {.python .cell-code}\n# 搭建 MLP 模型\nmodel = keras.Sequential([\n    # 输入层 + 第一个隐藏层\n    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    # Dropout 层（防止过拟合）\n    layers.Dropout(0.2),\n    # 第二个隐藏层\n    layers.Dense(32, activation='relu'),\n    # Dropout 层\n    layers.Dropout(0.2),\n    # 输出层（二分类，使用 sigmoid 激活函数）\n    layers.Dense(1, activation='sigmoid')\n])\n\n# 查看模型结构\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 64)                576       \n                                                                 \n dropout (Dropout)           (None, 64)                0         \n                                                                 \n dense_1 (Dense)             (None, 32)                2080      \n                                                                 \n dropout_1 (Dropout)         (None, 32)                0         \n                                                                 \n dense_2 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 2689 (10.50 KB)\nTrainable params: 2689 (10.50 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n```\n:::\n:::\n\n\n## 编译模型\n\n设置损失函数、优化器和评估指标：\n\n::: {#e2b8dfeb .cell message='false' execution_count=4}\n``` {.python .cell-code}\n# 编译模型\nmodel.compile(\n    optimizer='adam',                    # 优化器\n    loss='binary_crossentropy',          # 二分类损失函数\n    metrics=['accuracy',                 # 准确率\n             tf.keras.metrics.AUC()]     # AUC 指标\n)\n```\n:::\n\n\n## 训练模型\n\n设置训练参数并开始训练：\n\n::: {#c2ba1d8c .cell message='false' execution_count=5}\n``` {.python .cell-code}\n# 训练参数\nepochs = 50          # 训练轮数\nbatch_size = 32      # 批次大小\n\n# 训练模型\nhistory = model.fit(\n    X_train, y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    validation_split=0.2,  # 从训练集中划分验证集\n    verbose=1\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/50\n\r 1/18 [>.............................] - ETA: 3s - loss: 0.7019 - accuracy: 0.4688 - auc: 0.6397\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 6ms/step - loss: 0.6765 - accuracy: 0.6004 - auc: 0.6303 - val_loss: 0.6181 - val_accuracy: 0.7063 - val_auc: 0.7347\nEpoch 2/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5630 - accuracy: 0.8438 - auc: 0.8250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.5938 - accuracy: 0.7271 - auc: 0.7388 - val_loss: 0.5517 - val_accuracy: 0.7413 - val_auc: 0.8382\nEpoch 3/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5848 - accuracy: 0.7500 - auc: 0.7980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.5283 - accuracy: 0.7746 - auc: 0.8209 - val_loss: 0.5051 - val_accuracy: 0.7762 - val_auc: 0.8515\nEpoch 4/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4790 - accuracy: 0.7812 - auc: 0.8312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4966 - accuracy: 0.7746 - auc: 0.8231 - val_loss: 0.4707 - val_accuracy: 0.7972 - val_auc: 0.8589\nEpoch 5/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4503 - accuracy: 0.8125 - auc: 0.8455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 994us/step - loss: 0.4589 - accuracy: 0.7993 - auc: 0.8458 - val_loss: 0.4538 - val_accuracy: 0.7902 - val_auc: 0.8621\nEpoch 6/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4292 - accuracy: 0.8125 - auc: 0.8810\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 978us/step - loss: 0.4435 - accuracy: 0.7975 - auc: 0.8569 - val_loss: 0.4445 - val_accuracy: 0.8112 - val_auc: 0.8672\nEpoch 7/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5629 - accuracy: 0.6875 - auc: 0.7490\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 987us/step - loss: 0.4341 - accuracy: 0.8081 - auc: 0.8564 - val_loss: 0.4424 - val_accuracy: 0.8252 - val_auc: 0.8662\nEpoch 8/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4580 - accuracy: 0.7188 - auc: 0.8182\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 997us/step - loss: 0.4351 - accuracy: 0.8222 - auc: 0.8534 - val_loss: 0.4407 - val_accuracy: 0.8182 - val_auc: 0.8643\nEpoch 9/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4560 - accuracy: 0.7812 - auc: 0.8208\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 991us/step - loss: 0.4179 - accuracy: 0.8134 - auc: 0.8610 - val_loss: 0.4458 - val_accuracy: 0.8182 - val_auc: 0.8618\nEpoch 10/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4324 - accuracy: 0.7812 - auc: 0.8918\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 989us/step - loss: 0.4373 - accuracy: 0.7993 - auc: 0.8524 - val_loss: 0.4410 - val_accuracy: 0.8042 - val_auc: 0.8647\nEpoch 11/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4756 - accuracy: 0.8125 - auc: 0.8219\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 987us/step - loss: 0.4226 - accuracy: 0.8239 - auc: 0.8577 - val_loss: 0.4461 - val_accuracy: 0.7972 - val_auc: 0.8627\nEpoch 12/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5006 - accuracy: 0.7812 - auc: 0.8083\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 990us/step - loss: 0.4212 - accuracy: 0.8222 - auc: 0.8620 - val_loss: 0.4485 - val_accuracy: 0.8182 - val_auc: 0.8613\nEpoch 13/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3827 - accuracy: 0.8750 - auc: 0.8788\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4258 - accuracy: 0.8187 - auc: 0.8576 - val_loss: 0.4461 - val_accuracy: 0.8042 - val_auc: 0.8627\nEpoch 14/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4640 - accuracy: 0.7812 - auc: 0.8118\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 996us/step - loss: 0.4173 - accuracy: 0.8134 - auc: 0.8608 - val_loss: 0.4508 - val_accuracy: 0.8042 - val_auc: 0.8659\nEpoch 15/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4921 - accuracy: 0.8438 - auc: 0.7773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 983us/step - loss: 0.4023 - accuracy: 0.8327 - auc: 0.8737 - val_loss: 0.4497 - val_accuracy: 0.7972 - val_auc: 0.8658\nEpoch 16/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4872 - accuracy: 0.8125 - auc: 0.8091\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 970us/step - loss: 0.4009 - accuracy: 0.8310 - auc: 0.8734 - val_loss: 0.4506 - val_accuracy: 0.8042 - val_auc: 0.8676\nEpoch 17/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2165 - accuracy: 0.9688 - auc: 0.9955\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 976us/step - loss: 0.3947 - accuracy: 0.8363 - auc: 0.8744 - val_loss: 0.4537 - val_accuracy: 0.7972 - val_auc: 0.8645\nEpoch 18/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3799 - accuracy: 0.8125 - auc: 0.9048\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 967us/step - loss: 0.4060 - accuracy: 0.8239 - auc: 0.8683 - val_loss: 0.4563 - val_accuracy: 0.7972 - val_auc: 0.8652\nEpoch 19/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4140 - accuracy: 0.8438 - auc: 0.8917\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 976us/step - loss: 0.3953 - accuracy: 0.8239 - auc: 0.8759 - val_loss: 0.4601 - val_accuracy: 0.7972 - val_auc: 0.8658\nEpoch 20/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3847 - accuracy: 0.8750 - auc: 0.8396\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 983us/step - loss: 0.4000 - accuracy: 0.8239 - auc: 0.8728 - val_loss: 0.4590 - val_accuracy: 0.8042 - val_auc: 0.8665\nEpoch 21/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.6445 - accuracy: 0.6875 - auc: 0.6932\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 979us/step - loss: 0.3994 - accuracy: 0.8257 - auc: 0.8774 - val_loss: 0.4577 - val_accuracy: 0.7972 - val_auc: 0.8667\nEpoch 22/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4614 - accuracy: 0.7812 - auc: 0.8611\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 967us/step - loss: 0.3919 - accuracy: 0.8363 - auc: 0.8821 - val_loss: 0.4607 - val_accuracy: 0.7972 - val_auc: 0.8657\nEpoch 23/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5847 - accuracy: 0.7812 - auc: 0.7417\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 951us/step - loss: 0.3912 - accuracy: 0.8380 - auc: 0.8773 - val_loss: 0.4651 - val_accuracy: 0.7902 - val_auc: 0.8651\nEpoch 24/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5456 - accuracy: 0.7500 - auc: 0.7874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 970us/step - loss: 0.3976 - accuracy: 0.8310 - auc: 0.8772 - val_loss: 0.4724 - val_accuracy: 0.7902 - val_auc: 0.8651\nEpoch 25/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4991 - accuracy: 0.7812 - auc: 0.8373\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 975us/step - loss: 0.4018 - accuracy: 0.8099 - auc: 0.8770 - val_loss: 0.4712 - val_accuracy: 0.7762 - val_auc: 0.8646\nEpoch 26/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2261 - accuracy: 0.9375 - auc: 0.9603\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 980us/step - loss: 0.4024 - accuracy: 0.8345 - auc: 0.8753 - val_loss: 0.4649 - val_accuracy: 0.7902 - val_auc: 0.8666\nEpoch 27/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5206 - accuracy: 0.7500 - auc: 0.8353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3878 - accuracy: 0.8363 - auc: 0.8822 - val_loss: 0.4708 - val_accuracy: 0.7902 - val_auc: 0.8630\nEpoch 28/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3450 - accuracy: 0.8438 - auc: 0.9010\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 989us/step - loss: 0.3980 - accuracy: 0.8257 - auc: 0.8781 - val_loss: 0.4722 - val_accuracy: 0.7902 - val_auc: 0.8611\nEpoch 29/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3123 - accuracy: 0.9375 - auc: 0.9048\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 996us/step - loss: 0.3943 - accuracy: 0.8327 - auc: 0.8808 - val_loss: 0.4710 - val_accuracy: 0.7902 - val_auc: 0.8632\nEpoch 30/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.6737 - accuracy: 0.6875 - auc: 0.6732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 952us/step - loss: 0.3870 - accuracy: 0.8380 - auc: 0.8787 - val_loss: 0.4733 - val_accuracy: 0.7902 - val_auc: 0.8625\nEpoch 31/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4373 - accuracy: 0.9062 - auc: 0.8359\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 947us/step - loss: 0.3871 - accuracy: 0.8415 - auc: 0.8809 - val_loss: 0.4729 - val_accuracy: 0.7902 - val_auc: 0.8591\nEpoch 32/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4096 - accuracy: 0.7812 - auc: 0.8869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 950us/step - loss: 0.3821 - accuracy: 0.8327 - auc: 0.8877 - val_loss: 0.4798 - val_accuracy: 0.7832 - val_auc: 0.8626\nEpoch 33/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5346 - accuracy: 0.7188 - auc: 0.7996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 954us/step - loss: 0.3725 - accuracy: 0.8504 - auc: 0.8899 - val_loss: 0.4744 - val_accuracy: 0.7902 - val_auc: 0.8667\nEpoch 34/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5334 - accuracy: 0.7188 - auc: 0.8164\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 960us/step - loss: 0.3988 - accuracy: 0.8363 - auc: 0.8718 - val_loss: 0.4769 - val_accuracy: 0.7902 - val_auc: 0.8646\nEpoch 35/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3000 - accuracy: 0.8750 - auc: 0.9396\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 951us/step - loss: 0.3902 - accuracy: 0.8257 - auc: 0.8765 - val_loss: 0.4753 - val_accuracy: 0.7832 - val_auc: 0.8627\nEpoch 36/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3751 - accuracy: 0.8438 - auc: 0.9180\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 952us/step - loss: 0.3977 - accuracy: 0.8380 - auc: 0.8714 - val_loss: 0.4799 - val_accuracy: 0.7692 - val_auc: 0.8655\nEpoch 37/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4656 - accuracy: 0.8125 - auc: 0.7798\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 958us/step - loss: 0.3886 - accuracy: 0.8451 - auc: 0.8844 - val_loss: 0.4767 - val_accuracy: 0.7832 - val_auc: 0.8623\nEpoch 38/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5498 - accuracy: 0.8125 - auc: 0.7938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3910 - accuracy: 0.8415 - auc: 0.8768 - val_loss: 0.4770 - val_accuracy: 0.7902 - val_auc: 0.8645\nEpoch 39/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3539 - accuracy: 0.8438 - auc: 0.9091\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3838 - accuracy: 0.8363 - auc: 0.8861 - val_loss: 0.4781 - val_accuracy: 0.7902 - val_auc: 0.8616\nEpoch 40/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3207 - accuracy: 0.8438 - auc: 0.9372\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 964us/step - loss: 0.3853 - accuracy: 0.8504 - auc: 0.8802 - val_loss: 0.4840 - val_accuracy: 0.7762 - val_auc: 0.8571\nEpoch 41/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3140 - accuracy: 0.9062 - auc: 0.9714\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 943us/step - loss: 0.3826 - accuracy: 0.8327 - auc: 0.8845 - val_loss: 0.4766 - val_accuracy: 0.7832 - val_auc: 0.8589\nEpoch 42/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3496 - accuracy: 0.8750 - auc: 0.8773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 940us/step - loss: 0.3829 - accuracy: 0.8380 - auc: 0.8825 - val_loss: 0.4863 - val_accuracy: 0.7692 - val_auc: 0.8620\nEpoch 43/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2276 - accuracy: 0.9062 - auc: 0.9870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 938us/step - loss: 0.3863 - accuracy: 0.8433 - auc: 0.8778 - val_loss: 0.4836 - val_accuracy: 0.7762 - val_auc: 0.8629\nEpoch 44/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4164 - accuracy: 0.8125 - auc: 0.9150\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 943us/step - loss: 0.3947 - accuracy: 0.8363 - auc: 0.8782 - val_loss: 0.4826 - val_accuracy: 0.7762 - val_auc: 0.8622\nEpoch 45/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3581 - accuracy: 0.8750 - auc: 0.8790\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 946us/step - loss: 0.3826 - accuracy: 0.8468 - auc: 0.8845 - val_loss: 0.4744 - val_accuracy: 0.7902 - val_auc: 0.8644\nEpoch 46/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3703 - accuracy: 0.8438 - auc: 0.9160\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 933us/step - loss: 0.3836 - accuracy: 0.8327 - auc: 0.8845 - val_loss: 0.4775 - val_accuracy: 0.7832 - val_auc: 0.8640\nEpoch 47/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5499 - accuracy: 0.6875 - auc: 0.7396\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 947us/step - loss: 0.3763 - accuracy: 0.8398 - auc: 0.8850 - val_loss: 0.4829 - val_accuracy: 0.7692 - val_auc: 0.8639\nEpoch 48/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3893 - accuracy: 0.7812 - auc: 0.9028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 940us/step - loss: 0.3882 - accuracy: 0.8169 - auc: 0.8812 - val_loss: 0.4738 - val_accuracy: 0.7902 - val_auc: 0.8610\nEpoch 49/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4616 - accuracy: 0.7812 - auc: 0.8492\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 953us/step - loss: 0.3836 - accuracy: 0.8292 - auc: 0.8847 - val_loss: 0.4774 - val_accuracy: 0.7832 - val_auc: 0.8600\nEpoch 50/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2762 - accuracy: 0.9062 - auc: 0.9275\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 941us/step - loss: 0.3516 - accuracy: 0.8627 - auc: 0.9015 - val_loss: 0.4797 - val_accuracy: 0.7832 - val_auc: 0.8608\n```\n:::\n:::\n\n\n## 可视化学习曲线\n\n观察训练过程中的损失和准确率变化：\n\n::: {#3d89de8b .cell message='false' execution_count=6}\n``` {.python .cell-code}\n# 绘制学习曲线\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 损失曲线\naxes[0].plot(history.history['loss'], label='训练损失', linewidth=2)\naxes[0].plot(history.history['val_loss'], label='验证损失', linewidth=2)\naxes[0].set_xlabel('训练轮数', fontsize=12)\naxes[0].set_ylabel('损失', fontsize=12)\naxes[0].set_title('损失曲线', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# 准确率曲线\naxes[1].plot(history.history['accuracy'], label='训练准确率', linewidth=2)\naxes[1].plot(history.history['val_accuracy'], label='验证准确率', linewidth=2)\naxes[1].set_xlabel('训练轮数', fontsize=12)\naxes[1].set_ylabel('准确率', fontsize=12)\naxes[1].set_title('准确率曲线', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# AUC 曲线\naxes[2].plot(history.history['auc'], label='训练 AUC', linewidth=2)\naxes[2].plot(history.history['val_auc'], label='验证 AUC', linewidth=2)\naxes[2].set_xlabel('训练轮数', fontsize=12)\naxes[2].set_ylabel('AUC', fontsize=12)\naxes[2].set_title('AUC 曲线', fontsize=14)\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"学习曲线观察:\")\nprint(\"- 训练损失应该逐渐下降\")\nprint(\"- 验证损失开始上升可能表示过拟合\")\nprint(\"- 训练和验证指标差距过大也可能表示过拟合\")\n```\n\n::: {.cell-output .cell-output-display}\n![](w6_practice_files/figure-html/cell-7-output-1.png){width=1719 height=470}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n学习曲线观察:\n- 训练损失应该逐渐下降\n- 验证损失开始上升可能表示过拟合\n- 训练和验证指标差距过大也可能表示过拟合\n```\n:::\n:::\n\n\n## 模型评估\n\n在训练集和测试集上评估模型性能：\n\n- 准确率 (Accuracy)\n- AUC (Area Under ROC Curve)\n- 分类报告：精确率、召回率、F1 分数\n\n::: {#bd002fb1 .cell message='false' execution_count=7}\n``` {.python .cell-code}\n# 预测\ny_train_pred_prob = model.predict(X_train).flatten()\ny_test_pred_prob = model.predict(X_test).flatten()\n\ny_train_pred = (y_train_pred_prob > 0.5).astype(int)\ny_test_pred = (y_test_pred_prob > 0.5).astype(int)\n\n# 评估\nprint(\"=== 训练集 ===\")\nprint(f\"准确率: {accuracy_score(y_train, y_train_pred):.3f}\")\nprint(f\"AUC: {roc_auc_score(y_train, y_train_pred_prob):.3f}\")\n\nprint(\"\\n=== 测试集 ===\")\nprint(f\"准确率: {accuracy_score(y_test, y_test_pred):.3f}\")\nprint(f\"AUC: {roc_auc_score(y_test, y_test_pred_prob):.3f}\")\n\nprint(\"\\n分类报告:\")\nprint(classification_report(y_test, y_test_pred, target_names=['死亡', '存活']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r 1/23 [>.............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 0s 275us/step\n\r1/6 [====>.........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6/6 [==============================] - 0s 333us/step\n=== 训练集 ===\n准确率: 0.844\nAUC: 0.894\n\n=== 测试集 ===\n准确率: 0.798\nAUC: 0.854\n\n分类报告:\n              precision    recall  f1-score   support\n\n          死亡       0.81      0.87      0.84       110\n          存活       0.77      0.68      0.72        68\n\n    accuracy                           0.80       178\n   macro avg       0.79      0.77      0.78       178\nweighted avg       0.80      0.80      0.79       178\n\n```\n:::\n:::\n\n\n## MLP vs 随机森林对比\n\n将 MLP 与传统的机器学习算法（随机森林）进行对比。\n\n**注意：** 随机森林不需要特征标准化，所以使用原始特征。\n\n::: {#a4bdceef .cell message='false' execution_count=8}\n``` {.python .cell-code}\n# 准备随机森林的数据（不需要标准化）\nX_train_orig, X_test_orig, _, _ = train_test_split(\n    X.values,\n    y.values,\n    test_size=0.2, random_state=42, stratify=y.values\n)\n\n# 训练随机森林\nrf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\nrf.fit(X_train_orig, y_train)\ny_test_pred_rf = rf.predict_proba(X_test_orig)[:, 1]\n\n# 计算 AUC\nmlp_auc = roc_auc_score(y_test, y_test_pred_prob)\nrf_auc = roc_auc_score(y_test, y_test_pred_rf)\n\n# 对比表格\ncomparison = pd.DataFrame({\n    '模型': ['MLP', '随机森林'],\n    '测试集 AUC': [mlp_auc, rf_auc]\n})\n\nprint(\"模型对比:\")\nprint(comparison.to_string(index=False))\nprint(f\"\\nMLP 优势: {mlp_auc - rf_auc:+.3f} (AUC 差异)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n模型对比:\n  模型  测试集 AUC\n MLP 0.854011\n随机森林 0.846791\n\nMLP 优势: +0.007 (AUC 差异)\n```\n:::\n:::\n\n\n::: {#533b8a2f .cell message='false' execution_count=9}\n``` {.python .cell-code}\n# ROC 曲线对比\nfpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_test_pred_prob)\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_pred_rf)\n\nplt.figure(figsize=(10, 8))\nplt.plot(fpr_mlp, tpr_mlp, linewidth=3, label=f'MLP (AUC={mlp_auc:.3f})')\nplt.plot(fpr_rf, tpr_rf, linewidth=3, label=f'随机森林 (AUC={rf_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='随机猜测')\nplt.xlabel('FPR (False Positive Rate)', fontsize=12)\nplt.ylabel('TPR (True Positive Rate)', fontsize=12)\nplt.title('MLP vs 随机森林 - ROC 曲线对比', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](w6_practice_files/figure-html/cell-10-output-1.png){width=815 height=676}\n:::\n:::\n\n\n## 小结\n\n**结论：** 在 Titanic 数据集上，随机森林通常表现略好于或持平于基础 MLP。这是正常的，因为：\n\n1. **数据集较小**：神经网络需要大量数据才能充分发挥优势\n2. **特征工程**：随机森林对特征预处理要求较低\n3. **模型复杂度**：我们使用的 MLP 结构相对简单\n\n**MLP 的优势将在更大规模的数据集和更复杂的任务中体现。**\n\n**练习任务：**\n\n- 尝试调整 MLP 的隐藏层神经元数量\n- 尝试不同的激活函数\n- 观察过拟合现象（增加 epochs）\n- 尝试其他数据集（如 MNIST 手写数字识别）\n\n**神经网络的关键概念：**\n\n- **层（Layers）**：神经网络的基本构建块\n- **激活函数（Activation）**：为网络引入非线性\n- **损失函数（Loss）**：衡量预测与真实值的差距\n- **优化器（Optimizer）**：更新网络参数的算法\n- **批次（Batch）**：每次更新使用的样本数量\n- **轮次（Epoch）**：完整遍历训练集的次数\n\n**超参数调优建议：**\n\n- **网络结构**：尝试不同的隐藏层数量和神经元数量\n- **学习率**：太大会震荡，太小会收敛慢\n- **批次大小**：通常 32-128 之间\n- **正则化**：Dropout、L1/L2 正则化防止过拟合\n- **早停（Early Stopping）**：监控验证损失，防止过拟合\n\n",
    "supporting": [
      "w6_practice_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}