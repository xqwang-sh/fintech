{
  "hash": "2cb247e034d09808ced402bff749fef5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 第六讲上机实践\n---\n\n\n\n本上机讲义覆盖以下内容：\n\n- 环境准备：安装/验证 TensorFlow/Keras\n- 数据准备：读取 Titanic 预处理后数据\n- 搭建 MLP：使用 Sequential API 构建多层感知机\n- 训练模型：设置超参数并训练\n- 可视化学习曲线：观察训练过程\n- 模型评估：计算准确率、AUC 等指标\n- 模型对比：MLP vs 随机森林\n\n**目标：** 掌握 Keras 构建、训练和评估神经网络的基本流程\n\n## 环境准备\n\n首先，我们需要导入必要的库并验证 TensorFlow/Keras 的安装。\n\n::: {#6058b466 .cell message='false' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import roc_auc_score, accuracy_score, classification_report, roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import fetch_openml\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(f\"TensorFlow 版本: {tf.__version__}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow 版本: 2.15.0\n```\n:::\n:::\n\n\n## 数据准备\n\n我们使用 sklearn 的 `fetch_openml` 直接加载 Titanic 数据集，然后进行数据预处理：\n\n- 处理缺失值\n- 编码分类变量\n- 划分训练/测试集\n- **重要：** 对数值特征进行标准化（神经网络对特征尺度敏感）\n\n::: {#804e6ccc .cell message='false' execution_count=3}\n``` {.python .cell-code}\n# 读取 W4 预处理后的数据\nX = pd.read_csv('titanic_data/features_processed.csv')\ny = pd.read_csv('titanic_data/labels_processed.csv')\n\n# 划分训练/测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# 归一化（重要！神经网络对特征尺度敏感）\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(f\"训练集形状: {X_train.shape}\")\nprint(f\"测试集形状: {X_test.shape}\")\nprint(f\"特征数量: {X_train.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n训练集形状: (711, 8)\n测试集形状: (178, 8)\n特征数量: 8\n```\n:::\n:::\n\n\n## 搭建 MLP 模型\n\n使用 Keras 的 Sequential API 构建多层感知机：\n\n::: {#42c9e0c5 .cell message='false' execution_count=4}\n``` {.python .cell-code}\n# 搭建 MLP 模型\nmodel = keras.Sequential([\n    # 输入层 + 第一个隐藏层\n    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    # Dropout 层（防止过拟合）\n    layers.Dropout(0.2),\n    # 第二个隐藏层\n    layers.Dense(32, activation='relu'),\n    # Dropout 层\n    layers.Dropout(0.2),\n    # 输出层（二分类，使用 sigmoid 激活函数）\n    layers.Dense(1, activation='sigmoid')\n])\n\n# 查看模型结构\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 64)                576       \n                                                                 \n dropout (Dropout)           (None, 64)                0         \n                                                                 \n dense_1 (Dense)             (None, 32)                2080      \n                                                                 \n dropout_1 (Dropout)         (None, 32)                0         \n                                                                 \n dense_2 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 2689 (10.50 KB)\nTrainable params: 2689 (10.50 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n```\n:::\n:::\n\n\n## 编译模型\n\n设置损失函数、优化器和评估指标：\n\n::: {#5569a298 .cell message='false' execution_count=5}\n``` {.python .cell-code}\n# 编译模型\nmodel.compile(\n    optimizer='adam',                    # 优化器\n    loss='binary_crossentropy',          # 二分类损失函数\n    metrics=['accuracy',                 # 准确率\n             tf.keras.metrics.AUC()]     # AUC 指标\n)\n```\n:::\n\n\n## 训练模型\n\n设置训练参数并开始训练：\n\n::: {#03f7743d .cell message='false' execution_count=6}\n``` {.python .cell-code}\n# 训练参数\nepochs = 50          # 训练轮数\nbatch_size = 32      # 批次大小\n\n# 训练模型\nhistory = model.fit(\n    X_train, y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    validation_split=0.2,  # 从训练集中划分验证集\n    verbose=1\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/50\n\r 1/18 [>.............................] - ETA: 3s - loss: 0.7491 - accuracy: 0.5312 - auc: 0.5571\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 6ms/step - loss: 0.6482 - accuracy: 0.6778 - auc: 0.6875 - val_loss: 0.5984 - val_accuracy: 0.7622 - val_auc: 0.8584\nEpoch 2/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.6276 - accuracy: 0.6250 - auc: 0.7798\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.5875 - accuracy: 0.7394 - auc: 0.7785 - val_loss: 0.5292 - val_accuracy: 0.8042 - val_auc: 0.8763\nEpoch 3/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4944 - accuracy: 0.8125 - auc: 0.9286\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.5400 - accuracy: 0.7535 - auc: 0.7998 - val_loss: 0.4789 - val_accuracy: 0.8322 - val_auc: 0.8848\nEpoch 4/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3960 - accuracy: 0.9062 - auc: 0.9697\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4946 - accuracy: 0.7852 - auc: 0.8376 - val_loss: 0.4494 - val_accuracy: 0.8252 - val_auc: 0.8831\nEpoch 5/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5352 - accuracy: 0.6875 - auc: 0.7614\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4680 - accuracy: 0.7694 - auc: 0.8430 - val_loss: 0.4324 - val_accuracy: 0.8392 - val_auc: 0.8842\nEpoch 6/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3498 - accuracy: 0.8438 - auc: 0.9327\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.7923 - auc: 0.8505 - val_loss: 0.4268 - val_accuracy: 0.8462 - val_auc: 0.8833\nEpoch 7/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3168 - accuracy: 0.8750 - auc: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4396 - accuracy: 0.8081 - auc: 0.8578 - val_loss: 0.4258 - val_accuracy: 0.8182 - val_auc: 0.8824\nEpoch 8/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4209 - accuracy: 0.7812 - auc: 0.9000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4348 - accuracy: 0.8028 - auc: 0.8601 - val_loss: 0.4242 - val_accuracy: 0.8392 - val_auc: 0.8808\nEpoch 9/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3885 - accuracy: 0.7500 - auc: 0.9042\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4357 - accuracy: 0.7940 - auc: 0.8545 - val_loss: 0.4261 - val_accuracy: 0.8462 - val_auc: 0.8824\nEpoch 10/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4011 - accuracy: 0.8750 - auc: 0.8961\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.7993 - auc: 0.8564 - val_loss: 0.4255 - val_accuracy: 0.8322 - val_auc: 0.8829\nEpoch 11/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3086 - accuracy: 0.7812 - auc: 0.9500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4243 - accuracy: 0.8116 - auc: 0.8621 - val_loss: 0.4262 - val_accuracy: 0.8252 - val_auc: 0.8836\nEpoch 12/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2755 - accuracy: 0.9375 - auc: 0.9486\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4088 - accuracy: 0.8239 - auc: 0.8727 - val_loss: 0.4304 - val_accuracy: 0.8392 - val_auc: 0.8805\nEpoch 13/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.6135 - accuracy: 0.7500 - auc: 0.7361\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4009 - accuracy: 0.8257 - auc: 0.8748 - val_loss: 0.4301 - val_accuracy: 0.8322 - val_auc: 0.8802\nEpoch 14/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3328 - accuracy: 0.8750 - auc: 0.9458\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4236 - accuracy: 0.8169 - auc: 0.8597 - val_loss: 0.4398 - val_accuracy: 0.8112 - val_auc: 0.8754\nEpoch 15/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5889 - accuracy: 0.6562 - auc: 0.7874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4022 - accuracy: 0.8310 - auc: 0.8755 - val_loss: 0.4421 - val_accuracy: 0.8112 - val_auc: 0.8772\nEpoch 16/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4963 - accuracy: 0.7500 - auc: 0.8239\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.8239 - auc: 0.8726 - val_loss: 0.4435 - val_accuracy: 0.8112 - val_auc: 0.8755\nEpoch 17/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4288 - accuracy: 0.8125 - auc: 0.8588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4005 - accuracy: 0.8222 - auc: 0.8748 - val_loss: 0.4421 - val_accuracy: 0.8112 - val_auc: 0.8764\nEpoch 18/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4893 - accuracy: 0.7500 - auc: 0.7879\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4031 - accuracy: 0.8257 - auc: 0.8731 - val_loss: 0.4421 - val_accuracy: 0.8112 - val_auc: 0.8753\nEpoch 19/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3317 - accuracy: 0.9062 - auc: 0.9039\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3923 - accuracy: 0.8433 - auc: 0.8779 - val_loss: 0.4421 - val_accuracy: 0.8042 - val_auc: 0.8766\nEpoch 20/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4476 - accuracy: 0.8125 - auc: 0.8471\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3900 - accuracy: 0.8345 - auc: 0.8808 - val_loss: 0.4460 - val_accuracy: 0.8112 - val_auc: 0.8762\nEpoch 21/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4767 - accuracy: 0.8125 - auc: 0.8353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4001 - accuracy: 0.8099 - auc: 0.8760 - val_loss: 0.4481 - val_accuracy: 0.8112 - val_auc: 0.8725\nEpoch 22/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2389 - accuracy: 0.9375 - auc: 0.9844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3923 - accuracy: 0.8345 - auc: 0.8807 - val_loss: 0.4466 - val_accuracy: 0.8112 - val_auc: 0.8767\nEpoch 23/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4529 - accuracy: 0.8438 - auc: 0.7818\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.8363 - auc: 0.8632 - val_loss: 0.4505 - val_accuracy: 0.8182 - val_auc: 0.8721\nEpoch 24/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4292 - accuracy: 0.8125 - auc: 0.8271\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4000 - accuracy: 0.8380 - auc: 0.8743 - val_loss: 0.4527 - val_accuracy: 0.8182 - val_auc: 0.8720\nEpoch 25/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4729 - accuracy: 0.7188 - auc: 0.8417\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4034 - accuracy: 0.8275 - auc: 0.8736 - val_loss: 0.4512 - val_accuracy: 0.8182 - val_auc: 0.8715\nEpoch 26/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4781 - accuracy: 0.8125 - auc: 0.8512\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4065 - accuracy: 0.8310 - auc: 0.8676 - val_loss: 0.4512 - val_accuracy: 0.8182 - val_auc: 0.8722\nEpoch 27/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2750 - accuracy: 0.9062 - auc: 0.9808\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8310 - auc: 0.8773 - val_loss: 0.4524 - val_accuracy: 0.8112 - val_auc: 0.8724\nEpoch 28/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4622 - accuracy: 0.7812 - auc: 0.7708\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3884 - accuracy: 0.8345 - auc: 0.8820 - val_loss: 0.4552 - val_accuracy: 0.8112 - val_auc: 0.8732\nEpoch 29/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5014 - accuracy: 0.7812 - auc: 0.7333\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4021 - accuracy: 0.8398 - auc: 0.8680 - val_loss: 0.4594 - val_accuracy: 0.8112 - val_auc: 0.8719\nEpoch 30/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4964 - accuracy: 0.7188 - auc: 0.8194\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3853 - accuracy: 0.8363 - auc: 0.8866 - val_loss: 0.4572 - val_accuracy: 0.8182 - val_auc: 0.8701\nEpoch 31/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3621 - accuracy: 0.8438 - auc: 0.9177\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3899 - accuracy: 0.8398 - auc: 0.8807 - val_loss: 0.4559 - val_accuracy: 0.8112 - val_auc: 0.8705\nEpoch 32/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2246 - accuracy: 0.9375 - auc: 0.9919\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3903 - accuracy: 0.8292 - auc: 0.8835 - val_loss: 0.4572 - val_accuracy: 0.8112 - val_auc: 0.8692\nEpoch 33/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3472 - accuracy: 0.8750 - auc: 0.8382\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.8310 - auc: 0.8734 - val_loss: 0.4601 - val_accuracy: 0.8182 - val_auc: 0.8678\nEpoch 34/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.6066 - accuracy: 0.7188 - auc: 0.7182\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4054 - accuracy: 0.8257 - auc: 0.8698 - val_loss: 0.4577 - val_accuracy: 0.8182 - val_auc: 0.8697\nEpoch 35/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4550 - accuracy: 0.8438 - auc: 0.7633\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3981 - accuracy: 0.8380 - auc: 0.8735 - val_loss: 0.4592 - val_accuracy: 0.8182 - val_auc: 0.8691\nEpoch 36/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3461 - accuracy: 0.8438 - auc: 0.8864\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3936 - accuracy: 0.8363 - auc: 0.8753 - val_loss: 0.4601 - val_accuracy: 0.8112 - val_auc: 0.8708\nEpoch 37/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3895 - accuracy: 0.8750 - auc: 0.9062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3850 - accuracy: 0.8398 - auc: 0.8854 - val_loss: 0.4652 - val_accuracy: 0.7972 - val_auc: 0.8711\nEpoch 38/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3095 - accuracy: 0.8750 - auc: 0.9208\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3894 - accuracy: 0.8380 - auc: 0.8774 - val_loss: 0.4620 - val_accuracy: 0.8182 - val_auc: 0.8699\nEpoch 39/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3395 - accuracy: 0.8750 - auc: 0.9886\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3985 - accuracy: 0.8363 - auc: 0.8680 - val_loss: 0.4612 - val_accuracy: 0.8182 - val_auc: 0.8691\nEpoch 40/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3411 - accuracy: 0.8750 - auc: 0.9451\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3741 - accuracy: 0.8415 - auc: 0.8963 - val_loss: 0.4617 - val_accuracy: 0.8112 - val_auc: 0.8703\nEpoch 41/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5425 - accuracy: 0.7500 - auc: 0.8125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3938 - accuracy: 0.8310 - auc: 0.8766 - val_loss: 0.4629 - val_accuracy: 0.8182 - val_auc: 0.8682\nEpoch 42/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2829 - accuracy: 0.9375 - auc: 0.8237\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3790 - accuracy: 0.8415 - auc: 0.8856 - val_loss: 0.4623 - val_accuracy: 0.8182 - val_auc: 0.8689\nEpoch 43/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.5077 - accuracy: 0.7188 - auc: 0.8213\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3846 - accuracy: 0.8433 - auc: 0.8836 - val_loss: 0.4629 - val_accuracy: 0.8182 - val_auc: 0.8677\nEpoch 44/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3455 - accuracy: 0.8125 - auc: 0.9375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.4084 - accuracy: 0.8187 - auc: 0.8662 - val_loss: 0.4631 - val_accuracy: 0.8112 - val_auc: 0.8672\nEpoch 45/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.3305 - accuracy: 0.9062 - auc: 0.8622\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3759 - accuracy: 0.8380 - auc: 0.8883 - val_loss: 0.4636 - val_accuracy: 0.8182 - val_auc: 0.8695\nEpoch 46/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4033 - accuracy: 0.8438 - auc: 0.7947\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3754 - accuracy: 0.8415 - auc: 0.8891 - val_loss: 0.4653 - val_accuracy: 0.8042 - val_auc: 0.8697\nEpoch 47/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2747 - accuracy: 0.9375 - auc: 0.8454\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3824 - accuracy: 0.8433 - auc: 0.8803 - val_loss: 0.4639 - val_accuracy: 0.8112 - val_auc: 0.8710\nEpoch 48/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2392 - accuracy: 0.8750 - auc: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3815 - accuracy: 0.8468 - auc: 0.8813 - val_loss: 0.4668 - val_accuracy: 0.8042 - val_auc: 0.8687\nEpoch 49/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.4601 - accuracy: 0.7812 - auc: 0.7705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3756 - accuracy: 0.8468 - auc: 0.8908 - val_loss: 0.4678 - val_accuracy: 0.8042 - val_auc: 0.8672\nEpoch 50/50\n\r 1/18 [>.............................] - ETA: 0s - loss: 0.2766 - accuracy: 0.8750 - auc: 0.9697\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r18/18 [==============================] - 0s 1ms/step - loss: 0.3810 - accuracy: 0.8363 - auc: 0.8886 - val_loss: 0.4632 - val_accuracy: 0.8112 - val_auc: 0.8672\n```\n:::\n:::\n\n\n## 可视化学习曲线\n\n观察训练过程中的损失和准确率变化：\n\n::: {#e017de9c .cell message='false' execution_count=7}\n``` {.python .cell-code}\n# 绘制学习曲线\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 损失曲线\naxes[0].plot(history.history['loss'], label='训练损失', linewidth=2)\naxes[0].plot(history.history['val_loss'], label='验证损失', linewidth=2)\naxes[0].set_xlabel('训练轮数', fontsize=12)\naxes[0].set_ylabel('损失', fontsize=12)\naxes[0].set_title('损失曲线', fontsize=14)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# 准确率曲线\naxes[1].plot(history.history['accuracy'], label='训练准确率', linewidth=2)\naxes[1].plot(history.history['val_accuracy'], label='验证准确率', linewidth=2)\naxes[1].set_xlabel('训练轮数', fontsize=12)\naxes[1].set_ylabel('准确率', fontsize=12)\naxes[1].set_title('准确率曲线', fontsize=14)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# AUC 曲线\naxes[2].plot(history.history['auc'], label='训练 AUC', linewidth=2)\naxes[2].plot(history.history['val_auc'], label='验证 AUC', linewidth=2)\naxes[2].set_xlabel('训练轮数', fontsize=12)\naxes[2].set_ylabel('AUC', fontsize=12)\naxes[2].set_title('AUC 曲线', fontsize=14)\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"学习曲线观察:\")\nprint(\"- 训练损失应该逐渐下降\")\nprint(\"- 验证损失开始上升可能表示过拟合\")\nprint(\"- 训练和验证指标差距过大也可能表示过拟合\")\n```\n\n::: {.cell-output .cell-output-display}\n![](w6_practice_files/figure-html/cell-8-output-1.png){width=1718 height=470}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n学习曲线观察:\n- 训练损失应该逐渐下降\n- 验证损失开始上升可能表示过拟合\n- 训练和验证指标差距过大也可能表示过拟合\n```\n:::\n:::\n\n\n## 模型评估\n\n在训练集和测试集上评估模型性能：\n\n- 准确率 (Accuracy)\n- AUC (Area Under ROC Curve)\n- 分类报告：精确率、召回率、F1 分数\n\n::: {#d6cfb400 .cell message='false' execution_count=8}\n``` {.python .cell-code}\n# 预测\ny_train_pred_prob = model.predict(X_train).flatten()\ny_test_pred_prob = model.predict(X_test).flatten()\n\ny_train_pred = (y_train_pred_prob > 0.5).astype(int)\ny_test_pred = (y_test_pred_prob > 0.5).astype(int)\n\n# 评估\nprint(\"=== 训练集 ===\")\nprint(f\"准确率: {accuracy_score(y_train, y_train_pred):.3f}\")\nprint(f\"AUC: {roc_auc_score(y_train, y_train_pred_prob):.3f}\")\n\nprint(\"\\n=== 测试集 ===\")\nprint(f\"准确率: {accuracy_score(y_test, y_test_pred):.3f}\")\nprint(f\"AUC: {roc_auc_score(y_test, y_test_pred_prob):.3f}\")\n\nprint(\"\\n分类报告:\")\nprint(classification_report(y_test, y_test_pred, target_names=['死亡', '存活']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r 1/23 [>.............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 0s 301us/step\n\r1/6 [====>.........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6/6 [==============================] - 0s 363us/step\n=== 训练集 ===\n准确率: 0.847\nAUC: 0.891\n\n=== 测试集 ===\n准确率: 0.803\nAUC: 0.857\n\n分类报告:\n              precision    recall  f1-score   support\n\n          死亡       0.80      0.92      0.85       110\n          存活       0.82      0.62      0.71        68\n\n    accuracy                           0.80       178\n   macro avg       0.81      0.77      0.78       178\nweighted avg       0.81      0.80      0.80       178\n\n```\n:::\n:::\n\n\n## MLP vs 随机森林对比\n\n将 MLP 与传统的机器学习算法（随机森林）进行对比。\n\n**注意：** 随机森林不需要特征标准化，所以使用原始特征。\n\n::: {#9d3e56c7 .cell message='false' execution_count=9}\n``` {.python .cell-code}\n# 准备随机森林的数据（不需要标准化）\nX_train_orig, X_test_orig, _, _ = train_test_split(\n    X.values,\n    y.values,\n    test_size=0.2, random_state=42, stratify=y.values\n)\n\n# 训练随机森林\nrf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\nrf.fit(X_train_orig, y_train)\ny_test_pred_rf = rf.predict_proba(X_test_orig)[:, 1]\n\n# 计算 AUC\nmlp_auc = roc_auc_score(y_test, y_test_pred_prob)\nrf_auc = roc_auc_score(y_test, y_test_pred_rf)\n\n# 对比表格\ncomparison = pd.DataFrame({\n    '模型': ['MLP', '随机森林'],\n    '测试集 AUC': [mlp_auc, rf_auc]\n})\n\nprint(\"模型对比:\")\nprint(comparison.to_string(index=False))\nprint(f\"\\nMLP 优势: {mlp_auc - rf_auc:+.3f} (AUC 差异)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n模型对比:\n  模型  测试集 AUC\n MLP 0.857353\n随机森林 0.846791\n\nMLP 优势: +0.011 (AUC 差异)\n```\n:::\n:::\n\n\n::: {#d152b560 .cell message='false' execution_count=10}\n``` {.python .cell-code}\n# ROC 曲线对比\nfpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_test_pred_prob)\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_pred_rf)\n\nplt.figure(figsize=(10, 8))\nplt.plot(fpr_mlp, tpr_mlp, linewidth=3, label=f'MLP (AUC={mlp_auc:.3f})')\nplt.plot(fpr_rf, tpr_rf, linewidth=3, label=f'随机森林 (AUC={rf_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='随机猜测')\nplt.xlabel('FPR (False Positive Rate)', fontsize=12)\nplt.ylabel('TPR (True Positive Rate)', fontsize=12)\nplt.title('MLP vs 随机森林 - ROC 曲线对比', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](w6_practice_files/figure-html/cell-11-output-1.png){width=807 height=674}\n:::\n:::\n\n\n## 小结\n\n**结论：** 在 Titanic 数据集上，随机森林通常表现略好于或持平于基础 MLP。这是正常的，因为：\n\n1. **数据集较小**：神经网络需要大量数据才能充分发挥优势\n2. **特征工程**：随机森林对特征预处理要求较低\n3. **模型复杂度**：我们使用的 MLP 结构相对简单\n\n**MLP 的优势将在更大规模的数据集和更复杂的任务中体现。**\n\n**练习任务：**\n\n- 尝试调整 MLP 的隐藏层神经元数量\n- 尝试不同的激活函数\n- 观察过拟合现象（增加 epochs）\n- 尝试其他数据集（如 MNIST 手写数字识别）\n\n**神经网络的关键概念：**\n\n- **层（Layers）**：神经网络的基本构建块\n- **激活函数（Activation）**：为网络引入非线性\n- **损失函数（Loss）**：衡量预测与真实值的差距\n- **优化器（Optimizer）**：更新网络参数的算法\n- **批次（Batch）**：每次更新使用的样本数量\n- **轮次（Epoch）**：完整遍历训练集的次数\n\n**超参数调优建议：**\n\n- **网络结构**：尝试不同的隐藏层数量和神经元数量\n- **学习率**：太大会震荡，太小会收敛慢\n- **批次大小**：通常 32-128 之间\n- **正则化**：Dropout、L1/L2 正则化防止过拟合\n- **早停（Early Stopping）**：监控验证损失，防止过拟合\n\n",
    "supporting": [
      "w6_practice_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}