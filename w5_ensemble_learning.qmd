---
title: 第五讲：集成学习 - 随机森林与 GBDT
---

```{python}
#| echo: false
import matplotlib.pyplot as plt
# 根据操作系统设置不同的字体
import platform

# 获取操作系统类型
system = platform.system()

# 设置 matplotlib 字体
if system == 'Windows':
    plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows 使用黑体
elif system == 'Darwin':
    plt.rcParams['font.sans-serif'] = ['Songti SC']  # Mac 使用宋体
else:
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei']  # Linux 使用文泉驿正黑

# 解决负号显示问题
plt.rcParams['axes.unicode_minus'] = False

```

## 开场：单棵树的困境


### 上周回顾

**决策树**：

- 易于理解和可视化
- 能处理非线性问题
- 但有一个致命问题...


### 过拟合演示

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 模拟数据
np.random.seed(42)
X = np.random.rand(100, 2) * 10
y = ((X[:, 0] > 5) & (X[:, 1] > 5)).astype(int)
# 添加噪声
noise_idx = np.random.choice(100, 10, replace=False)
y[noise_idx] = 1 - y[noise_idx]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 训练不同深度的树
depths = [2, 5, 10, None]
fig, axes = plt.subplots(1, 4, figsize=(16, 4))

for idx, depth in enumerate(depths):
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    tree.fit(X_train, y_train)
    
    train_acc = accuracy_score(y_train, tree.predict(X_train))
    test_acc = accuracy_score(y_test, tree.predict(X_test))
    
    # 决策边界
    xx, yy = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 10, 100))
    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')
    axes[idx].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', edgecolors='k', s=50)
    axes[idx].set_title(f'深度={depth}\n训练={train_acc:.2f} 测试={test_acc:.2f}')
    axes[idx].set_xlabel('特征 1')
    axes[idx].set_ylabel('特征 2')

plt.tight_layout()
plt.show()
```

**观察**：

- 深度=2：欠拟合（训练和测试都差）
- 深度=5：恰好（训练和测试接近）
- 深度=10 或无限制：**过拟合**（训练完美，测试很差）


### 本周的解决方案

**核心思想**："三个臭皮匠，顶个诸葛亮"

不再依赖单棵树，而是：

1. 训练多棵不同的树
2. 让它们共同决策（投票/平均）
3. 减少过拟合，提高稳定性

这就是 **集成学习（Ensemble Learning）**


### 本周学习目标

#### 知识目标
1. 理解集成学习的核心思想
2. 掌握 Bagging 和 Boosting 的本质区别
3. 了解随机森林的工作原理
4. 了解 GBDT 的工作原理
5. 理解关键超参数的作用

#### 技能目标
1. 使用 sklearn 训练随机森林
2. 使用 LightGBM 训练 GBDT
3. 进行超参数调整
4. 对比多个模型性能
5. 绘制特征重要性图


## 第一部分：集成学习核心思想


### 类比：专家团队 vs 单个专家

#### 单个专家（单棵树）

- 可能有盲点
- 受个人经验限制
- 容易出错

#### 专家团队（多棵树）

- 多个视角
- 互补长短
- 投票决策，更可靠


### 集成学习的数学直觉

假设每个模型准确率 60%（略强于随机猜测 50%）

- **1 个模型**：准确率 60%
- **3 个模型投票**：至少 2 个正确的概率 ≈ 65%
- **5 个模型投票**：至少 3 个正确的概率 ≈ 68%
- **更多模型**：准确率持续提升

**前提**：各模型的错误要**独立**（不能都在同一个地方出错）


### 如何保证模型"不一样"？

```{mermaid}
graph TD
    A[如何让多个模型不同?] --> B[Bagging<br/>数据随机]
    A --> C[Boosting<br/>关注错误]
    
    B --> B1[每个模型用不同的训练数据]
    B --> B2[代表: 随机森林]
    
    C --> C1[后面的模型专注于<br/>前面模型的错误]
    C --> C2[代表: GBDT/XGBoost]
    
    style B fill:#c8e6c9
    style C fill:#bbdefb
```


## 第二部分：Bagging 与随机森林


### Bagging 原理

**Bagging = Bootstrap Aggregating**


#### Step 1: Bootstrap（自助采样）

**问题**：只有一份训练数据，如何创造多份不同的数据？

**方法**：有放回地随机抽样

```{python}
# 演示 Bootstrap
np.random.seed(42)
original_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

fig, axes = plt.subplots(1, 4, figsize=(16, 4))

axes[0].bar(range(len(original_data)), original_data, color='skyblue')
axes[0].set_title('原始数据 (10个样本)', fontsize=12)
axes[0].set_xlabel('索引')
axes[0].set_ylabel('值')

for i in range(1, 4):
    bootstrap_sample = np.random.choice(original_data, size=10, replace=True)
    axes[i].bar(range(len(bootstrap_sample)), bootstrap_sample, alpha=0.7)
    axes[i].set_title(f'Bootstrap 样本 {i}', fontsize=12)
    axes[i].set_xlabel('索引')
    axes[i].set_ylabel('值')
    axes[i].set_ylim(0, 11)
    
    # 标注哪些是重复的
    unique_count = len(np.unique(bootstrap_sample))
    axes[i].text(0.5, 10, f'唯一值: {unique_count}/10', fontsize=10, ha='center')

plt.tight_layout()
plt.show()
```

**特点**：

- 每个 Bootstrap 样本大小与原数据相同
- 某些样本会被重复选中
- 约 63% 的唯一样本（其余是重复）


#### Step 2: Aggregating（聚合）

```{mermaid}
graph TD
    A[原始训练数据<br/>1000个样本] --> B[Bootstrap 1]
    A --> C[Bootstrap 2]
    A --> D[Bootstrap 3]
    A --> E[...]
    A --> F[Bootstrap M]
    
    B --> G[树1]
    C --> H[树2]
    D --> I[树3]
    E --> J[...]
    F --> K[树M]
    
    G --> L[投票/平均]
    H --> L
    I --> L
    J --> L
    K --> L
    
    L --> M[最终预测]
    
    style A fill:#fff9c4
    style L fill:#c8e6c9
    style M fill:#4caf50,color:#fff
```


#### Bagging 完整流程

1. **重复 M 次**：
   - 从训练集中 Bootstrap 采样
   - 在该样本上训练一棵树
   
2. **预测时**：
   - 分类：M 棵树投票，多数票获胜
   - 回归：M 棵树预测值的平均

3. **为什么有效**：
   - 每棵树看到不同的数据
   - 每棵树的误差方向不同
   - 投票后误差相互抵消


### 随机森林（Random Forest）

**随机森林 = Bagging + 特征随机**


#### 额外的随机性：特征随机选择

**问题**：如果某个特征特别强（如 Titanic 中的性别），所有树都会优先用它分裂 → 树之间太相似

**解决**：每次分裂时，只考虑 **随机选择的一部分特征**


#### 特征随机演示

```{mermaid}
graph TD
    A[9个特征] --> B[每次分裂时]
    B --> C[随机选3个特征<br/>√9≈3]
    
    C --> D[树1选: 特征2, 5, 8]
    C --> E[树2选: 特征1, 3, 6]
    C --> F[树3选: 特征4, 7, 9]
    
    D --> G[树之间更不同]
    E --> G
    F --> G
    
    G --> H[集成效果更好]
    
    style C fill:#fff9c4
    style H fill:#4caf50,color:#fff
```

**常用规则**：

- 分类：每次考虑 $\sqrt{p}$ 个特征（p = 总特征数）
- 回归：每次考虑 $p/3$ 个特征


### 随机森林关键参数

| 参数 | 含义 | 默认值 | 调参建议 |
|------|------|--------|----------|
| `n_estimators` | 树的数量 | 100 | 越多越好（但速度慢）<br/>50-200 |
| `max_depth` | 每棵树的最大深度 | None | 通常 5-20<br/>防止过拟合 |
| `max_features` | 每次分裂考虑的特征数 | √p | 可尝试 log₂(p) |
| `min_samples_split` | 分裂所需最小样本数 | 2 | 增大防止过拟合<br/>5-20 |
| `min_samples_leaf` | 叶节点最小样本数 | 1 | 增大防止过拟合<br/>2-10 |


### 随机森林 vs 单棵决策树

| 维度 | 决策树 | 随机森林 |
|------|--------|----------|
| **偏差** | 低 | 低 |
| **方差** | 高（易过拟合） | **低（稳定）** ✓ |
| **训练速度** | 快 | 慢（M 棵树） |
| **预测速度** | 快 | 慢（M 次预测） |
| **可解释性** | 高 | 低（黑盒） |
| **准确率** | 中 | **高** ✓ |

**核心优势**：**降低方差，防止过拟合**


## 第三部分：Boosting 与 GBDT


### Boosting 核心思想

**Bagging**：独立专家各自诊断，最后投票

**Boosting**：第一个医生诊断，第二个医生专门看第一个医生漏掉的症状


#### Bagging vs Boosting

```{mermaid}
graph TD
    A[Bagging] --> A1[并行训练]
    A --> A2[数据: Bootstrap]
    A --> A3[目标: 降低方差]
    A --> A4[代表: 随机森林]
    
    B[Boosting] --> B1[串行训练]
    B --> B2[数据: 加权/残差]
    B --> B3[目标: 降低偏差]
    B --> B4[代表: GBDT/XGBoost]
    
    style A fill:#c8e6c9
    style B fill:#bbdefb
```


#### Boosting 流程

```{mermaid}
graph LR
    A[训练集] --> B[树1]
    B --> C[找出错误样本]
    C --> D[树2<br/>专注错误]
    D --> E[再找错误]
    E --> F[树3<br/>继续修正]
    F --> G[...]
    G --> H[组合所有树]
    
    style B fill:#fff9c4
    style D fill:#ffccbc
    style F fill:#ffccbc
    style H fill:#4caf50,color:#fff
```

**关键**：后面的树专注于前面树的错误


### GBDT 原理

**GBDT = Gradient Boosting Decision Tree**

**核心思想**：每棵新树去拟合之前所有树的"预测误差"（残差）


#### GBDT 示例（回归）

**数据**：真实房价 [100, 150, 200]

**Step 1**：初始预测 = 平均值 = 150

```
真实值: [100, 150, 200]
预测值: [150, 150, 150]
残差:   [-50,   0,  50]
```

**Step 2**：训练树1 拟合残差 [-50, 0, 50]

假设树1学到：`预测残差 = [-45, 0, 45]`

更新预测 = 150 + 0.1 × [-45, 0, 45] = [145.5, 150, 154.5]

（0.1 是学习率）


**Step 3**：计算新残差

```
真实值: [100, 150, 200]
预测值: [145.5, 150, 154.5]
新残差: [-45.5,   0,  45.5]
```

**Step 4**：训练树2 拟合新残差，继续更新...


#### GBDT 可视化

```{python}
# GBDT 逐步修正示意
np.random.seed(42)
X_demo = np.linspace(0, 10, 50).reshape(-1, 1)
y_demo = 2*X_demo.flatten() + 3 + np.random.randn(50)*2

from sklearn.ensemble import GradientBoostingRegressor

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

stages = [1, 5, 20, 100]
for idx, n_estimators in enumerate(stages):
    ax = axes[idx//2, idx%2]
    
    gbdt = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=0.1, 
                                      max_depth=3, random_state=42)
    gbdt.fit(X_demo, y_demo)
    y_pred = gbdt.predict(X_demo)
    
    ax.scatter(X_demo, y_demo, alpha=0.5, label='真实数据')
    ax.plot(X_demo, y_pred, 'r-', linewidth=2, label=f'{n_estimators}棵树')
    ax.set_xlabel('X')
    ax.set_ylabel('y')
    ax.set_title(f'GBDT - {n_estimators} 棵树')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

**观察**：随着树数量增加，拟合越来越精确


### GBDT 关键参数

| 参数 | 含义 | 默认值 | 调参建议 |
|------|------|--------|----------|
| `n_estimators` | 树的数量 | 100 | 100-1000<br/>配合 learning_rate |
| `learning_rate` | 学习率 | 0.1 | **0.01-0.3**<br/>越小需要越多树 |
| `max_depth` | 树的深度 | 3 | 3-8<br/>通常用**浅树** |
| `subsample` | 每次迭代用多少数据 | 1.0 | 0.8-1.0<br/>防止过拟合 |


#### 学习率的作用

```{python}
# 学习率对比
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

learning_rates = [0.01, 0.1, 0.5]

for idx, lr in enumerate(learning_rates):
    ax = axes[idx]
    
    train_scores = []
    test_scores = []
    
    for n in range(1, 101):
        gbdt = GradientBoostingRegressor(n_estimators=n, learning_rate=lr, 
                                          max_depth=3, random_state=42)
        gbdt.fit(X_demo, y_demo)
        train_scores.append(gbdt.score(X_demo, y_demo))
    
    ax.plot(range(1, 101), train_scores, label=f'lr={lr}')
    ax.set_xlabel('树的数量')
    ax.set_ylabel('R² 分数')
    ax.set_title(f'学习率 = {lr}')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

**规律**：

- lr 小（0.01）：收敛慢，需要更多树，但最终可能更好
- lr 大（0.5）：收敛快，但容易过拟合


## 总结


### 本讲知识回顾

#### 集成学习

- **核心思想**："三个臭皮匠顶个诸葛亮"
- **关键**：让多个模型"不一样"

#### Bagging（随机森林）

- **方法**：Bootstrap 采样 + 特征随机
- **优点**：降低方差，防止过拟合
- **适用**：数据噪声大、需要稳定模型

#### Boosting（GBDT）

- **方法**：串行训练，逐步修正错误
- **优点**：降低偏差，精度高
- **适用**：数据干净、追求极致精度


### Bagging vs Boosting 总结

| 维度 | Bagging（随机森林） | Boosting（GBDT） |
|------|---------------------|------------------|
| **训练方式** | 并行 | 串行 |
| **数据** | Bootstrap | 加权/残差 |
| **降低** | 方差（过拟合） | 偏差（欠拟合） |
| **速度** | 可并行，较快 | 串行，较慢 |
| **鲁棒性** | 对异常值鲁棒 | 对异常值敏感 |
| **调参** | 较简单 | 较复杂 |
| **何时用** | 数据噪声大 | 数据干净、追求精度 |

**选择建议**：

- 不确定 → **都试试**
- 需要稳定 → 随机森林
- 追求精度 → GBDT


### 实战要点

1. **先用默认参数跑通**  
   了解基线性能

2. **对比多个模型**  
   决策树 vs 随机森林 vs GBDT

3. **查看特征重要性**  
   了解哪些特征重要

4. **调参时控制变量**  
   一次只改一个参数

5. **警惕过拟合**  
   训练集和测试集差距过大是警报


## Q&A

**Q1：集成学习（Ensemble Learning）试图解决单棵决策树的什么核心问题？**

**A：**
核心问题是过拟合（Overfitting）。单棵决策树如果深度没有限制，很容易学到训练数据中的噪声，导致训练集表现完美，但测试集表现很差。集成学习通过组合多棵树的决策来降低这种过拟合，提高模型的稳定性和泛化能力。


**Q2：Bagging（例如随机森林）和 Boosting（例如 GBDT）在训练模型时，最核心的区别是什么？**

**A：**
最核心的区别在于模型的训练方式：

* Bagging (并行)：像一个“专家团队”，每棵树（专家）独立并行地在不同的数据子集上训练，最后“投票”决定结果。
* Boosting (串行)：像一个“师徒体系”，树（学徒）是串行训练的，后一棵树的主要任务是专注于修正前一棵树犯下的错误（残差）。


**Q3：随机森林（Random Forest）中的“随机”体现在哪两个方面？**

**A：**
体现在两个方面：

1. 数据随机（Bootstrap）：每棵树的训练数据都是从原始数据集中有放回地随机抽样（Bootstrap）得到的。
2. 特征随机：在构建每棵树的每个分裂节点时，并不会考虑所有特征，而是随机选择一部分特征（例如 $\sqrt{p}$ 个）作为候选，再从中选择最好的分裂点。


**Q4：GBDT (梯度提升决策树) 的核心思想是什么？（即，后一棵树是如何“修正”前一棵树的？）**

**A：**
GBDT 的核心思想是拟合残差（Residuals）。

1.  模型从一个初始预测（例如平均值）开始。
2.  第一棵树训练的目标是拟合真实值与初始预测之间的误差（残差）。
3.  第二棵树训练的目标是拟合真实值与“初始预测+第一棵树预测”之间的新残差。
4.  以此类推，每棵新树都在逐步修正前面所有树累积下来的预测误差。


**Q5：Bagging（随机森林）和 Boosting（GBDT）分别主要致力于降低哪种误差（偏差 vs 方差）？**

**A：**

* Bagging (随机森林) 主要通过平均/投票来抵消噪声，降低方差（Variance），解决的是过拟合问题。
* Boosting (GBDT) 主要通过不断修正错误来提高模型精度，降低偏差（Bias），解决的是欠拟合问题。


**Q6：为什么随机森林在“数据随机”之外，还需要引入“特征随机”？**

**A：**
这是为了“去相关性”，保证树的多样性。

* 问题：假设数据中有一个特征（例如“性别”）特别重要，如果 Bagging 时每棵树都能看到所有特征，那么几乎每棵树都会在根节点附近使用这个强特征进行分裂。
* 后果：这会导致所有树的结构都非常相似，它们会犯同样的错误。
* 解决：“特征随机”强迫一些树在分裂时“看不到”那个强特征，必须寻找其他次优特征来分裂。这使得每棵树长得更不一样，它们之间的相关性降低，集成“投票”时的纠错能力更强，最终的集成效果更好。


**Q7：在 GBDT 中，`learning_rate`（学习率）和 `n_estimators`（树的数量）之间是什么关系？我们为什么通常倾向于使用“较小”的学习率？**

**A：**

* 关系：它们是权衡（Trade-off）关系。`learning_rate` 控制了每棵树修正错误的“步长”。
    * 高 `learning_rate` (如 0.5)：收敛快，但可能“冲过头”导致过拟合，需要的 `n_estimators` 较少。
    * 低 `learning_rate` (如 0.01)：收敛慢，模型拟合更精细，需要更多的 `n_estimators` (树) 才能达到同样的拟合程度，但通常泛化能力更强。
* 为何用较小的 LR：我们倾向于用“较小”的学习率（如 0.01-0.1）配合“较多”的树（`n_estimators`），这被称为“Shrinkage”。这使得模型在修正错误的道路上“小步慢走”，防止在训练过程中过早地过拟合，从而找到一个更稳健、泛化能力更强的最终模型。


**Q8：为什么 Boosting (GBDT) 对异常值（Outliers）比 Bagging (随机森林) 更敏感？**

**A：**
* Boosting (GBDT)：因为 GBDT 的核心机制是串行地关注错误（残差）。异常值（Outlier）通常会产生巨大的残差。在后续的迭代中，GBDT 会越来越“专注”地试图去拟合这个由异常值导致的巨大残差，这会扭曲整个模型，导致模型为了迁就一个异常点而牺牲了整体的泛化能力。
* Bagging (随机森林)：随机森林是并行地，且最后通过投票或平均来聚合结果。异常值可能只会影响到少数（包含该异常值的 Bootstrap 样本）树的决策，但在最终的“民主投票”中，这些少数树的错误决策很容易被其他大量正确的树“淹没”或“平均掉”，因此模型整体表现更稳健（鲁棒）。


**Q9：在 GBDT 和随机森林中，我们通常对单棵树的 `max_depth`（最大深度）有截然不同的设置（一个深一个浅）。请问哪个模型通常使用“浅树”，哪个使用“深树”，为什么？**

**A：**

* GBDT (Boosting)：通常使用“浅树”（例如 `max_depth` = 3 到 8）。
    * 原因：GBDT 的强大之处在于“集成”而非“单树”。它依靠大量简单的（高偏差、低方差）浅树串行叠加，逐步降低整体模型的偏差。如果单棵树太深（低偏差、高方差），模型会过快地过拟合，失去 Boosting 的优势。
* 随机森林 (Bagging)：通常使用“深树”（例如 `max_depth` = None 或 10-20），让树充分生长。
    * 原因：随机森林的目标是降低方差。它希望每棵树都是一个“低偏差、高方差”的“聪明但偏科”的专家。通过让每棵树充分生长（深树）来保证“低偏差”（拟合能力强）。然后通过 Bagging 和特征随机来保证树之间的差异性，最后通过“投票”来消除“高方差”，实现整体的低方差。