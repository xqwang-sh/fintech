---
title: 第三讲：线性回归的正则化
---

```{python}
#| echo: false
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
# 根据操作系统设置不同的字体
import platform

# 获取操作系统类型
system = platform.system()

# 设置 matplotlib 字体
if system == 'Windows':
    plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows 使用黑体
elif system == 'Darwin':
    plt.rcParams['font.sans-serif'] = ['Songti SC']  # Mac 使用宋体
else:
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei']  # Linux 使用文泉驿正黑

# 解决负号显示问题
plt.rcParams['axes.unicode_minus'] = False

```

## 开场：一个问题

---

### 当模型"太聪明"时...

**场景**：你训练了一个房价预测模型

| 数据集 | RMSE | 表现 |
|--------|------|------|
| 训练集 | 5万 | ✓ 很好 |
| 测试集 | 50万 | ✗ 糟糕 |

**问题出在哪里？**

::: {.fragment}
🚨 **过拟合（Overfitting）**：模型记住了训练数据的细节（包括噪声），但不能泛化到新数据！
:::

---

### 本周学习目标

#### 知识目标
1. 理解过拟合与欠拟合的概念
2. 理解正则化的基本思想（限制模型复杂度）
3. 掌握 Ridge、Lasso、Elastic Net 的区别和应用场景
4. 了解交叉验证的作用

#### 技能目标
1. 使用 sklearn 训练正则化回归模型
2. 使用交叉验证选择最优正则化参数
3. 绘制正则化路径图
4. 解释 Lasso 的特征选择结果

#### 核心理念

**"简单的模型往往更好"（Occam's Razor）**  
复杂度与泛化能力的权衡是机器学习的核心！

---

## 第一部分：过拟合与欠拟合

---

### 用多项式回归理解过拟合

#### 真实场景模拟

假设真实关系是： $y = 2x + 1 + \epsilon$，其中 $\epsilon$ 是噪声。

我们用不同次数的多项式拟合：

```{python}
#| echo: false
#| fig-width: 14
#| fig-height: 5

np.random.seed(42)
# 生成数据
X_true = np.linspace(0, 10, 20)
y_true = 2 * X_true + 1 + np.random.randn(20) * 3

# 测试数据
X_test = np.linspace(0, 10, 100)
y_test_true = 2 * X_test + 1

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

degrees = [1, 4, 15]
titles = ['欠拟合（次数=1）', '刚刚好（次数=4）', '过拟合（次数=15）']
colors = ['green', 'blue', 'red']

for idx, (degree, title, color) in enumerate(zip(degrees, titles, colors)):
    # 多项式特征
    poly = PolynomialFeatures(degree=degree)
    X_poly = poly.fit_transform(X_true.reshape(-1, 1))
    X_test_poly = poly.transform(X_test.reshape(-1, 1))
    
    # 训练
    model = LinearRegression()
    model.fit(X_poly, y_true)
    y_pred = model.predict(X_test_poly)
    
    # 计算误差
    train_mse = mean_squared_error(y_true, model.predict(X_poly))
    test_mse = mean_squared_error(y_test_true, y_pred)
    
    # 绘图
    axes[idx].scatter(X_true, y_true, alpha=0.6, s=80, label='训练数据', zorder=3)
    axes[idx].plot(X_test, y_test_true, 'k--', alpha=0.3, linewidth=2, label='真实关系')
    axes[idx].plot(X_test, y_pred, color=color, linewidth=2.5, label=f'拟合曲线')
    axes[idx].set_xlabel('X', fontsize=12)
    axes[idx].set_ylabel('y', fontsize=12)
    axes[idx].set_title(f'{title}\n训练MSE={train_mse:.1f}, 测试MSE={test_mse:.1f}', fontsize=12)
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)
    axes[idx].set_ylim(-5, 25)

plt.tight_layout()
plt.show()
```

---

### 欠拟合 vs 过拟合 vs 刚刚好

| 模型状态   | 模型复杂度 | 训练集误差 | 测试集误差 | 问题         | 表现                  |
|:--------:|:---------:|:----------:|:----------:|:-----------:|:---------------------:|
| 欠拟合    | 太简单    | 大         | 大         | 无法学到规律 | 🙁 差                 |
| 刚刚好    | 适中      | 小         | 小         | -           | 😊 好                 |
| 过拟合    | 太复杂    | 非常小     | 大         | 记住了噪声   | 😱 看起来好但实际差    |

**关键观察**：

- 欠拟合：训练和测试都差
- 过拟合：训练很好，测试很差（**差距大**是关键信号）
- 刚刚好：训练和测试都还不错，且差距小

---

### Bias-Variance Tradeoff（偏差-方差权衡）

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 6

# 模拟 bias-variance tradeoff
complexity = np.linspace(1, 20, 50)
bias = 10 / complexity  # 偏差随复杂度降低
variance = 0.05 * complexity**1.5  # 方差随复杂度增加
total_error = bias + variance + 0.5  # 总误差

plt.figure(figsize=(10, 6))
plt.plot(complexity, bias, 'b-', linewidth=2.5, label='偏差（Bias）', marker='o', markersize=4, markevery=5)
plt.plot(complexity, variance, 'r-', linewidth=2.5, label='方差（Variance）', marker='s', markersize=4, markevery=5)
plt.plot(complexity, total_error, 'g-', linewidth=3, label='总误差（Bias + Variance）', marker='^', markersize=5, markevery=5)

# 标注最优点
optimal_idx = np.argmin(total_error)
plt.axvline(x=complexity[optimal_idx], color='purple', linestyle='--', alpha=0.7, linewidth=2)
plt.plot(complexity[optimal_idx], total_error[optimal_idx], 'purple', marker='*', markersize=20, label='最优复杂度')

plt.xlabel('模型复杂度', fontsize=13)
plt.ylabel('误差', fontsize=13)
plt.title('偏差-方差权衡：找到最优平衡点', fontsize=15)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)

# 添加区域标注
plt.text(3, 8, '欠拟合区域\n（偏差高）', fontsize=11, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
plt.text(15, 8, '过拟合区域\n（方差高）', fontsize=11, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))

plt.tight_layout()
plt.show()
```

**直观理解**：
 
- **偏差（Bias）**：模型的"先天不足"，太简单无法拟合真实关系
- **方差（Variance）**：模型对训练数据的"过度敏感"，训练数据稍有变化预测就变很多
- **目标**：找到偏差和方差都较低的平衡点

---

## 第二部分：正则化的直觉

---

### 什么是正则化？

#### 问题

**普通线性回归**的目标：
$$
\min \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 = \min \text{MSE}
$$

只关心**拟合训练数据**，不管模型复杂度！

#### 正则化的想法

**在损失函数中加入惩罚项**：
$$
\min \underbrace{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}_{\text{拟合误差}} + \underbrace{\alpha \times \text{惩罚项}}_{\text{复杂度惩罚}}
$$

- **拟合误差**：让模型拟合数据
- **复杂度惩罚**：防止模型过于复杂
- **α（alpha）**：权衡两者的超参数

---

### 正则化的类比

**类比 1：给模型"戴镣铐"**

- 不正则化 = 模型完全自由，可能"乱跑"（过拟合）
- 正则化 = 给模型加限制，让它"老实点"

**类比 2：奥卡姆剃刀（Occam's Razor）**

> "如无必要，勿增实体"

- 在同等拟合效果下，选择更简单的模型
- 简单模型 = 系数小、特征少

---

### 正则化的几何直觉

```{python}
#| echo: false
#| fig-width: 12
#| fig-height: 5

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# L2 正则化（Ridge）- 圆形约束
theta = np.linspace(0, 2*np.pi, 100)
circle_x = np.cos(theta)
circle_y = np.sin(theta)

axes[0].plot(circle_x, circle_y, 'b-', linewidth=3, label='L2约束区域')
axes[0].fill(circle_x, circle_y, alpha=0.2, color='blue')
axes[0].plot([0, 0.7], [0, 0.7], 'ro-', markersize=10, linewidth=2, label='约束后的系数')
axes[0].plot(0, 0, 'go', markersize=15, alpha=0.5, label='原点（所有系数=0）')
axes[0].set_xlabel('系数 w₁', fontsize=12)
axes[0].set_ylabel('系数 w₂', fontsize=12)
axes[0].set_title('Ridge (L2)：圆形约束\n系数变小但不为0', fontsize=13)
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)
axes[0].set_xlim(-1.5, 1.5)
axes[0].set_ylim(-1.5, 1.5)
axes[0].axhline(y=0, color='k', linewidth=0.5)
axes[0].axvline(x=0, color='k', linewidth=0.5)

# L1 正则化（Lasso）- 菱形约束
diamond_x = [1, 0, -1, 0, 1]
diamond_y = [0, 1, 0, -1, 0]

axes[1].plot(diamond_x, diamond_y, 'r-', linewidth=3, label='L1约束区域')
axes[1].fill(diamond_x, diamond_y, alpha=0.2, color='red')
axes[1].plot([0, 1], [0, 0], 'ro-', markersize=10, linewidth=2, label='约束后的系数')
axes[1].plot(0, 0, 'go', markersize=15, alpha=0.5, label='原点（所有系数=0）')
axes[1].set_xlabel('系数 w₁', fontsize=12)
axes[1].set_ylabel('系数 w₂', fontsize=12)
axes[1].set_title('Lasso (L1)：菱形约束\n系数容易变为0（碰到坐标轴）', fontsize=13)
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)
axes[1].set_xlim(-1.5, 1.5)
axes[1].set_ylim(-1.5, 1.5)
axes[1].axhline(y=0, color='k', linewidth=0.5)
axes[1].axvline(x=0, color='k', linewidth=0.5)

plt.tight_layout()
plt.show()
```

**关键区别**：

- **L2（圆形）**：约束边界是平滑的，最优解很少正好在坐标轴上 → 系数小但不为0
- **L1（菱形）**：约束边界有"尖角"在坐标轴上，最优解容易碰到尖角 → 某些系数直接为0

---

## 第三部分：三种正则化方法

---

### Ridge 回归（L2 正则化）

#### 损失函数

$$
\min \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \alpha \sum_{j=1}^{p}w_j^2
$$

**惩罚项**：$ \alpha \sum_{j=1}^{p}w_j^2 $ = 权重的平方和

#### 特点

✅ **让所有权重变小**  
✅ **不会让权重变成 0**  
✅ **适合所有特征都重要的情况**  
✅ **对多重共线性有帮助**

---

### Lasso 回归（L1 正则化）

#### 损失函数

$$
\min \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \alpha \sum_{j=1}^{p}|w_j|
$$

**惩罚项**： $ \alpha \sum_{j=1}^{p}|w_j| $ = 权重的绝对值和

#### 特点

✅ **让某些权重直接变为 0**（自动特征选择）  
✅ **产生稀疏模型**（只保留重要特征）  
✅ **适合认为只有少数特征重要的情况**  
✅ **结果易解释**（特征少）

---

### Elastic Net（L1 + L2）

#### 损失函数

$$
\min \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \alpha \left( \rho \sum_{j=1}^{p}|w_j| + \frac{1-\rho}{2} \sum_{j=1}^{p}w_j^2 \right)
$$

**两个超参数**：

- **α**：正则化强度
- **ρ**（l1_ratio）：L1 和 L2 的混合比例（0 = 纯 L2，1 = 纯 L1）

#### 特点

✅ **结合 Ridge 和 Lasso 的优点**  
✅ **在相关特征较多时表现更好**  
✅ **不确定用哪个时的折中选择**

---

### 三种方法对比总结
| 方法             | 正则化项                | 权重特点        | 特征选择 | 适用场景         | sklearn类         |
|------------------|------------------------|----------------|----------|------------------|-------------------|
| 普通线性回归     | 无                     | 可能很大       | 否       | 特征少、数据多   | LinearRegression  |
| Ridge (L2)       | ∑w²                    | 变小但不为0    | 否       | 特征都重要       | Ridge             |
| Lasso (L1)       | ∑\|w\|                 | 部分变为0      | 是       | 特征稀疏         | Lasso             |
| Elastic Net      | ρ∑\|w\| + (1-ρ)∑w²    | 折中           | 是       | 不确定时         | ElasticNet        |

---

## 第四部分：正则化参数 α 的选择

---

### α 的作用

```{python}
#| echo: false
#| fig-width: 12
#| fig-height: 5

from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

# 生成模拟数据
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=20, n_informative=5, noise=10, random_state=42)

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 切分数据
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# 展示不同 alpha 的效果
alphas = [0.001, 0.1, 1, 10, 100]
train_scores = []
test_scores = []

for alpha in alphas:
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    train_scores.append(model.score(X_train, y_train))
    test_scores.append(model.score(X_test, y_test))

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 图1：R² vs alpha
axes[0].plot(alphas, train_scores, 'bo-', linewidth=2, markersize=8, label='训练集 R²')
axes[0].plot(alphas, test_scores, 'ro-', linewidth=2, markersize=8, label='测试集 R²')
axes[0].set_xlabel('α (正则化强度)', fontsize=12)
axes[0].set_ylabel('R²', fontsize=12)
axes[0].set_title('正则化参数 α 对模型性能的影响', fontsize=13)
axes[0].set_xscale('log')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)
axes[0].axvline(x=1, color='green', linestyle='--', alpha=0.5, label='可能的最优α')

# 图2：系数大小 vs alpha
axes[1].set_xlabel('α (正则化强度)', fontsize=12)
axes[1].set_ylabel('系数绝对值和', fontsize=12)
axes[1].set_title('α 越大，系数越小', fontsize=13)

coef_sums = []
for alpha in np.logspace(-3, 3, 50):
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    coef_sums.append(np.sum(np.abs(model.coef_)))

axes[1].plot(np.logspace(-3, 3, 50), coef_sums, 'b-', linewidth=2.5)
axes[1].set_xscale('log')
axes[1].grid(True, alpha=0.3)
axes[1].fill_between(np.logspace(-3, 3, 50), 0, coef_sums, alpha=0.2)

plt.tight_layout()
plt.show()
```

**关键点**：

- **α = 0**：无正则化（普通线性回归）
- **α 很小**：弱正则化，接近普通回归
- **α 适中**：平衡拟合与复杂度 ✓
- **α 很大**：强正则化，可能欠拟合

---

### 交叉验证（Cross-Validation）

#### 为什么需要交叉验证？

**错误做法** ❌：在测试集上尝试不同的 α，选最好的

- 这样会"泄露"测试集信息
- 导致对模型效果的过于乐观估计

**正确做法** ✓：用交叉验证在训练集上选 α

#### K-Fold 交叉验证

```{python}
#| echo: false
#| fig-width: 10
#| fig-height: 6

from matplotlib.patches import Rectangle

fig, ax = plt.subplots(figsize=(10, 6))

# 5-Fold 示例
k = 5
colors_train = ['lightblue'] * k
colors_val = ['lightcoral']

for fold in range(k):
    y_pos = k - fold - 1
    
    # 画训练集
    for i in range(k):
        if i == fold:
            # 验证集
            rect = Rectangle((i * 0.18, y_pos * 0.15), 0.18, 0.1, 
                           facecolor='lightcoral', edgecolor='red', linewidth=2)
        else:
            # 训练集
            rect = Rectangle((i * 0.18, y_pos * 0.15), 0.18, 0.1, 
                           facecolor='lightblue', edgecolor='blue', linewidth=1)
        ax.add_patch(rect)
    
    ax.text(-0.15, y_pos * 0.15 + 0.05, f'Fold {fold+1}', 
           fontsize=11, verticalalignment='center', weight='bold')

# 图例
legend_elements = [
    Rectangle((0, 0), 1, 1, facecolor='lightblue', edgecolor='blue', label='训练集'),
    Rectangle((0, 0), 1, 1, facecolor='lightcoral', edgecolor='red', label='验证集')
]
ax.legend(handles=legend_elements, loc='upper right', fontsize=11)

ax.set_xlim(-0.2, 1.0)
ax.set_ylim(-0.1, 0.9)
ax.set_title('5-Fold 交叉验证示意图\n每次用不同的1/5作为验证集，其余作为训练集', fontsize=14, weight='bold')
ax.axis('off')

plt.tight_layout()
plt.show()
```

**流程**：

1. 将训练集分成 K 份（例如 K=5）
2. 每次用其中 1 份作验证，其余 K-1 份训练
3. 重复 K 次，得到 K 个评分
4. 取平均作为该 α 的评估分数
5. 选择评分最好的 α

---

## 第五部分：正则化路径与可视化

---

### 正则化路径图（Regularization Path）

**问题**：随着 α 增大，各特征的系数如何变化？

```{python}
#| echo: false
#| fig-width: 12
#| fig-height: 5

from sklearn.linear_model import lasso_path, ridge_regression, Lasso
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 生成模拟数据（如果前面没有定义）
if 'X_train' not in globals():
    np.random.seed(42)
    X, y = make_regression(n_samples=100, n_features=20, n_informative=5, noise=10, random_state=42)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Lasso 路径
alphas_lasso = np.logspace(-3, 1, 100)
coefs_lasso = []

for alpha in alphas_lasso:
    model = Lasso(alpha=alpha, max_iter=10000)
    model.fit(X_train, y_train)
    coefs_lasso.append(model.coef_)

coefs_lasso = np.array(coefs_lasso)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Lasso 路径
for i in range(coefs_lasso.shape[1]):
    axes[0].plot(alphas_lasso, coefs_lasso[:, i], linewidth=1.5, alpha=0.7)

axes[0].set_xlabel('α (正则化强度)', fontsize=12)
axes[0].set_ylabel('系数值', fontsize=12)
axes[0].set_title('Lasso 正则化路径\nα 增大时，系数逐渐变为 0', fontsize=13)
axes[0].set_xscale('log')
axes[0].axhline(y=0, color='black', linestyle='--', linewidth=1)
axes[0].grid(True, alpha=0.3)

# 非零系数数量
n_nonzero = [np.sum(coefs != 0) for coefs in coefs_lasso]
axes[1].plot(alphas_lasso, n_nonzero, 'b-', linewidth=2.5)
axes[1].set_xlabel('α (正则化强度)', fontsize=12)
axes[1].set_ylabel('非零系数个数', fontsize=12)
axes[1].set_title('Lasso 特征选择效果\nα 越大，保留的特征越少', fontsize=13)
axes[1].set_xscale('log')
axes[1].grid(True, alpha=0.3)
axes[1].fill_between(alphas_lasso, 0, n_nonzero, alpha=0.3)

plt.tight_layout()
plt.show()
```

**观察**：

- α 很小时：所有特征都保留
- α 逐渐增大：不重要的特征系数先变为 0
- α 很大时：几乎所有特征都被剔除

---

## 第六部分：实践建议

---

### 正则化回归完整流程

```{mermaid}
graph TD
    A[加载数据] --> B[数据标准化<br>StandardScaler]
    B --> C[训练/测试切分]
    C --> D[选择正则化方法]
    D --> E1[Ridge: 特征都重要]
    D --> E2[Lasso: 需要特征选择]
    D --> E3[Elastic Net: 不确定]
    
    E1 --> F1[RidgeCV 自动选α]
    E2 --> F2[LassoCV 自动选α]
    E3 --> F3[ElasticNetCV 自动选α]
    
    F1 --> G[训练最终模型]
    F2 --> G
    F3 --> G
    
    G --> H[测试集评估]
    H --> I[可视化分析<br>系数/路径/残差]
    
    style B fill:#fff9c4
    style F1 fill:#c8e6c9
    style F2 fill:#c8e6c9
    style F3 fill:#c8e6c9
    style I fill:#ffccbc
```

---

### 常见错误与注意事项

#### ❌ 错误 1：忘记标准化

**错误做法**：直接在原始数据上训练正则化模型

- 特征尺度差异很大时，正则化会不公平地惩罚大尺度特征
- 例如：面积特征（0-200）和卧室数特征（1-10），面积特征的权重会被过度惩罚

**正确做法**：

1. 先用 `StandardScaler` 在训练集上 fit（计算均值和标准差）
2. 用训练集的参数 transform 训练集和测试集
3. 确保所有特征都在同一尺度（均值0，标准差1）

**为什么重要**：正则化惩罚权重的大小，如果特征尺度不同，会导致不公平的惩罚。

---

#### ❌ 错误 2：在测试集上选 α

**错误做法**：尝试不同的 α 值，在测试集上选择表现最好的

- 这会"泄露"测试集信息，导致对模型效果的过于乐观估计
- 相当于用测试数据训练模型，违背了机器学习的基本原则

**正确做法**：

1. 用交叉验证在训练集上选择最优 α
2. 训练最终模型后，再在测试集上评估
3. 确保测试集只用于最终评估

**为什么重要**：避免数据泄露，得到真实的模型评估结果。

---

#### ❌ 错误 3：选择不合适的正则化方法

| 场景                   | 推荐方法      | 原因                   |
|------------------------|-------------|------------------------|
| 100个特征，认为都重要  | Ridge       | Ridge保留所有特征      |
| 100个特征，只有10个重要| Lasso       | Lasso自动特征选择      |
| 特征间高度相关         | Elastic Net | Elastic Net处理共线性更好|
| 不确定哪些特征重要     | Elastic Net | Elastic Net是折中方案  |
| 需要可解释性（特征少） | Lasso       | Lasso产生稀疏模型      |

---

### 实用技巧

#### 技巧 1：先尝试 Lasso

**为什么**：Lasso 可以自动做特征选择，告诉你哪些特征真正重要

- 如果 Lasso 把很多特征剔除了，说明数据有稀疏性
- 可以根据 Lasso 的结果决定是否需要特征工程
- 帮助理解数据的特征重要性分布

#### 技巧 2：绘制验证曲线

**目的**：观察 α 参数如何影响模型性能

- X轴：α 值（对数刻度，从 0.001 到 1000）
- Y轴：模型评分（R² 或其他指标）
- 两条线：训练集和验证集的性能

**如何解读**：

- α 太小：训练和验证都表现良好（可能过拟合）
- α 适中：验证集表现最好 ✓
- α 太大：训练和验证都表现差（欠拟合）

#### 技巧 3：对比多种方法

**推荐做法**：

1. 同时训练 Ridge、Lasso 和 Elastic Net（都用 CV 自动选参）
2. 比较它们的测试集性能
3. 选择最适合业务场景的方法
4. 记录不同方法的优缺点，便于后续解释

**为什么重要**：不同正则化方法适合不同场景，盲目选择可能错过最佳方案。

---

## 总结

---

### 本讲核心概念

#### 过拟合与正则化
- **过拟合**：模型太复杂，记住了训练数据的噪声
- **正则化**：在损失函数中加入复杂度惩罚，限制模型复杂度
- **Bias-Variance Tradeoff**：找到偏差和方差的平衡点

#### 三种正则化方法
- **Ridge (L2)**：让系数变小但不为 0，适合所有特征都重要
- **Lasso (L1)**：让某些系数变为 0，自动特征选择
- **Elastic Net**：结合 Ridge 和 Lasso，折中方案

#### 参数选择
- **交叉验证**：避免在测试集上调参
- **RidgeCV/LassoCV**：自动选择最优 α
- **正则化路径**：观察 α 变化时系数的变化

---

### 重要提醒

1. **正则化前必须标准化特征** ⚠️  
   否则正则化会不公平地惩罚某些特征

2. **不要在测试集上选参数** ⚠️  
   用交叉验证在训练集上选

3. **多试几种方法** ⚠️  
   Ridge、Lasso、Elastic Net 都试试，对比效果

4. **可视化很重要** ⚠️  
   正则化路径图、系数图、残差图

---

## Q&A

**Q1：什么是“过拟合” (Overfitting)？根据讲义的开场案例，它的关键信号是什么？**

**A：**

* 定义：过拟合是指模型过度学习了训练数据中的细节和噪声，导致它无法很好地泛化到未见过的新数据上。
* 关键信号：模型在训练集上表现很好（例如RMSE 5万），但在测试集上表现很差（例如RMSE 50万）。训练集表现和测试集表现之间存在巨大差距是过拟合的关键信号。

---

**Q2：什么是“正则化”？它如何修改普通线性回归的损失函数？**

**A：**

* 定义：正则化是一种用来防止过拟合、控制模型复杂度的技术。
* 修改方式：它在普通线性回归的损失函数（拟合误差，如MSE）基础上，额外加入一个“复杂度惩罚项”。
* 新目标：$\min(\text{拟合误差} + \alpha \times \text{复杂度惩罚})$。这迫使模型在“拟合数据”和“保持简单”之间找到一个平衡。

---

**Q3：Ridge 回归和 Lasso 回归各自使用的是哪种正则化惩罚项（L1 还是 L2）？它们的惩罚项在数学上有什么不同？**

**A：**

* Ridge (L2)：使用 L2 正则化。惩罚项是所有特征系数（权重）的平方和 ($\sum w_j^2$)。
* Lasso (L1)：使用 L1 正则化。惩罚项是所有特征系数（权重）的绝对值和 ($\sum |w_j|$)。

---

**Q4：Lasso (L1) 正则化最独特的特点是什么？这使它在实践中有什么重要应用？**

**A：**

* 最独特的特点：Lasso 最独特的特点是它能将某些不重要特征的系数（权重）直接压缩到 0。
* 重要应用：这使 Lasso 具有自动特征选择的功能。它可以帮助我们从大量特征中筛选出真正重要的特征，从而得到一个更简单（稀疏）、可解释性更强的模型。

---

**Q5：为什么我们必须使用“交叉验证” (Cross-Validation) 来选择正则化参数 $\alpha$，而不是直接在测试集上选择？**

**A：**

* 因为测试集的核心原则是“只在最后使用一次”，它用来评估最终模型的泛化能力。
* 如果在测试集上反复尝试不同的 $\alpha$ 值来“调参”，就相当于让模型在训练过程中“偷看”到了测试集的信息，这称为“数据泄露”。这会导致我们对模型性能的评估过于乐观，得到的不是模型真实的泛化能力。交叉验证允许我们在训练集内部安全地模拟这一评估过程，以找到最优的 $\alpha$。

---

**Q6：正则化是如何体现在“偏差-方差权衡” (Bias-Variance Tradeoff) 上的？增加正则化强度（即增大 $\alpha$）时，偏差和方差分别会如何变化？**

**A：**

* 关系：正则化是管理偏差-方差权衡的关键工具。一个过拟合的模型（$\alpha=0$）通常具有低偏差（能完美拟合训练数据）和高方差（对训练数据中的噪声高度敏感）。
* 增大 $\alpha$ 的影响：
    1.  增加偏差：当我们增大 $\alpha$（加强惩罚），我们是在强迫模型变得更简单。这种限制会使模型增加偏差（Bias），因为它可能无法再完美捕捉训练数据中的所有复杂规律。
    2.  降低方差：作为交换，一个更简单的模型对训练数据中特定噪声的敏感度显著降低了，即降低了方差（Variance）。
* 目标：正则化的目标是找到一个最优的 $\alpha$，在“欠拟合”（偏差高）和“过拟合”（方差高）之间找到一个平衡点，使得总误差（偏差 + 方差）最小。

---

**Q7：为什么讲义中反复强调“在应用正则化之前必须标准化特征”？如果忘记标准化（例如，一个特征“面积”范围 50-200，另一个“卧室数”范围 1-5），会导致什么严重后果？**

**A：**

* 核心原因：正则化（L1 和 L2）是通过惩罚系数（权重 $w$）的大小来限制模型复杂度的。
* 严重后果：
    1.  尺度影响系数：如果特征尺度不同，“面积”（50-200）的系数 $w_1$ 自然会很小（例如 0.01），而“卧室数”（1-5）的系数 $w_2$ 会相对很大（例如 10）。
    2.  不公平的惩罚：在计算惩罚项时（例如 L2：$\alpha (w_1^2 + w_2^2)$），模型会极大地惩罚那个尺度小、系数大的特征（$w_2=10$），而几乎忽略那个尺度大、系数小的特征（$w_1=0.01$）。
    3.  结论：忘记标准化会导致正则化的惩罚变得不公平且毫无意义。它会错误地惩罚那些仅仅因为单位尺度小而系数大的特征，而不是真正“不重要”的特征。标准化（如 Z-score）使所有特征处于同一尺度（例如均值0，标准差1），惩罚才会公平有效。

---

**Q8：在以下三种场景中，你应优先选择哪种正则化方法（Ridge, Lasso, Elastic Net），并说明理由？**

1.  **你有 500 个特征，但你怀疑其中只有约 20 个真正有用。**
2.  **你有 50 个特征，它们都与目标相关，但彼此之间（例如“收入”和“房产价值”）高度相关。**
3.  **你有 50 个特征，你认为它们都对预测有贡献，没有哪个是完全无用的。**

**A：**

1.  优先 Lasso：这是一个典型的“稀疏”场景。Lasso 的自动特征选择功能非常适合从大量特征中筛选出少数有用的特征，同时将其他无用特征的系数变为 0，得到一个简洁、可解释性强的模型。
2.  优先 Elastic Net：Lasso 在处理高度相关的特征时表现不稳定（它可能会随机选择一个，而把其他相关的特征系数压到0）。Elastic Net 结合了 L2（Ridge）的特性，在处理相关特征时更稳健，倾向于将它们“成组”地保留或剔除。
3.  优先 Ridge：因为我们认为所有特征都重要，我们不希望 Lasso 把任何一个特征的系数变为 0。Ridge (L2) 会保留所有特征，同时通过“缩小”所有系数来防止模型过拟合，这非常适合“所有特征都有用”的场景。